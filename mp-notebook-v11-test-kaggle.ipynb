{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "14sXDYyegbMQjRaEottMtnEr-SDILHZrW",
     "timestamp": 1771328894458
    }
   ],
   "gpuType": "T4"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 14872089,
     "sourceType": "datasetVersion",
     "datasetId": 9514196
    }
   ],
   "dockerImageVersionId": 31260,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "<div style=\"color:rgb(0,0,255);font-size: 40px;font-weight:700;\">\nMAIN SETTINGS\n</div>",
   "metadata": {
    "id": "8u27b0sPG4SA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "### SETTINGS (VAST.AI ONLY) ###\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import subprocess\n",
    "import shutil\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from getpass import getpass\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "# Notebook is now Vast.ai-only.\n",
    "ON_KAGGLE = False\n",
    "ON_COLAB = False\n",
    "ON_VAST = True\n",
    "\n",
    "MAX_PARALLEL_DOWNLOADS = max(1, int(os.environ.get(\"MAX_PARALLEL_DOWNLOADS\", \"3\")))\n",
    "MIN_VALID_FILE_BYTES = int(os.environ.get(\"MIN_VALID_FILE_BYTES\", \"1000000\"))\n",
    "\n",
    "BASE_DIR = os.environ.get(\"BASE_DIR\", \"/workspace\")\n",
    "if not os.path.isdir(BASE_DIR):\n",
    "    raise FileNotFoundError(f\"BASE_DIR not found: {BASE_DIR}\")\n",
    "print(\"Working directory:\", BASE_DIR)\n",
    "\n",
    "# Configuration\n",
    "FORGE_DIR = os.path.join(BASE_DIR, \"stable-diffusion-webui-forge\")\n",
    "MODELS_DIR = os.path.join(FORGE_DIR, \"models\", \"Stable-diffusion\")\n",
    "LORA_DIR = os.path.join(FORGE_DIR, \"models\", \"Lora\")\n",
    "CONTROLNET_DIR = os.path.join(FORGE_DIR, \"extensions\", \"sd-webui-controlnet\")\n",
    "CONTROLNET_MODELS_DIR = os.path.join(CONTROLNET_DIR, \"models\")\n",
    "EXTENSIONS_DIR = os.path.join(FORGE_DIR, \"extensions\")\n",
    "OUTPUTS_DIR = os.path.join(FORGE_DIR, \"outputs\")\n",
    "VOLUME_DIR = os.path.join(BASE_DIR, \"volume\")\n",
    "GEN_DIR = os.path.join(BASE_DIR, \"gen\")\n",
    "IMAGES_DIR = os.path.join(GEN_DIR, \"Images\")\n",
    "\n",
    "for d in [MODELS_DIR, LORA_DIR, CONTROLNET_MODELS_DIR, EXTENSIONS_DIR, OUTPUTS_DIR, VOLUME_DIR, GEN_DIR, IMAGES_DIR]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "\n",
    "def ensure_python_package(pkg_name: str, import_name: str | None = None):\n",
    "    module_name = import_name or pkg_name\n",
    "    try:\n",
    "        __import__(module_name)\n",
    "        print(f\"Python package already installed: {pkg_name}\")\n",
    "    except Exception:\n",
    "        print(f\"Installing missing Python package: {pkg_name}\")\n",
    "        subprocess.run([\"python\", \"-m\", \"pip\", \"install\", \"-q\", pkg_name], check=False)\n",
    "\n",
    "\n",
    "def ensure_aria2c():\n",
    "    if shutil.which(\"aria2c\") is not None:\n",
    "        print(\"aria2c already available\")\n",
    "        return\n",
    "\n",
    "    print(\"aria2c not found -> installing...\")\n",
    "    subprocess.run([\"apt\", \"update\", \"-qq\"], check=False)\n",
    "    subprocess.run([\"apt\", \"install\", \"-y\", \"-qq\", \"aria2\"], check=False)\n",
    "\n",
    "\n",
    "# Dependencies used by generation cell and optional downloads\n",
    "ensure_python_package(\"openpyxl\")\n",
    "ensure_python_package(\"requests\")\n",
    "ensure_aria2c()\n",
    "\n",
    "\n",
    "def get_secret(name: str):\n",
    "    value = os.environ.get(name)\n",
    "    if value:\n",
    "        return value.strip(), \"env\"\n",
    "    return None, None\n",
    "\n",
    "\n",
    "CIVITAI_TOKEN, CIVITAI_SRC = get_secret(\"CIVITAI_TOKEN\")\n",
    "HF_TOKEN, HF_SRC = get_secret(\"HF_TOKEN\")\n",
    "\n",
    "if not CIVITAI_TOKEN:\n",
    "    manual_civitai = getpass(\"Enter CIVITAI_TOKEN (leave blank to skip): \").strip()\n",
    "    if manual_civitai:\n",
    "        CIVITAI_TOKEN, CIVITAI_SRC = manual_civitai, \"manual_input\"\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    manual_hf = getpass(\"Enter HF_TOKEN (leave blank to skip): \").strip()\n",
    "    if manual_hf:\n",
    "        HF_TOKEN, HF_SRC = manual_hf, \"manual_input\"\n",
    "\n",
    "TOKENS = {}\n",
    "if CIVITAI_TOKEN:\n",
    "    TOKENS[\"CIVITAI\"] = CIVITAI_TOKEN\n",
    "if HF_TOKEN:\n",
    "    TOKENS[\"HF_TOKEN\"] = HF_TOKEN\n",
    "\n",
    "print(\"Token sources:\")\n",
    "print(f\"  CIVITAI_TOKEN: {CIVITAI_SRC or 'not found'}\")\n",
    "print(f\"  HF_TOKEN: {HF_SRC or 'not found'}\")\n",
    "if not TOKENS:\n",
    "    print(\"No tokens provided: token-protected downloads will be skipped.\")\n"
   ],
   "metadata": {
    "trusted": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kTzG8SwHG4SC",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1771328411100,
     "user_tz": -120,
     "elapsed": 18125,
     "user": {
      "displayName": "Sokolenko Timofei",
      "userId": "14202647928619858996"
     }
    },
    "outputId": "f393f94c-ad89-4ada-cf09-46ff7662971f",
    "execution": {
     "iopub.status.busy": "2026-02-17T20:09:54.316956Z",
     "iopub.execute_input": "2026-02-17T20:09:54.317263Z",
     "iopub.status.idle": "2026-02-17T20:10:08.135076Z",
     "shell.execute_reply.started": "2026-02-17T20:09:54.317236Z",
     "shell.execute_reply": "2026-02-17T20:10:08.134387Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "aria2c not found → installing...\naria2c installed successfully\nWorking directory: /kaggle/working\nToken sources:\n  CIVITAI_TOKEN: kaggle_secrets\n  HF_TOKEN: kaggle_secrets\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "### FORGE INSTALL/BOOTSTRAP (DISABLED FOR VAST-ONLY NOTEBOOK) ###\n",
    "\n",
    "print(\"Skipped: this notebook no longer installs Forge or system packages for Kaggle/Colab.\")\n",
    "print(\"Expected setup: Forge is already provisioned by your Vast.ai template.\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wM3J4QA4G4SK",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1771328433337,
     "user_tz": -120,
     "elapsed": 22233,
     "user": {
      "displayName": "Sokolenko Timofei",
      "userId": "14202647928619858996"
     }
    },
    "outputId": "6be9f00e-054b-4d09-a4c0-9c4486b602bd",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-17T20:10:08.136512Z",
     "iopub.execute_input": "2026-02-17T20:10:08.136793Z",
     "iopub.status.idle": "2026-02-17T20:10:31.748555Z",
     "shell.execute_reply.started": "2026-02-17T20:10:08.136771Z",
     "shell.execute_reply": "2026-02-17T20:10:31.747926Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Checking/installing platform dependencies...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nWARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n\nW: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n\nWARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "171 packages can be upgraded. Run 'apt list --upgradable' to see them.\ngit is already the newest version (1:2.34.1-1ubuntu1.15).\nThe following additional packages will be installed:\n  libpython3.10 libpython3.10-dev libpython3.10-minimal libpython3.10-stdlib\n  python3-pip-whl python3-pkg-resources python3-setuptools\n  python3-setuptools-whl python3-wheel python3.10 python3.10-dev\n  python3.10-minimal python3.10-venv\nSuggested packages:\n  python-setuptools-doc python3.10-doc binfmt-support\nThe following NEW packages will be installed:\n  python3-pip python3-pip-whl python3-setuptools python3-setuptools-whl\n  python3-venv python3-wheel python3.10-venv\nThe following packages will be upgraded:\n  libpython3.10 libpython3.10-dev libpython3.10-minimal libpython3.10-stdlib\n  python3-pkg-resources python3.10 python3.10-dev python3.10-minimal\n8 upgraded, 7 newly installed, 0 to remove and 163 not upgraded.\nNeed to get 17.2 MB of archives.\nAfter this operation, 12.4 MB of additional disk space will be used.\n(Reading database ... 129147 files and directories currently installed.)\nPreparing to unpack .../00-python3.10-dev_3.10.12-1~22.04.14_amd64.deb ...\nUnpacking python3.10-dev (3.10.12-1~22.04.14) over (3.10.12-1~22.04.11) ...\nPreparing to unpack .../01-libpython3.10-dev_3.10.12-1~22.04.14_amd64.deb ...\nUnpacking libpython3.10-dev:amd64 (3.10.12-1~22.04.14) over (3.10.12-1~22.04.11) ...\nPreparing to unpack .../02-libpython3.10_3.10.12-1~22.04.14_amd64.deb ...\nUnpacking libpython3.10:amd64 (3.10.12-1~22.04.14) over (3.10.12-1~22.04.11) ...\nPreparing to unpack .../03-python3.10_3.10.12-1~22.04.14_amd64.deb ...\nUnpacking python3.10 (3.10.12-1~22.04.14) over (3.10.12-1~22.04.11) ...\nPreparing to unpack .../04-libpython3.10-stdlib_3.10.12-1~22.04.14_amd64.deb ...\nUnpacking libpython3.10-stdlib:amd64 (3.10.12-1~22.04.14) over (3.10.12-1~22.04.11) ...\nPreparing to unpack .../05-python3.10-minimal_3.10.12-1~22.04.14_amd64.deb ...\nUnpacking python3.10-minimal (3.10.12-1~22.04.14) over (3.10.12-1~22.04.11) ...\nPreparing to unpack .../06-libpython3.10-minimal_3.10.12-1~22.04.14_amd64.deb ...\nUnpacking libpython3.10-minimal:amd64 (3.10.12-1~22.04.14) over (3.10.12-1~22.04.11) ...\nPreparing to unpack .../07-python3-pkg-resources_68.1.2-2~jammy3_all.deb ...\nUnpacking python3-pkg-resources (68.1.2-2~jammy3) over (59.6.0-1.2ubuntu0.22.04.3) ...\nSelecting previously unselected package python3-setuptools.\nPreparing to unpack .../08-python3-setuptools_68.1.2-2~jammy3_all.deb ...\nUnpacking python3-setuptools (68.1.2-2~jammy3) ...\nSelecting previously unselected package python3-wheel.\nPreparing to unpack .../09-python3-wheel_0.37.1-2ubuntu0.22.04.1_all.deb ...\nUnpacking python3-wheel (0.37.1-2ubuntu0.22.04.1) ...\nSelecting previously unselected package python3-pip.\nPreparing to unpack .../10-python3-pip_22.0.2+dfsg-1ubuntu0.7_all.deb ...\nUnpacking python3-pip (22.0.2+dfsg-1ubuntu0.7) ...\nSelecting previously unselected package python3-pip-whl.\nPreparing to unpack .../11-python3-pip-whl_22.0.2+dfsg-1ubuntu0.7_all.deb ...\nUnpacking python3-pip-whl (22.0.2+dfsg-1ubuntu0.7) ...\nSelecting previously unselected package python3-setuptools-whl.\nPreparing to unpack .../12-python3-setuptools-whl_68.1.2-2~jammy3_all.deb ...\nUnpacking python3-setuptools-whl (68.1.2-2~jammy3) ...\nSelecting previously unselected package python3.10-venv.\nPreparing to unpack .../13-python3.10-venv_3.10.12-1~22.04.14_amd64.deb ...\nUnpacking python3.10-venv (3.10.12-1~22.04.14) ...\nSelecting previously unselected package python3-venv.\nPreparing to unpack .../14-python3-venv_3.10.6-1~22.04.1_amd64.deb ...\nUnpacking python3-venv (3.10.6-1~22.04.1) ...\nSetting up python3-pkg-resources (68.1.2-2~jammy3) ...\nSetting up python3-setuptools-whl (68.1.2-2~jammy3) ...\nSetting up python3-setuptools (68.1.2-2~jammy3) ...\nSetting up python3-pip-whl (22.0.2+dfsg-1ubuntu0.7) ...\nSetting up python3-wheel (0.37.1-2ubuntu0.22.04.1) ...\nSetting up libpython3.10-minimal:amd64 (3.10.12-1~22.04.14) ...\nSetting up python3-pip (22.0.2+dfsg-1ubuntu0.7) ...\nSetting up python3.10-minimal (3.10.12-1~22.04.14) ...\nSetting up libpython3.10-stdlib:amd64 (3.10.12-1~22.04.14) ...\nSetting up libpython3.10:amd64 (3.10.12-1~22.04.14) ...\nSetting up python3.10 (3.10.12-1~22.04.14) ...\nSetting up libpython3.10-dev:amd64 (3.10.12-1~22.04.14) ...\nSetting up python3.10-dev (3.10.12-1~22.04.14) ...\nSetting up python3.10-venv (3.10.12-1~22.04.14) ...\nSetting up python3-venv (3.10.6-1~22.04.1) ...\nProcessing triggers for libc-bin (2.35-0ubuntu3.8) ...\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n\nProcessing triggers for man-db (2.10.2-1) ...\nProcessing triggers for mailcap (3.70+nmu1ubuntu1) ...\nFORGE_DIR exists but WebUI Forge is incomplete/corrupted. Recreating...\nCloning WebUI Forge...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Cloning into '/kaggle/working/stable-diffusion-webui-forge'...\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Optional bootstrap finished.\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "### CONTROLNET INSTALL OPTIONS (DISABLED) ###\n",
    "\n",
    "print(\"Skipped: ControlNet install/update is handled by provisioning (woload.sh), not by notebook.\")\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-17T20:10:31.749362Z",
     "iopub.execute_input": "2026-02-17T20:10:31.749559Z",
     "iopub.status.idle": "2026-02-17T20:11:34.603459Z",
     "shell.execute_reply.started": "2026-02-17T20:10:31.749540Z",
     "shell.execute_reply": "2026-02-17T20:11:34.602577Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "ControlNet extension path: /kaggle/working/stable-diffusion-webui-forge/extensions/sd-webui-controlnet\nInstall mode: reinstall\nCloning ControlNet repository...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Cloning into '/kaggle/working/stable-diffusion-webui-forge/extensions/sd-webui-controlnet'...\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "ControlNet repository is ready\nControlNet models directory: /kaggle/working/stable-diffusion-webui-forge/extensions/sd-webui-controlnet/models\n\n=== ControlNet ===\nParallel downloads: 3\n[DOWNLOADING] t2i-adapter_xl_openpose 151 MB\n[DOWNLOADING] t2i-adapter_xl_canny 148 MB\n[DOWNLOADING] t2i-adapter_xl_sketch 148 MB\n[DOWNLOADING] t2i-adapter_diffusers_xl_depth_midas 151 MB[OK]   t2i-adapter_xl_sketch 148 MB -> 147.9 MB\n\n[DOWNLOADING] t2i-adapter_diffusers_xl_depth_zoe 151 MB\n[OK]   t2i-adapter_xl_canny 148 MB -> 147.9 MB\n[DOWNLOADING] t2i-adapter_diffusers_xl_lineart 151 MB\n[OK]   t2i-adapter_xl_openpose 151 MB -> 150.7 MB\n[OK]   t2i-adapter_diffusers_xl_depth_zoe 151 MB -> 150.7 MB\n[OK]   t2i-adapter_diffusers_xl_depth_midas 151 MB -> 150.7 MB\n[OK]   t2i-adapter_diffusers_xl_lineart 151 MB -> 150.7 MB\nDone: OK=6, FAIL=0\n\n=== Checkpoints ===\nParallel downloads: 1\n[DOWNLOADING] WAI ILL V16.0 6,46 GB\n[OK]   WAI ILL V16.0 6,46 GB -> 6616.6 MB\nDone: OK=1, FAIL=0\n\n=== LoRA ===\nParallel downloads: 3\n[DOWNLOADING] Realistic filter V1 55 MB\n[DOWNLOADING] Detailer IL V2 218 MB\n[DOWNLOADING] Hyperrealistic V4 ILL 435 MB\n[DOWNLOADING] Niji semi realism V3.5 ILL 435 MB\n[OK]   Realistic filter V1 55 MB -> 54.8 MB\n[OK]   Detailer IL V2 218 MB -> 217.9 MB\n[DOWNLOADING] ATNR Style ILL V1.1 350 MB\n[DOWNLOADING] Face Enhancer Ill 218 MB\n[OK]   Hyperrealistic V4 ILL 435 MB -> 435.4 MB\n[OK]   Face Enhancer Ill 218 MB -> 217.9 MB\n[DOWNLOADING] Smooth Detailer Booster V4 243 MB\n[DOWNLOADING] USNR Style V-pred 157 MB\n[OK]   Niji semi realism V3.5 ILL 435 MB -> 435.4 MB\n[OK]   ATNR Style ILL V1.1 350 MB -> 350.2 MB\n[DOWNLOADING] 748cm Style V1 243 MB\n[OK]   Smooth Detailer Booster V4 243 MB -> 243.2 MB\n[DOWNLOADING] Velvet's Mythic Fantasy Styles IL 218 MB\n[OK]   USNR Style V-pred 157 MB -> 156.9 MB\n[DOWNLOADING] Pixel Art Style IL V7 435 MB\n[OK]   748cm Style V1 243 MB -> 243.2 MB\n  WARNING: file size looks low (217.9 MB vs expected ~435.0 MB)\n[OK]   Velvet's Mythic Fantasy Styles IL 218 MB -> 217.9 MB\n[OK]   Pixel Art Style IL V7 435 MB -> 217.9 MB\nDone: OK=11, FAIL=0\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "### RUN WEBUI (VAST: START EXISTING FORGE ONLY) ###\n",
    "\n",
    "import os\n",
    "import time\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "forge_dir = Path(FORGE_DIR)\n",
    "if not forge_dir.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"FORGE_DIR not found: {forge_dir}. Use a Vast template with Forge preinstalled.\"\n",
    "    )\n",
    "\n",
    "cmd = [\"bash\", \"webui.sh\", \"-f\", \"--xformers\", \"--api\", \"--port\", \"17860\"]\n",
    "run_env = os.environ.copy()\n",
    "run_env[\"MPLBACKEND\"] = \"Agg\"\n",
    "print(\"Running (background):\", \" \".join(cmd), \"in\", forge_dir)\n",
    "\n",
    "pid_path = forge_dir / \"webui.pid\"\n",
    "log_path = forge_dir / \"webui.log\"\n",
    "\n",
    "\n",
    "def _pid_alive(pid: int) -> bool:\n",
    "    try:\n",
    "        os.kill(pid, 0)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def _log_tail(chars: int = 4000) -> str:\n",
    "    if not log_path.exists():\n",
    "        return \"(webui.log not found yet)\"\n",
    "    return log_path.read_text(encoding=\"utf-8\", errors=\"ignore\")[-chars:]\n",
    "\n",
    "\n",
    "running_pid = None\n",
    "if pid_path.exists():\n",
    "    try:\n",
    "        old_pid = int(pid_path.read_text(encoding=\"utf-8\").strip())\n",
    "        if _pid_alive(old_pid):\n",
    "            running_pid = old_pid\n",
    "            print(f\"WebUI already running with PID={old_pid}. Можно запускать последнюю ячейку.\")\n",
    "        else:\n",
    "            pid_path.unlink(missing_ok=True)\n",
    "    except Exception:\n",
    "        pid_path.unlink(missing_ok=True)\n",
    "\n",
    "started_proc = None\n",
    "if running_pid is None:\n",
    "    with open(log_path, \"a\", encoding=\"utf-8\") as log_f:\n",
    "        started_proc = subprocess.Popen(\n",
    "            cmd,\n",
    "            cwd=forge_dir,\n",
    "            env=run_env,\n",
    "            stdout=log_f,\n",
    "            stderr=subprocess.STDOUT,\n",
    "            start_new_session=True,\n",
    "            text=True,\n",
    "        )\n",
    "    pid_path.write_text(str(started_proc.pid), encoding=\"utf-8\")\n",
    "    running_pid = started_proc.pid\n",
    "    print(f\"WebUI started in background. PID={running_pid}\")\n",
    "    print(f\"Logs: {log_path}\")\n",
    "\n",
    "observe_timeout = int(os.environ.get(\"FORGE_STARTUP_OBSERVE_TIMEOUT\", \"120\"))\n",
    "observe_interval = float(os.environ.get(\"FORGE_STARTUP_OBSERVE_INTERVAL\", \"2\"))\n",
    "\n",
    "for _ in range(max(1, int(observe_timeout / observe_interval))):\n",
    "    tail = _log_tail()\n",
    "\n",
    "    if started_proc is not None:\n",
    "        rc = started_proc.poll()\n",
    "        if rc is not None:\n",
    "            raise RuntimeError(\n",
    "                f\"WebUI process exited early with code {rc}.\\n\"\n",
    "                f\"Check logs at: {log_path}\\n\"\n",
    "                f\"Last log lines:\\n{tail}\"\n",
    "            )\n",
    "\n",
    "    if \"Running on local URL\" in tail or \"Uvicorn running\" in tail:\n",
    "        print(\"WebUI appears ready. Можно запускать последнюю ячейку.\")\n",
    "        break\n",
    "\n",
    "    time.sleep(observe_interval)\n",
    "else:\n",
    "    print(\"WebUI всё ещё запускается в фоне. Запускайте последнюю ячейку: она дождётся API.\")\n",
    "    print(f\"Если API не поднимется, проверьте лог: {log_path}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cpZBUyZoYFHL",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1771328692825,
     "user_tz": -120,
     "elapsed": 188283,
     "user": {
      "displayName": "Sokolenko Timofei",
      "userId": "14202647928619858996"
     }
    },
    "outputId": "9fa9759a-2652-4c2e-d911-73a85dcf233c",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-17T20:11:34.604836Z",
     "iopub.execute_input": "2026-02-17T20:11:34.605071Z",
     "iopub.status.idle": "2026-02-17T20:13:34.647133Z",
     "shell.execute_reply.started": "2026-02-17T20:11:34.605049Z",
     "shell.execute_reply": "2026-02-17T20:13:34.646273Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Patched torch stage (reinstalled numpy 2.x + scikit-image for ABI compatibility).\nPatched start stage (re-pin numpy/scikit-image before importing webui).\nPatched CLIP install command (fixed flags + preserved indentation).\nsoft_inpainting.py already disabled or not present.\nRunning (background): bash webui.sh -f --xformers --api --port 17860 in /kaggle/working/stable-diffusion-webui-forge\nMPLBACKEND forced to: Agg\nWarning: venv python not found at /kaggle/working/stable-diffusion-webui-forge/venv/bin/python, skipping joblib/insightface install.\nWebUI started in background. PID=1927\nLogs: /kaggle/working/stable-diffusion-webui-forge/webui.log\nWebUI всё ещё запускается в фоне. Запускайте последнюю ячейку: она дождётся API.\nЕсли API не поднимется, проверьте лог: /kaggle/working/stable-diffusion-webui-forge/webui.log\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "### API GENERATION FROM XLSX ###\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import glob\n",
    "import time\n",
    "import shutil\n",
    "import zipfile\n",
    "import base64\n",
    "from datetime import datetime\n",
    "\n",
    "import requests\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "API_BASE = os.environ.get(\"FORGE_API_BASE\", \"http://127.0.0.1:17860\")\n",
    "TXT2IMG_URL = f\"{API_BASE}/sdapi/v1/txt2img\"\n",
    "OPENAPI_URL = f\"{API_BASE}/openapi.json\"\n",
    "\n",
    "PROMPTS_CANDIDATES = [\n",
    "    os.path.join(GEN_DIR, \"prompts.xlsx\"),\n",
    "    os.path.join(GEN_DIR, \"prompts.xlxs\"),\n",
    "    os.path.join(GEN_DIR, \"Prompts.xlsx\"),\n",
    "    os.path.join(GEN_DIR, \"Prompts.xlxs\"),\n",
    "    os.path.join(BASE_DIR, \"prompts.xlsx\"),\n",
    "    os.path.join(BASE_DIR, \"prompts.xlxs\"),\n",
    "    os.path.join(BASE_DIR, \"Prompts.xlsx\"),\n",
    "    os.path.join(BASE_DIR, \"Prompts.xlxs\"),\n",
    "    \"/workspace/gen/prompts.xlsx\",\n",
    "    \"/workspace/gen/prompts.xlxs\",\n",
    "]\n",
    "\n",
    "if ON_COLAB:\n",
    "    PROMPTS_CANDIDATES.extend([\n",
    "        \"/content/prompts.xlsx\",\n",
    "        \"/content/prompts.xlxs\",\n",
    "        \"/content/Prompts.xlsx\",\n",
    "        \"/content/Prompts.xlxs\",\n",
    "    ])\n",
    "\n",
    "if ON_KAGGLE:\n",
    "    PROMPTS_CANDIDATES.append(\"/kaggle/input/datasets/sokolenkotimofei/prompts/Prompts.xlsx\")\n",
    "    for pattern in [\n",
    "        \"/kaggle/input/*/*/prompts/prompts.xlsx\",\n",
    "        \"/kaggle/input/*/*/prompts/prompts.xlxs\",\n",
    "        \"/kaggle/input/*/*/prompts/Prompts.xlsx\",\n",
    "        \"/kaggle/input/*/*/prompts/Prompts.xlxs\",\n",
    "        \"/kaggle/input/*/prompts/prompts.xlsx\",\n",
    "        \"/kaggle/input/*/prompts/prompts.xlxs\",\n",
    "        \"/kaggle/input/*/prompts/Prompts.xlsx\",\n",
    "        \"/kaggle/input/*/prompts/Prompts.xlxs\",\n",
    "    ]:\n",
    "        PROMPTS_CANDIDATES.extend(sorted(glob.glob(pattern)))\n",
    "\n",
    "PROMPTS_CANDIDATES = list(dict.fromkeys(PROMPTS_CANDIDATES))\n",
    "\n",
    "API_READY_TIMEOUT = int(os.environ.get(\"FORGE_API_READY_TIMEOUT\", \"600\"))\n",
    "API_READY_INTERVAL = float(os.environ.get(\"FORGE_API_READY_INTERVAL\", \"3\"))\n",
    "\n",
    "\n",
    "def wait_for_api_ready(base_url: str, timeout_s: int = API_READY_TIMEOUT, interval_s: float = API_READY_INTERVAL):\n",
    "    start = time.time()\n",
    "    last_error = None\n",
    "    webui_log = os.path.join(FORGE_DIR, \"webui.log\")\n",
    "    webui_pid = os.path.join(FORGE_DIR, \"webui.pid\")\n",
    "\n",
    "    def _log_tail(chars: int = 4000):\n",
    "        if not os.path.exists(webui_log):\n",
    "            return \"(webui.log not found yet)\"\n",
    "        with open(webui_log, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            return f.read()[-chars:]\n",
    "\n",
    "    def _is_pid_alive():\n",
    "        try:\n",
    "            if not os.path.exists(webui_pid):\n",
    "                return None\n",
    "            with open(webui_pid, \"r\", encoding=\"utf-8\") as f:\n",
    "                pid = int(f.read().strip())\n",
    "            os.kill(pid, 0)\n",
    "            return pid\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    while (time.time() - start) < timeout_s:\n",
    "        try:\n",
    "            r = requests.get(f\"{base_url}/sdapi/v1/progress\", timeout=10)\n",
    "            if r.status_code < 500:\n",
    "                return True\n",
    "            last_error = f\"HTTP {r.status_code}\"\n",
    "        except Exception as e:\n",
    "            last_error = str(e)\n",
    "\n",
    "        pid_state = _is_pid_alive()\n",
    "        if pid_state is False:\n",
    "            raise RuntimeError(\n",
    "                \"Forge API недоступен и процесс WebUI уже завершился.\\n\"\n",
    "                f\"Last error: {last_error}\\n\"\n",
    "                f\"WebUI log tail:\\n{_log_tail()}\"\n",
    "            )\n",
    "\n",
    "        time.sleep(interval_s)\n",
    "\n",
    "    raise RuntimeError(\n",
    "        f\"Forge API is not reachable at {base_url} after {timeout_s}s. \"\n",
    "        f\"Last error: {last_error}.\\n\"\n",
    "        f\"WebUI log tail:\\n{_log_tail()}\\n\"\n",
    "        \"Run the Forge startup cell first. If this is first launch, increase FORGE_API_READY_TIMEOUT (e.g. 900).\"\n",
    "    )\n",
    "\n",
    "\n",
    "LOG_PATH = os.path.join(VOLUME_DIR, \"log.txt\")\n",
    "API_IMAGES_DIR = os.path.join(OUTPUTS_DIR, \"api_generated\")\n",
    "REQUEST_DUMPS_DIR = os.path.join(VOLUME_DIR, \"request_dumps\")\n",
    "os.makedirs(API_IMAGES_DIR, exist_ok=True)\n",
    "os.makedirs(REQUEST_DUMPS_DIR, exist_ok=True)\n",
    "\n",
    "INPUT_IMAGE_DIRS = [\n",
    "    os.path.join(GEN_DIR, \"Images\"),\n",
    "    os.path.join(GEN_DIR, \"images\"),\n",
    "    \"/kaggle/input/datasets/sokolenkotimofei/images\",\n",
    "]\n",
    "if ON_KAGGLE:\n",
    "    INPUT_IMAGE_DIRS.extend(sorted(glob.glob(\"/kaggle/input/*/*/images\")))\n",
    "    INPUT_IMAGE_DIRS.extend(sorted(glob.glob(\"/kaggle/input/*/images\")))\n",
    "INPUT_IMAGE_DIRS = [p for i, p in enumerate(INPUT_IMAGE_DIRS) if p and p not in INPUT_IMAGE_DIRS[:i]]\n",
    "\n",
    "CONFLICT_RULES = [\n",
    "    ((\"hr_resize_x\", \"hr_resize_y\"), (\"hr_scale\",)),\n",
    "]\n",
    "\n",
    "CONTROLNET_FIELDS = [\n",
    "    \"enabled\", \"image\", \"mask\", \"weight\", \"module\", \"model\", \"resize_mode\", \"lowvram\",\n",
    "    \"processor_res\", \"threshold_a\", \"threshold_b\", \"guidance_start\", \"guidance_end\",\n",
    "    \"control_mode\", \"pixel_perfect\",\n",
    "]\n",
    "\n",
    "\n",
    "def log_line(text: str):\n",
    "    ts = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    line = f\"[{ts}] {text}\"\n",
    "    print(line)\n",
    "    with open(LOG_PATH, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "\n",
    "def normalize_value(value):\n",
    "    if isinstance(value, str):\n",
    "        stripped = value.strip()\n",
    "        lowered = stripped.lower()\n",
    "        if lowered in {\"\", \"null\", \"none\", \"nan\"}:\n",
    "            return None\n",
    "        if lowered in {\"true\", \"yes\", \"1\"}:\n",
    "            return True\n",
    "        if lowered in {\"false\", \"no\", \"0\"}:\n",
    "            return False\n",
    "        if re.fullmatch(r\"-?\\d+\", stripped):\n",
    "            return int(stripped)\n",
    "        if re.fullmatch(r\"-?\\d+\\.\\d+\", stripped):\n",
    "            return float(stripped)\n",
    "        return stripped\n",
    "    return value\n",
    "\n",
    "\n",
    "def first_existing_path(candidates):\n",
    "    for p in candidates:\n",
    "        if os.path.exists(p):\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "\n",
    "def parse_workbook(path):\n",
    "    wb = load_workbook(path, data_only=True)\n",
    "    ws = wb.active\n",
    "\n",
    "    row1 = [normalize_value(v) for v in next(ws.iter_rows(min_row=1, max_row=1, values_only=True))]\n",
    "    row2 = [normalize_value(v) for v in next(ws.iter_rows(min_row=2, max_row=2, values_only=True))]\n",
    "\n",
    "    row1_headers_like = any(isinstance(v, str) and v in {\"prompt\", \"negative_prompt\", \"sampler_name\", \"steps\", \"cn1_enabled\"} for v in row1)\n",
    "\n",
    "    if row1_headers_like:\n",
    "        instruction = \"{prompt}\"\n",
    "        headers = [str(v).strip() if v is not None else \"\" for v in row1]\n",
    "        data_start_row = 2\n",
    "    else:\n",
    "        instruction_parts = [str(v).strip() for v in row1 if v is not None and str(v).strip()]\n",
    "        if not instruction_parts:\n",
    "            raise ValueError(\"Первая строка (инструкция) пустая\")\n",
    "        instruction = \" \".join(instruction_parts)\n",
    "        headers = [str(v).strip() if v is not None else \"\" for v in row2]\n",
    "        data_start_row = 3\n",
    "\n",
    "    if not any(headers):\n",
    "        raise ValueError(\"Не найдены заголовки таблицы с переменными\")\n",
    "\n",
    "    rows = []\n",
    "    for row_idx in range(data_start_row, ws.max_row + 1):\n",
    "        row_values = [normalize_value(v) for v in next(ws.iter_rows(min_row=row_idx, max_row=row_idx, values_only=True))]\n",
    "        if all(v is None for v in row_values):\n",
    "            continue\n",
    "\n",
    "        row_map = {}\n",
    "        for idx, key in enumerate(headers):\n",
    "            if not key or key.startswith(\"S_\"):\n",
    "                continue\n",
    "            value = row_values[idx] if idx < len(row_values) else None\n",
    "            if value is None:\n",
    "                continue\n",
    "            row_map[key] = value\n",
    "        rows.append(row_map)\n",
    "\n",
    "    if not rows:\n",
    "        raise ValueError(\"Не найдено строк данных\")\n",
    "\n",
    "    return instruction, rows\n",
    "\n",
    "\n",
    "def render_instruction(template: str, variables: dict):\n",
    "    def repl(match):\n",
    "        key = match.group(1)\n",
    "        return str(variables.get(key, match.group(0)))\n",
    "    return re.sub(r\"\\{([a-zA-Z0-9_]+)\\}\", repl, template)\n",
    "\n",
    "\n",
    "def apply_conflict_rules(payload: dict):\n",
    "    cleaned = dict(payload)\n",
    "    for primary_keys, conflicting_keys in CONFLICT_RULES:\n",
    "        primary_present = all((k in cleaned and cleaned[k] is not None) for k in primary_keys)\n",
    "        if primary_present:\n",
    "            for ck in conflicting_keys:\n",
    "                cleaned.pop(ck, None)\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def fetch_txt2img_params_dump():\n",
    "    try:\n",
    "        response = requests.get(OPENAPI_URL, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        spec = response.json()\n",
    "        schemas = spec.get(\"components\", {}).get(\"schemas\", {})\n",
    "        candidates = [\n",
    "            \"StableDiffusionTxt2ImgProcessingApi\",\n",
    "            \"Txt2ImgRequest\",\n",
    "            \"StableDiffusionProcessingTxt2Img\",\n",
    "        ]\n",
    "        for name in candidates:\n",
    "            if name in schemas:\n",
    "                props = schemas[name].get(\"properties\", {})\n",
    "                return json.dumps({k: v.get(\"type\", \"unknown\") for k, v in props.items()}, ensure_ascii=False, indent=2)\n",
    "        return json.dumps(spec.get(\"paths\", {}).get(\"/sdapi/v1/txt2img\", {}), ensure_ascii=False, indent=2)\n",
    "    except Exception as e:\n",
    "        return f\"Не удалось получить список переменных: {e}\"\n",
    "\n",
    "\n",
    "def resolve_image_path(image_ref):\n",
    "    if image_ref is None:\n",
    "        return None\n",
    "    if isinstance(image_ref, str) and image_ref.startswith(\"data:image\"):\n",
    "        return image_ref\n",
    "\n",
    "    ref = str(image_ref).strip()\n",
    "    if not ref:\n",
    "        return None\n",
    "\n",
    "    if os.path.isabs(ref) and os.path.exists(ref):\n",
    "        return ref\n",
    "\n",
    "    candidates = [ref]\n",
    "    if not os.path.splitext(ref)[1]:\n",
    "        candidates += [f\"{ref}.png\", f\"{ref}.jpg\", f\"{ref}.jpeg\", f\"{ref}.webp\"]\n",
    "\n",
    "    for base_dir in INPUT_IMAGE_DIRS:\n",
    "        if not os.path.isdir(base_dir):\n",
    "            continue\n",
    "        for candidate in candidates:\n",
    "            full = os.path.join(base_dir, candidate)\n",
    "            if os.path.exists(full):\n",
    "                return full\n",
    "\n",
    "    for pattern in [f\"**/{ref}\", f\"**/{ref}.*\"]:\n",
    "        for base_dir in INPUT_IMAGE_DIRS:\n",
    "            if not os.path.isdir(base_dir):\n",
    "                continue\n",
    "            matches = glob.glob(os.path.join(base_dir, pattern), recursive=True)\n",
    "            if matches:\n",
    "                return matches[0]\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def image_to_base64(image_ref):\n",
    "    if image_ref is None:\n",
    "        return None\n",
    "    if isinstance(image_ref, str) and image_ref.startswith(\"data:image\"):\n",
    "        return image_ref\n",
    "\n",
    "    image_path = resolve_image_path(image_ref)\n",
    "    if not image_path:\n",
    "        raise FileNotFoundError(f\"Image/Mask not found: {image_ref}. Search dirs: {INPUT_IMAGE_DIRS}\")\n",
    "\n",
    "    ext = os.path.splitext(image_path)[1].lower()\n",
    "    mime = {\n",
    "        \".png\": \"image/png\",\n",
    "        \".jpg\": \"image/jpeg\",\n",
    "        \".jpeg\": \"image/jpeg\",\n",
    "        \".webp\": \"image/webp\",\n",
    "        \".bmp\": \"image/bmp\",\n",
    "    }.get(ext, \"application/octet-stream\")\n",
    "\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        b64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "    return f\"data:{mime};base64,{b64}\"\n",
    "\n",
    "\n",
    "def parse_override_settings(value):\n",
    "    if value in (None, \"\", {}):\n",
    "        return {}\n",
    "    if isinstance(value, dict):\n",
    "        return value\n",
    "    if isinstance(value, str):\n",
    "        txt = value.strip()\n",
    "        if not txt:\n",
    "            return {}\n",
    "        if txt.startswith(\"{\"):\n",
    "            return json.loads(txt)\n",
    "        return json.loads(\"{\" + txt + \"}\")\n",
    "    raise ValueError(f\"Unsupported override_settings format: {type(value)}\")\n",
    "\n",
    "def deep_clean(value):\n",
    "    if isinstance(value, dict):\n",
    "        cleaned = {}\n",
    "        for k, v in value.items():\n",
    "            cv = deep_clean(v)\n",
    "            if cv is None:\n",
    "                continue\n",
    "            if isinstance(cv, str) and not cv.strip():\n",
    "                continue\n",
    "            if isinstance(cv, dict) and not cv:\n",
    "                continue\n",
    "            cleaned[k] = cv\n",
    "        return cleaned\n",
    "\n",
    "    if isinstance(value, list):\n",
    "        cleaned_list = []\n",
    "        for item in value:\n",
    "            ci = deep_clean(item)\n",
    "            if ci is None:\n",
    "                continue\n",
    "            if isinstance(ci, str) and not ci.strip():\n",
    "                continue\n",
    "            if isinstance(ci, dict) and not ci:\n",
    "                continue\n",
    "            cleaned_list.append(ci)\n",
    "        return cleaned_list\n",
    "\n",
    "    if isinstance(value, str):\n",
    "        stripped = value.strip()\n",
    "        if stripped == \"[]\":\n",
    "            return []\n",
    "        if stripped.lower() in {\"\", \"null\", \"none\", \"nan\"}:\n",
    "            return None\n",
    "        return stripped\n",
    "\n",
    "    return value\n",
    "\n",
    "\n",
    "def dump_payload(payload, generation_idx):\n",
    "    dump_path = os.path.join(REQUEST_DUMPS_DIR, f\"request_{generation_idx:04d}.json\")\n",
    "    with open(dump_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, ensure_ascii=False, indent=2)\n",
    "    return dump_path\n",
    "\n",
    "\n",
    "def build_payload(row_values, instruction_template):\n",
    "    payload = dict(row_values)\n",
    "\n",
    "    if \"prompt\" not in payload:\n",
    "        payload[\"prompt\"] = render_instruction(instruction_template, row_values)\n",
    "\n",
    "    payload = {k: v for k, v in payload.items() if v is not None}\n",
    "\n",
    "    if \"override_settings\" in payload:\n",
    "        payload[\"override_settings\"] = parse_override_settings(payload.get(\"override_settings\"))\n",
    "\n",
    "    controlnet_args = []\n",
    "    for idx in (1, 2, 3):\n",
    "        unit = {}\n",
    "        for field in CONTROLNET_FIELDS:\n",
    "            key = f\"cn{idx}_{field}\"\n",
    "            if key in payload:\n",
    "                unit[field] = payload.pop(key)\n",
    "\n",
    "        if not unit:\n",
    "            continue\n",
    "\n",
    "        if \"enabled\" not in unit:\n",
    "            unit[\"enabled\"] = True\n",
    "\n",
    "        if unit.get(\"image\") is not None:\n",
    "            unit[\"image\"] = image_to_base64(unit.get(\"image\"))\n",
    "        if unit.get(\"mask\") is not None:\n",
    "            unit[\"mask\"] = image_to_base64(unit.get(\"mask\"))\n",
    "\n",
    "        unit = deep_clean(unit)\n",
    "        if unit.get(\"enabled\"):\n",
    "            controlnet_args.append(unit)\n",
    "\n",
    "    if controlnet_args:\n",
    "        payload.setdefault(\"alwayson_scripts\", {})\n",
    "        payload[\"alwayson_scripts\"][\"ControlNet\"] = {\"args\": controlnet_args}\n",
    "\n",
    "    payload = deep_clean(payload)\n",
    "    payload = apply_conflict_rules(payload)\n",
    "    payload = deep_clean(payload)\n",
    "    return payload\n",
    "\n",
    "\n",
    "def expected_image_count(payload):\n",
    "    batch_size = int(payload.get(\"batch_size\", 1) or 1)\n",
    "    n_iter = int(payload.get(\"n_iter\", 1) or 1)\n",
    "    return max(1, batch_size) * max(1, n_iter)\n",
    "\n",
    "\n",
    "def save_images_from_response(images_b64, generation_idx):\n",
    "    saved = 0\n",
    "    for i, b64 in enumerate(images_b64, start=1):\n",
    "        image_data = b64.split(\",\", 1)[-1]\n",
    "        file_name = f\"gen_{generation_idx:04d}_{i:02d}.png\"\n",
    "        file_path = os.path.join(API_IMAGES_DIR, file_name)\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            f.write(base64.b64decode(image_data))\n",
    "        saved += 1\n",
    "    return saved\n",
    "\n",
    "\n",
    "def archive_outputs(tag: str):\n",
    "    stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    archive_name = os.path.join(VOLUME_DIR, f\"outputs_{tag}_{stamp}.zip\")\n",
    "    with zipfile.ZipFile(archive_name, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "        for root, _, files in os.walk(OUTPUTS_DIR):\n",
    "            for file in files:\n",
    "                full_path = os.path.join(root, file)\n",
    "                rel_path = os.path.relpath(full_path, OUTPUTS_DIR)\n",
    "                zf.write(full_path, rel_path)\n",
    "\n",
    "    for entry in os.listdir(OUTPUTS_DIR):\n",
    "        p = os.path.join(OUTPUTS_DIR, entry)\n",
    "        if os.path.isdir(p):\n",
    "            shutil.rmtree(p)\n",
    "        else:\n",
    "            os.remove(p)\n",
    "    os.makedirs(API_IMAGES_DIR, exist_ok=True)\n",
    "    log_line(f\"ARCHIVE created: {archive_name}. OUTPUTS_DIR cleaned.\")\n",
    "\n",
    "\n",
    "wait_for_api_ready(API_BASE)\n",
    "\n",
    "prompts_path = first_existing_path(PROMPTS_CANDIDATES)\n",
    "if not prompts_path:\n",
    "    raise FileNotFoundError(\n",
    "        \"Файл prompts.xlsx/prompts.xlxs не найден. Ожидались пути: \" + \", \".join(PROMPTS_CANDIDATES)\n",
    "    )\n",
    "\n",
    "instruction_template, generation_rows = parse_workbook(prompts_path)\n",
    "log_line(f\"Loaded prompts file: {prompts_path}. Rows for generation: {len(generation_rows)}\")\n",
    "log_line(f\"Image search dirs: {INPUT_IMAGE_DIRS}\")\n",
    "\n",
    "syntax_error_dumped = False\n",
    "images_since_archive = 0\n",
    "\n",
    "total_expected_images = 0\n",
    "total_saved_images = 0\n",
    "\n",
    "for generation_idx, row_values in enumerate(generation_rows, start=1):\n",
    "    try:\n",
    "        payload = build_payload(row_values, instruction_template)\n",
    "    except Exception as e:\n",
    "        log_line(f\"#{generation_idx} FAIL: payload build error: {e}\")\n",
    "        continue\n",
    "\n",
    "    planned_images = expected_image_count(payload)\n",
    "    total_expected_images += planned_images\n",
    "\n",
    "    payload_dump_path = dump_payload(payload, generation_idx)\n",
    "    log_line(f\"#{generation_idx} request dump: {payload_dump_path}\")\n",
    "    if \"alwayson_scripts\" not in payload or not payload.get(\"alwayson_scripts\", {}).get(\"ControlNet\", {}).get(\"args\"):\n",
    "        log_line(f\"#{generation_idx} INFO: no ControlNet image/mask attached in payload\")\n",
    "\n",
    "    try:\n",
    "        response = requests.post(TXT2IMG_URL, json=payload, timeout=1800)\n",
    "        if response.status_code >= 400:\n",
    "            err_text = response.text[:1200]\n",
    "            if not syntax_error_dumped:\n",
    "                params_dump = fetch_txt2img_params_dump()\n",
    "                log_line(f\"#{generation_idx} FAIL syntax/validation: {response.status_code} {err_text}\")\n",
    "                log_line(f\"#{generation_idx} FAIL payload: {payload_dump_path}\")\n",
    "                log_line(\"AVAILABLE PARAMS DUMP START\")\n",
    "                with open(LOG_PATH, \"a\", encoding=\"utf-8\") as f:\n",
    "                    f.write(params_dump + \"\\n\")\n",
    "                log_line(\"AVAILABLE PARAMS DUMP END\")\n",
    "                syntax_error_dumped = True\n",
    "            else:\n",
    "                log_line(f\"#{generation_idx} FAIL: {response.status_code} {err_text}\")\n",
    "                log_line(f\"#{generation_idx} FAIL payload: {payload_dump_path}\")\n",
    "            continue\n",
    "\n",
    "        result = response.json()\n",
    "        images = result.get(\"images\", []) or []\n",
    "        saved_now = save_images_from_response(images, generation_idx)\n",
    "        total_saved_images += saved_now\n",
    "        images_since_archive += saved_now\n",
    "\n",
    "        log_line(f\"#{generation_idx} OK images={saved_now} expected={planned_images}\")\n",
    "\n",
    "        if images_since_archive >= 15:\n",
    "            archive_outputs(tag=f\"part_{generation_idx:04d}\")\n",
    "            images_since_archive = 0\n",
    "\n",
    "    except requests.exceptions.Timeout:\n",
    "        log_line(f\"#{generation_idx} FAIL: timeout (possible heavy generation or API freeze)\")\n",
    "        log_line(f\"#{generation_idx} FAIL payload: {payload_dump_path}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        reason = str(e)\n",
    "        if \"out of memory\" in reason.lower() or \"oom\" in reason.lower():\n",
    "            reason = \"OOM\"\n",
    "        log_line(f\"#{generation_idx} FAIL: {reason}\")\n",
    "        log_line(f\"#{generation_idx} FAIL payload: {payload_dump_path}\")\n",
    "    except Exception as e:\n",
    "        log_line(f\"#{generation_idx} FAIL: unexpected error: {e}\")\n",
    "        log_line(f\"#{generation_idx} FAIL payload: {payload_dump_path}\")\n",
    "\n",
    "if images_since_archive > 0:\n",
    "    archive_outputs(tag=\"final\")\n",
    "\n",
    "log_line(f\"Generation cycle completed. Requests={len(generation_rows)} expected_images={total_expected_images} saved_images={total_saved_images}\")\n",
    "\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-17T20:13:34.663864Z",
     "iopub.execute_input": "2026-02-17T20:13:34.664227Z",
     "iopub.status.idle": "2026-02-17T20:16:23.213795Z",
     "shell.execute_reply.started": "2026-02-17T20:13:34.664195Z",
     "shell.execute_reply": "2026-02-17T20:16:23.212907Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "[2026-02-17 20:16:22] Loaded prompts file: /kaggle/input/datasets/sokolenkotimofei/prompts/Prompts.xlsx. Rows for generation: 14\n[2026-02-17 20:16:22] Image search dirs: ['/kaggle/working/gen/Images', '/kaggle/working/gen/images', '/kaggle/input/datasets/sokolenkotimofei/images']\n[2026-02-17 20:16:22] #1 request dump: /kaggle/working/volume/request_dumps/request_0001.json\n[2026-02-17 20:16:22] #1 INFO: no ControlNet image/mask attached in payload\n[2026-02-17 20:16:23] #1 FAIL syntax/validation: 422 {\"detail\":[{\"type\":\"list_type\",\"loc\":[\"body\",\"hr_additional_modules\"],\"msg\":\"Input should be a valid list\",\"input\":\"[]\",\"url\":\"https://errors.pydantic.dev/2.8/v/list_type\"}]}\n[2026-02-17 20:16:23] #1 FAIL payload: /kaggle/working/volume/request_dumps/request_0001.json\n[2026-02-17 20:16:23] AVAILABLE PARAMS DUMP START\n[2026-02-17 20:16:23] AVAILABLE PARAMS DUMP END\n[2026-02-17 20:16:23] #2 request dump: /kaggle/working/volume/request_dumps/request_0002.json\n[2026-02-17 20:16:23] #2 INFO: no ControlNet image/mask attached in payload\n[2026-02-17 20:16:23] #2 FAIL: 422 {\"detail\":[{\"type\":\"list_type\",\"loc\":[\"body\",\"hr_additional_modules\"],\"msg\":\"Input should be a valid list\",\"input\":\"[]\",\"url\":\"https://errors.pydantic.dev/2.8/v/list_type\"}]}\n[2026-02-17 20:16:23] #2 FAIL payload: /kaggle/working/volume/request_dumps/request_0002.json\n[2026-02-17 20:16:23] #3 request dump: /kaggle/working/volume/request_dumps/request_0003.json\n[2026-02-17 20:16:23] #3 INFO: no ControlNet image/mask attached in payload\n[2026-02-17 20:16:23] #3 FAIL: 422 {\"detail\":[{\"type\":\"list_type\",\"loc\":[\"body\",\"hr_additional_modules\"],\"msg\":\"Input should be a valid list\",\"input\":\"[]\",\"url\":\"https://errors.pydantic.dev/2.8/v/list_type\"}]}\n[2026-02-17 20:16:23] #3 FAIL payload: /kaggle/working/volume/request_dumps/request_0003.json\n[2026-02-17 20:16:23] #4 request dump: /kaggle/working/volume/request_dumps/request_0004.json\n[2026-02-17 20:16:23] #4 INFO: no ControlNet image/mask attached in payload\n[2026-02-17 20:16:23] #4 FAIL: 422 {\"detail\":[{\"type\":\"list_type\",\"loc\":[\"body\",\"hr_additional_modules\"],\"msg\":\"Input should be a valid list\",\"input\":\"[]\",\"url\":\"https://errors.pydantic.dev/2.8/v/list_type\"}]}\n[2026-02-17 20:16:23] #4 FAIL payload: /kaggle/working/volume/request_dumps/request_0004.json\n[2026-02-17 20:16:23] #5 request dump: /kaggle/working/volume/request_dumps/request_0005.json\n[2026-02-17 20:16:23] #5 INFO: no ControlNet image/mask attached in payload\n[2026-02-17 20:16:23] #5 FAIL: 422 {\"detail\":[{\"type\":\"list_type\",\"loc\":[\"body\",\"hr_additional_modules\"],\"msg\":\"Input should be a valid list\",\"input\":\"[]\",\"url\":\"https://errors.pydantic.dev/2.8/v/list_type\"}]}\n[2026-02-17 20:16:23] #5 FAIL payload: /kaggle/working/volume/request_dumps/request_0005.json\n[2026-02-17 20:16:23] #6 request dump: /kaggle/working/volume/request_dumps/request_0006.json\n[2026-02-17 20:16:23] #6 INFO: no ControlNet image/mask attached in payload\n[2026-02-17 20:16:23] #6 FAIL: 422 {\"detail\":[{\"type\":\"list_type\",\"loc\":[\"body\",\"hr_additional_modules\"],\"msg\":\"Input should be a valid list\",\"input\":\"[]\",\"url\":\"https://errors.pydantic.dev/2.8/v/list_type\"}]}\n[2026-02-17 20:16:23] #6 FAIL payload: /kaggle/working/volume/request_dumps/request_0006.json\n[2026-02-17 20:16:23] #7 request dump: /kaggle/working/volume/request_dumps/request_0007.json\n[2026-02-17 20:16:23] #7 INFO: no ControlNet image/mask attached in payload\n[2026-02-17 20:16:23] #7 FAIL: 422 {\"detail\":[{\"type\":\"list_type\",\"loc\":[\"body\",\"hr_additional_modules\"],\"msg\":\"Input should be a valid list\",\"input\":\"[]\",\"url\":\"https://errors.pydantic.dev/2.8/v/list_type\"}]}\n[2026-02-17 20:16:23] #7 FAIL payload: /kaggle/working/volume/request_dumps/request_0007.json\n[2026-02-17 20:16:23] #8 request dump: /kaggle/working/volume/request_dumps/request_0008.json\n[2026-02-17 20:16:23] #8 INFO: no ControlNet image/mask attached in payload\n[2026-02-17 20:16:23] #8 FAIL: 422 {\"detail\":[{\"type\":\"list_type\",\"loc\":[\"body\",\"hr_additional_modules\"],\"msg\":\"Input should be a valid list\",\"input\":\"[]\",\"url\":\"https://errors.pydantic.dev/2.8/v/list_type\"}]}\n[2026-02-17 20:16:23] #8 FAIL payload: /kaggle/working/volume/request_dumps/request_0008.json\n[2026-02-17 20:16:23] #9 request dump: /kaggle/working/volume/request_dumps/request_0009.json\n[2026-02-17 20:16:23] #9 INFO: no ControlNet image/mask attached in payload\n[2026-02-17 20:16:23] #9 FAIL: 422 {\"detail\":[{\"type\":\"list_type\",\"loc\":[\"body\",\"hr_additional_modules\"],\"msg\":\"Input should be a valid list\",\"input\":\"[]\",\"url\":\"https://errors.pydantic.dev/2.8/v/list_type\"}]}\n[2026-02-17 20:16:23] #9 FAIL payload: /kaggle/working/volume/request_dumps/request_0009.json\n[2026-02-17 20:16:23] #10 request dump: /kaggle/working/volume/request_dumps/request_0010.json\n[2026-02-17 20:16:23] #10 INFO: no ControlNet image/mask attached in payload\n[2026-02-17 20:16:23] #10 FAIL: 422 {\"detail\":[{\"type\":\"list_type\",\"loc\":[\"body\",\"hr_additional_modules\"],\"msg\":\"Input should be a valid list\",\"input\":\"[]\",\"url\":\"https://errors.pydantic.dev/2.8/v/list_type\"}]}\n[2026-02-17 20:16:23] #10 FAIL payload: /kaggle/working/volume/request_dumps/request_0010.json\n[2026-02-17 20:16:23] #11 request dump: /kaggle/working/volume/request_dumps/request_0011.json\n[2026-02-17 20:16:23] #11 INFO: no ControlNet image/mask attached in payload\n[2026-02-17 20:16:23] #11 FAIL: 422 {\"detail\":[{\"type\":\"list_type\",\"loc\":[\"body\",\"hr_additional_modules\"],\"msg\":\"Input should be a valid list\",\"input\":\"[]\",\"url\":\"https://errors.pydantic.dev/2.8/v/list_type\"}]}\n[2026-02-17 20:16:23] #11 FAIL payload: /kaggle/working/volume/request_dumps/request_0011.json\n[2026-02-17 20:16:23] #12 request dump: /kaggle/working/volume/request_dumps/request_0012.json\n[2026-02-17 20:16:23] #12 INFO: no ControlNet image/mask attached in payload\n[2026-02-17 20:16:23] #12 FAIL: 422 {\"detail\":[{\"type\":\"list_type\",\"loc\":[\"body\",\"hr_additional_modules\"],\"msg\":\"Input should be a valid list\",\"input\":\"[]\",\"url\":\"https://errors.pydantic.dev/2.8/v/list_type\"}]}\n[2026-02-17 20:16:23] #12 FAIL payload: /kaggle/working/volume/request_dumps/request_0012.json\n[2026-02-17 20:16:23] #13 request dump: /kaggle/working/volume/request_dumps/request_0013.json\n[2026-02-17 20:16:23] #13 INFO: no ControlNet image/mask attached in payload\n[2026-02-17 20:16:23] #13 FAIL: 422 {\"detail\":[{\"type\":\"list_type\",\"loc\":[\"body\",\"hr_additional_modules\"],\"msg\":\"Input should be a valid list\",\"input\":\"[]\",\"url\":\"https://errors.pydantic.dev/2.8/v/list_type\"}]}\n[2026-02-17 20:16:23] #13 FAIL payload: /kaggle/working/volume/request_dumps/request_0013.json\n[2026-02-17 20:16:23] #14 request dump: /kaggle/working/volume/request_dumps/request_0014.json\n[2026-02-17 20:16:23] #14 INFO: no ControlNet image/mask attached in payload\n[2026-02-17 20:16:23] #14 FAIL: 422 {\"detail\":[{\"type\":\"list_type\",\"loc\":[\"body\",\"hr_additional_modules\"],\"msg\":\"Input should be a valid list\",\"input\":\"[]\",\"url\":\"https://errors.pydantic.dev/2.8/v/list_type\"}]}\n[2026-02-17 20:16:23] #14 FAIL payload: /kaggle/working/volume/request_dumps/request_0014.json\n[2026-02-17 20:16:23] Generation cycle completed. Requests=14 expected_images=18 saved_images=0\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": "import requests\n\nurl = \"http://127.0.0.1:17860\" # замените на ваш адрес\n\n# Получить список текущих настроек (options)\noptions = requests.get(f'{url}/sdapi/v1/options').json()\nprint(options.keys()) # Выведет все доступные имена переменных для настроек\n\n# Получить данные о текущей модели\nmodels = requests.get(f'{url}/sdapi/v1/sd-models').json()\nprint(models)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-17T20:16:23.214791Z",
     "iopub.execute_input": "2026-02-17T20:16:23.215106Z",
     "iopub.status.idle": "2026-02-17T20:16:23.247247Z",
     "shell.execute_reply.started": "2026-02-17T20:16:23.215072Z",
     "shell.execute_reply": "2026-02-17T20:16:23.246579Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "dict_keys(['samples_save', 'samples_format', 'samples_filename_pattern', 'save_images_add_number', 'save_images_replace_action', 'grid_save', 'grid_format', 'grid_extended_filename', 'grid_only_if_multiple', 'grid_prevent_empty_spots', 'grid_zip_filename_pattern', 'n_rows', 'font', 'grid_text_active_color', 'grid_text_inactive_color', 'grid_background_color', 'save_images_before_face_restoration', 'save_images_before_highres_fix', 'save_images_before_color_correction', 'save_mask', 'save_mask_composite', 'jpeg_quality', 'webp_lossless', 'export_for_4chan', 'img_downscale_threshold', 'target_side_length', 'img_max_size_mp', 'use_original_name_batch', 'use_upscaler_name_as_suffix', 'save_selected_only', 'save_write_log_csv', 'save_init_img', 'temp_dir', 'clean_temp_dir_at_start', 'save_incomplete_images', 'notification_audio', 'notification_volume', 'outdir_samples', 'outdir_txt2img_samples', 'outdir_img2img_samples', 'outdir_extras_samples', 'outdir_grids', 'outdir_txt2img_grids', 'outdir_img2img_grids', 'outdir_save', 'outdir_init_images', 'save_to_dirs', 'grid_save_to_dirs', 'use_save_to_dirs_for_ui', 'directories_filename_pattern', 'directories_max_prompt_words', 'ESRGAN_tile', 'ESRGAN_tile_overlap', 'realesrgan_enabled_models', 'dat_enabled_models', 'DAT_tile', 'DAT_tile_overlap', 'upscaler_for_img2img', 'set_scale_by_when_changing_upscaler', 'face_restoration', 'face_restoration_model', 'code_former_weight', 'face_restoration_unload', 'auto_launch_browser', 'enable_console_prompts', 'show_warnings', 'show_gradio_deprecation_warnings', 'memmon_poll_rate', 'samples_log_stdout', 'multiple_tqdm', 'enable_upscale_progressbar', 'print_hypernet_extra', 'list_hidden_files', 'disable_mmap_load_safetensors', 'hide_ldm_prints', 'dump_stacks_on_signal', 'profiling_explanation', 'profiling_enable', 'profiling_activities', 'profiling_record_shapes', 'profiling_profile_memory', 'profiling_with_stack', 'profiling_filename', 'api_enable_requests', 'api_forbid_local_requests', 'api_useragent', 'unload_models_when_training', 'pin_memory', 'save_optimizer_state', 'save_training_settings_to_txt', 'dataset_filename_word_regex', 'dataset_filename_join_string', 'training_image_repeats_per_epoch', 'training_write_csv_every', 'training_xattention_optimizations', 'training_enable_tensorboard', 'training_tensorboard_save_images', 'training_tensorboard_flush_every', 'sd_model_checkpoint', 'sd_checkpoints_limit', 'sd_checkpoints_keep_in_cpu', 'sd_checkpoint_cache', 'sd_unet', 'enable_quantization', 'emphasis', 'enable_batch_seeds', 'comma_padding_backtrack', 'sdxl_clip_l_skip', 'CLIP_stop_at_last_layers', 'upcast_attn', 'randn_source', 'tiling', 'hires_fix_refiner_pass', 'sdxl_crop_top', 'sdxl_crop_left', 'sdxl_refiner_low_aesthetic_score', 'sdxl_refiner_high_aesthetic_score', 'sd3_enable_t5', 'sd_vae_explanation', 'sd_vae_checkpoint_cache', 'sd_vae', 'sd_vae_overrides_per_model_preferences', 'auto_vae_precision_bfloat16', 'auto_vae_precision', 'sd_vae_encode_method', 'sd_vae_decode_method', 'inpainting_mask_weight', 'initial_noise_multiplier', 'img2img_extra_noise', 'img2img_color_correction', 'img2img_fix_steps', 'img2img_background_color', 'img2img_sketch_default_brush_color', 'img2img_inpaint_mask_brush_color', 'img2img_inpaint_sketch_default_brush_color', 'img2img_inpaint_mask_high_contrast', 'img2img_inpaint_mask_scribble_alpha', 'return_mask', 'return_mask_composite', 'img2img_batch_show_results_limit', 'overlay_inpaint', 'img2img_autosize', 'img2img_batch_use_original_name', 'cross_attention_optimization', 's_min_uncond', 's_min_uncond_all', 'token_merging_ratio', 'token_merging_ratio_img2img', 'token_merging_ratio_hr', 'pad_cond_uncond', 'pad_cond_uncond_v0', 'persistent_cond_cache', 'batch_cond_uncond', 'fp8_storage', 'cache_fp16_weight', 'forge_try_reproduce', 'auto_backcompat', 'use_old_emphasis_implementation', 'use_old_karras_scheduler_sigmas', 'no_dpmpp_sde_batch_determinism', 'use_old_hires_fix_width_height', 'hires_fix_use_firstpass_conds', 'use_old_scheduling', 'use_downcasted_alpha_bar', 'refiner_switch_by_sample_steps', 'interrogate_keep_models_in_memory', 'interrogate_return_ranks', 'interrogate_clip_num_beams', 'interrogate_clip_min_length', 'interrogate_clip_max_length', 'interrogate_clip_dict_limit', 'interrogate_clip_skip_categories', 'interrogate_deepbooru_score_threshold', 'deepbooru_sort_alpha', 'deepbooru_use_spaces', 'deepbooru_escape', 'deepbooru_filter_tags', 'extra_networks_show_hidden_directories', 'extra_networks_dir_button_function', 'extra_networks_hidden_models', 'extra_networks_default_multiplier', 'extra_networks_card_width', 'extra_networks_card_height', 'extra_networks_card_text_scale', 'extra_networks_card_show_desc', 'extra_networks_card_description_is_html', 'extra_networks_card_order_field', 'extra_networks_card_order', 'extra_networks_tree_view_style', 'extra_networks_tree_view_default_enabled', 'extra_networks_tree_view_default_width', 'extra_networks_add_text_separator', 'ui_extra_networks_tab_reorder', 'textual_inversion_print_at_load', 'textual_inversion_add_hashes_to_infotext', 'sd_hypernetwork', 'keyedit_precision_attention', 'keyedit_precision_extra', 'keyedit_delimiters', 'keyedit_delimiters_whitespace', 'keyedit_move', 'disable_token_counters', 'include_styles_into_token_counters', 'return_grid', 'do_not_show_images', 'js_modal_lightbox', 'js_modal_lightbox_initially_zoomed', 'js_modal_lightbox_gamepad', 'js_modal_lightbox_gamepad_repeat', 'sd_webui_modal_lightbox_icon_opacity', 'sd_webui_modal_lightbox_toolbar_opacity', 'gallery_height', 'open_dir_button_choice', 'hires_button_gallery_insert', 'compact_prompt_box', 'samplers_in_dropdown', 'dimensions_and_batch_together', 'sd_checkpoint_dropdown_use_short', 'hires_fix_show_sampler', 'hires_fix_show_prompts', 'txt2img_settings_accordion', 'img2img_settings_accordion', 'interrupt_after_current', 'localization', 'quick_setting_list', 'ui_tab_order', 'hidden_tabs', 'tabs_without_quick_settings_bar', 'ui_reorder_list', 'gradio_theme', 'gradio_themes_cache', 'show_progress_in_title', 'send_seed', 'send_size', 'enable_reloading_ui_scripts', 'infotext_explanation', 'enable_pnginfo', 'stealth_pnginfo_option', 'save_txt', 'add_model_name_to_info', 'add_model_hash_to_info', 'add_vae_name_to_info', 'add_vae_hash_to_info', 'add_user_name_to_info', 'add_version_to_infotext', 'disable_weights_auto_swap', 'infotext_skip_pasting', 'infotext_styles', 'show_progressbar', 'live_previews_enable', 'live_previews_image_format', 'show_progress_grid', 'show_progress_every_n_steps', 'show_progress_type', 'live_preview_allow_lowvram_full', 'live_preview_content', 'live_preview_refresh_period', 'live_preview_fast_interrupt', 'js_live_preview_in_modal_lightbox', 'prevent_screen_sleep_during_generation', 'hide_samplers', 'eta_ddim', 'eta_ancestral', 'ddim_discretize', 's_churn', 's_tmin', 's_tmax', 's_noise', 'sigma_min', 'sigma_max', 'rho', 'eta_noise_seed_delta', 'always_discard_next_to_last_sigma', 'sgm_noise_multiplier', 'uni_pc_variant', 'uni_pc_skip_type', 'uni_pc_order', 'uni_pc_lower_order_final', 'sd_noise_schedule', 'skip_early_cond', 'beta_dist_alpha', 'beta_dist_beta', 'postprocessing_enable_in_main_ui', 'postprocessing_disable_in_extras', 'postprocessing_operation_order', 'upscaling_max_images_in_cache', 'postprocessing_existing_caption_action', 'disabled_extensions', 'disable_all_extensions', 'restore_config_state_file', 'sd_checkpoint_hash', 'forge_unet_storage_dtype', 'forge_inference_memory', 'forge_async_loading', 'forge_pin_shared_memory', 'forge_preset', 'forge_additional_modules', 'forge_canvas_plain', 'forge_canvas_toolbar_always', 'enable_prompt_comments', 'sd_t2i_width', 'sd_t2i_height', 'sd_t2i_cfg', 'sd_t2i_hr_cfg', 'sd_i2i_width', 'sd_i2i_height', 'sd_i2i_cfg', 'xl_t2i_width', 'xl_t2i_height', 'xl_t2i_cfg', 'xl_t2i_hr_cfg', 'xl_i2i_width', 'xl_i2i_height', 'xl_i2i_cfg', 'xl_GPU_MB', 'flux_t2i_width', 'flux_t2i_height', 'flux_t2i_cfg', 'flux_t2i_hr_cfg', 'flux_t2i_d_cfg', 'flux_t2i_hr_d_cfg', 'flux_i2i_width', 'flux_i2i_height', 'flux_i2i_cfg', 'flux_i2i_d_cfg', 'flux_GPU_MB', 'settings_in_ui', 'extra_options_txt2img', 'extra_options_img2img', 'extra_options_cols', 'extra_options_accordion'])\n[{'title': 'wai_v160.safetensors', 'model_name': 'wai_v160', 'hash': None, 'sha256': None, 'filename': '/kaggle/working/stable-diffusion-webui-forge/models/Stable-diffusion/wai_v160.safetensors', 'config': None}]\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 7
  }
 ]
}
