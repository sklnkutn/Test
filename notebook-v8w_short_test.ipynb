{
 "metadata": {
  "kernelspec": {
   "name": "python",
   "display_name": "Python (Pyodide)",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8"
  },
  "accelerator": "GPU"
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preflight diagnostics (non-blocking): network + GPU/PyTorch compatibility\n",
    "import sys\n",
    "import re\n",
    "import subprocess\n",
    "import importlib\n",
    "\n",
    "\n",
    "def section(title):\n",
    "    print()\n",
    "    print(\"=\" * 80)\n",
    "    print(title)\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "\n",
    "def run_command_live(cmd, desc=None):\n",
    "    if desc:\n",
    "        print(f\"[RUN] {desc}\")\n",
    "    print(\"[CMD]\", \" \".join(cmd))\n",
    "    proc = subprocess.Popen(\n",
    "        cmd,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT,\n",
    "        text=True,\n",
    "        bufsize=1,\n",
    "    )\n",
    "    lines = []\n",
    "    if proc.stdout is not None:\n",
    "        for line in proc.stdout:\n",
    "            lines.append(line)\n",
    "            print(line, end=\"\")\n",
    "    code = proc.wait()\n",
    "    print(f\"[EXIT CODE] {code}\")\n",
    "    return code, \"\".join(lines)\n",
    "\n",
    "\n",
    "def ensure_speedtest_cli():\n",
    "    if importlib.util.find_spec(\"speedtest\") is not None:\n",
    "        print(\"speedtest-cli module: already installed\")\n",
    "        return True\n",
    "\n",
    "    print(\"speedtest-cli module: not found; attempting installation with live logs...\")\n",
    "    code, _ = run_command_live([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"speedtest-cli\"], \"Installing speedtest-cli\")\n",
    "    if code != 0:\n",
    "        print(\"[WARN] Could not install speedtest-cli. Internet speed measurements will be skipped.\")\n",
    "        return False\n",
    "\n",
    "    ok = importlib.util.find_spec(\"speedtest\") is not None\n",
    "    print(\"speedtest-cli install status:\", \"OK\" if ok else \"FAILED\")\n",
    "    return ok\n",
    "\n",
    "\n",
    "def run_speedtest():\n",
    "    results = {\"download_mbps\": None, \"upload_mbps\": None, \"error\": None}\n",
    "    try:\n",
    "        import speedtest\n",
    "\n",
    "        print(\"Running speedtest-cli (this can take ~20-60s)...\")\n",
    "        st = speedtest.Speedtest(secure=True)\n",
    "        print(\"- Getting best server...\")\n",
    "        st.get_best_server()\n",
    "        print(\"- Testing download speed...\")\n",
    "        dl_bps = st.download()\n",
    "        print(\"- Testing upload speed...\")\n",
    "        ul_bps = st.upload()\n",
    "\n",
    "        results[\"download_mbps\"] = dl_bps / 1_000_000\n",
    "        results[\"upload_mbps\"] = ul_bps / 1_000_000\n",
    "    except Exception as e:\n",
    "        results[\"error\"] = str(e)\n",
    "    return results\n",
    "\n",
    "\n",
    "def avg_ping(host):\n",
    "    # Uses Linux ping summary line: rtt min/avg/max/mdev = ...\n",
    "    cmd = [\"ping\", \"-c\", \"3\", \"-W\", \"2\", host]\n",
    "    print(f\"\\nPinging {host}...\")\n",
    "    code, out = run_command_live(cmd)\n",
    "    if code != 0:\n",
    "        return None, f\"ping command failed with code {code}\"\n",
    "\n",
    "    m = re.search(r\"=\\s*([\\d\\.]+)/([\\d\\.]+)/([\\d\\.]+)/([\\d\\.]+)\", out)\n",
    "    if not m:\n",
    "        return None, \"unable to parse ping output\"\n",
    "    return float(m.group(2)), None\n",
    "\n",
    "\n",
    "def parse_version_tuple(v):\n",
    "    try:\n",
    "        core = re.match(r\"(\\d+)\\.(\\d+)\", str(v))\n",
    "        if not core:\n",
    "            return (0, 0)\n",
    "        return (int(core.group(1)), int(core.group(2)))\n",
    "    except Exception:\n",
    "        return (0, 0)\n",
    "\n",
    "\n",
    "def parse_semver_tuple(v):\n",
    "    match = re.match(r\"(\\d+)\\.(\\d+)\\.(\\d+)\", str(v))\n",
    "    if not match:\n",
    "        return (0, 0, 0)\n",
    "    return tuple(int(match.group(i)) for i in range(1, 4))\n",
    "\n",
    "\n",
    "def ensure_pillow_compatibility():\n",
    "    try:\n",
    "        from PIL import __version__ as pillow_version\n",
    "    except Exception:\n",
    "        print(\"Pillow version is compatible (below 12)\")\n",
    "        return\n",
    "\n",
    "    if parse_semver_tuple(pillow_version) >= (12, 0, 0):\n",
    "        code, _ = run_command_live([sys.executable, \"-m\", \"pip\", \"install\", \"Pillow==11.1.0\"], \"Downgrading Pillow to 11.1.0\")\n",
    "        print(\"Pillow downgraded to 11.1.0\")\n",
    "    else:\n",
    "        print(\"Pillow version is compatible (below 12)\")\n",
    "\n",
    "\n",
    "ensure_pillow_compatibility()\n",
    "\n",
    "section(\"1) Internet connection checks (non-blocking)\")\n",
    "can_speedtest = ensure_speedtest_cli()\n",
    "net = {\"download_mbps\": None, \"upload_mbps\": None, \"civitai_ping_ms\": None, \"hf_ping_ms\": None}\n",
    "\n",
    "if can_speedtest:\n",
    "    sp = run_speedtest()\n",
    "    net[\"download_mbps\"] = sp.get(\"download_mbps\")\n",
    "    net[\"upload_mbps\"] = sp.get(\"upload_mbps\")\n",
    "    if sp.get(\"error\"):\n",
    "        print(\"[WARN] speedtest failed:\", sp[\"error\"])\n",
    "else:\n",
    "    print(\"[WARN] speedtest skipped because speedtest-cli is unavailable.\")\n",
    "\n",
    "civitai_ping, civitai_err = avg_ping(\"civitai.com\")\n",
    "hf_ping, hf_err = avg_ping(\"huggingface.co\")\n",
    "net[\"civitai_ping_ms\"] = civitai_ping\n",
    "net[\"hf_ping_ms\"] = hf_ping\n",
    "\n",
    "print(\"\\n--- Network Results ---\")\n",
    "print(f\"Download (Mbps): {net['download_mbps']:.2f}\" if net['download_mbps'] is not None else \"Download (Mbps): N/A\")\n",
    "print(f\"Upload   (Mbps): {net['upload_mbps']:.2f}\" if net['upload_mbps'] is not None else \"Upload   (Mbps): N/A\")\n",
    "print(f\"Ping civitai.com (ms avg): {net['civitai_ping_ms']:.2f}\" if net['civitai_ping_ms'] is not None else f\"Ping civitai.com (ms avg): N/A ({civitai_err})\")\n",
    "print(f\"Ping huggingface.co (ms avg): {net['hf_ping_ms']:.2f}\" if net['hf_ping_ms'] is not None else f\"Ping huggingface.co (ms avg): N/A ({hf_err})\")\n",
    "\n",
    "net_ok = (\n",
    "    net['download_mbps'] is not None and net['download_mbps'] >= 50\n",
    "    and net['upload_mbps'] is not None and net['upload_mbps'] >= 100\n",
    "    and net['civitai_ping_ms'] is not None and net['civitai_ping_ms'] < 100\n",
    "    and net['hf_ping_ms'] is not None and net['hf_ping_ms'] < 100\n",
    ")\n",
    "print(\"Recommendation:\")\n",
    "if net_ok:\n",
    "    print(\"- Network looks OK (download >=50 Mbps, upload >=100 Mbps, pings <100 ms).\")\n",
    "else:\n",
    "    print(\"- Network is below suggested threshold or incomplete. Consider dropping instance if this impacts your workflow.\")\n",
    "\n",
    "\n",
    "section(\"2) GPU + PyTorch compatibility checks (non-blocking)\")\n",
    "try:\n",
    "    import torch\n",
    "    torch_import_ok = True\n",
    "except Exception as e:\n",
    "    torch_import_ok = False\n",
    "    torch = None\n",
    "    torch_import_err = str(e)\n",
    "\n",
    "if not torch_import_ok:\n",
    "    print(\"PyTorch import: FAILED\")\n",
    "    print(\"Error:\", torch_import_err)\n",
    "    print(\"Recommendation: PyTorch unavailable; consider dropping instance for compatibility reasons.\")\n",
    "else:\n",
    "    gpu_available = bool(torch.cuda.is_available())\n",
    "    gpu_name = None\n",
    "    compute_cap = None\n",
    "    arch_list = None\n",
    "    pytorch_test = \"Not run\"\n",
    "    pytorch_test_error = None\n",
    "\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"PyTorch CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU available: {'Yes' if gpu_available else 'No'}\")\n",
    "\n",
    "    if gpu_available:\n",
    "        try:\n",
    "            gpu_name = torch.cuda.get_device_name(0)\n",
    "        except Exception as e:\n",
    "            gpu_name = f\"Unavailable ({e})\"\n",
    "\n",
    "        try:\n",
    "            compute_cap = torch.cuda.get_device_capability(0)\n",
    "        except Exception as e:\n",
    "            compute_cap = f\"Unavailable ({e})\"\n",
    "\n",
    "        try:\n",
    "            arch_list = torch.cuda.get_arch_list()\n",
    "        except Exception as e:\n",
    "            arch_list = f\"Unavailable ({e})\"\n",
    "\n",
    "        try:\n",
    "            _ = torch.rand(1).cuda()\n",
    "            pytorch_test = \"Passed\"\n",
    "        except RuntimeError as e:\n",
    "            pytorch_test = \"Failed\"\n",
    "            pytorch_test_error = str(e)\n",
    "        except Exception as e:\n",
    "            pytorch_test = \"Failed\"\n",
    "            pytorch_test_error = str(e)\n",
    "\n",
    "    print(f\"GPU name: {gpu_name}\")\n",
    "    print(f\"Compute capability: {compute_cap}\")\n",
    "    print(f\"Supported architectures (torch.cuda.get_arch_list): {arch_list}\")\n",
    "    if pytorch_test == \"Passed\":\n",
    "        print(\"PyTorch CUDA test: Passed\")\n",
    "    elif pytorch_test == \"Failed\":\n",
    "        print(f\"PyTorch CUDA test: Failed: {pytorch_test_error}\")\n",
    "    else:\n",
    "        print(\"PyTorch CUDA test: Not run (GPU unavailable)\")\n",
    "\n",
    "    allowed_caps = {(8, 0), (8, 6), (8, 9)}\n",
    "    torch_ver = parse_version_tuple(torch.__version__)\n",
    "    torch_ok = torch_ver >= (2, 4)\n",
    "    cuda_ok = bool(torch.version.cuda and str(torch.version.cuda).startswith(\"12\"))\n",
    "    cap_ok = isinstance(compute_cap, tuple) and compute_cap in allowed_caps\n",
    "    test_ok = pytorch_test == \"Passed\"\n",
    "\n",
    "    print(\"Recommendation:\")\n",
    "    if gpu_available and cap_ok and torch_ok and cuda_ok and test_ok:\n",
    "        print(\"- Compatibility looks OK (compute_cap in [(8,0),(8,6),(8,9)], PyTorch >=2.4, CUDA 12.x, test passed).\")\n",
    "    else:\n",
    "        if pytorch_test_error and \"no kernel image\" in pytorch_test_error.lower():\n",
    "            print(\"- Detected 'no kernel image' runtime issue. Likely torch/CUDA arch mismatch.\")\n",
    "            print(\"- Consider dropping this instance or switching to a build that supports your GPU (e.g., RTX 50xx may need cu128+ nightly).\")\n",
    "        else:\n",
    "            print(\"- Potential compatibility issue detected (GPU/PyTorch/CUDA/test mismatch).\")\n",
    "            print(\"- Consider dropping for compatibility issues (e.g., RTX 50xx may need cu128+ nightly).\")\n",
    "\n",
    "print(\"\\nDone: diagnostics completed without raising hard errors.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "<div style=\"color:rgb(0,0,255);font-size: 40px;font-weight:700;\">\nMAIN SETTINGS\n</div>",
   "metadata": {
    "id": "8u27b0sPG4SA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#MAIN\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import subprocess\n",
    "import shutil\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from getpass import getpass\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "#main vars\n",
    "ON_VAST = any(k in os.environ for k in (\"VAST_CONTAINERLABEL\", \"VAST_TCP_PORT_22\", \"CONTAINER_ID\")) or os.path.exists('/workspace')\n",
    "MAX_PARALLEL_DOWNLOADS = max(1, int(os.environ.get(\"MAX_PARALLEL_DOWNLOADS\", \"3\")))\n",
    "MIN_VALID_FILE_BYTES = int(os.environ.get(\"MIN_VALID_FILE_BYTES\", \"1000000\"))\n",
    "\n",
    "###Main Dependencies###\n",
    "system_packages = [\n",
    "    \"aria2\",\n",
    "    \"libjpeg-dev\",\n",
    "    \"zlib1g-dev\",\n",
    "    \"libpng-dev\",\n",
    "    \"libfreetype6-dev\",\n",
    "    \"libopenjp2-7-dev\",\n",
    "    \"build-essential\",\n",
    "]\n",
    "print(\"Checking/installing download and Pillow build dependencies...\")\n",
    "try:\n",
    "    subprocess.run([\"apt\", \"update\", \"-qq\"], check=True, capture_output=True)\n",
    "    result = subprocess.run([\"apt\", \"install\", \"-y\", \"-qq\", *system_packages], capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(\"System dependencies are ready\")\n",
    "    else:\n",
    "        print(\"Install failed (code {}):\".format(result.returncode))\n",
    "        print(\"stderr:\", result.stderr.strip())\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"apt error (code {e.returncode}): {e.stderr}\")\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {e}\")\n",
    "if shutil.which(\"aria2c\") is None:\n",
    "    raise RuntimeError(\"aria2c is still unavailable after apt install\")\n",
    "\n",
    "###Determing directories###\n",
    "possible_bases = [\n",
    "    \"/workspace\",\n",
    "]\n",
    "BASE_DIR = None\n",
    "for path in possible_bases:\n",
    "    if os.path.isdir(path):\n",
    "        BASE_DIR = path\n",
    "        break\n",
    "if BASE_DIR is None:\n",
    "    BASE_DIR = os.getcwd()\n",
    "    print(\"WARNING: Known directory not found:\", BASE_DIR)\n",
    "print(\"Working directory:\", BASE_DIR)\n",
    "\n",
    "# Configuration\n",
    "FORGE_DIR        = os.path.join(BASE_DIR, \"stable-diffusion-webui-forge\")\n",
    "MODELS_DIR       = os.path.join(BASE_DIR, \"stable-diffusion-webui-forge\", \"models\", \"Stable-diffusion\")\n",
    "LORA_DIR         = os.path.join(BASE_DIR, \"stable-diffusion-webui-forge\", \"models\", \"Lora\")\n",
    "CONTROLNET_DIR   = os.path.join(FORGE_DIR, \"extensions\", \"sd-webui-controlnet\")\n",
    "CONTROLNET_MODELS_DIR = os.path.join(CONTROLNET_DIR, \"models\")\n",
    "EXTENSIONS_DIR   = os.path.join(FORGE_DIR, \"extensions\")\n",
    "OUTPUTS_DIR      = os.path.join(FORGE_DIR, \"outputs\")\n",
    "VOLUME_DIR       = os.path.join(BASE_DIR, \"volume\")\n",
    "GEN_DIR          = os.path.join(BASE_DIR, \"gen\")\n",
    "IMAGES_DIR       = os.path.join(GEN_DIR, \"Images\")\n",
    "for d in [MODELS_DIR, LORA_DIR, CONTROLNET_DIR, CONTROLNET_MODELS_DIR, EXTENSIONS_DIR, OUTPUTS_DIR, VOLUME_DIR, GEN_DIR, IMAGES_DIR]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "###Dependencies used by generation cell###\n",
    "for pkg in [\"openpyxl\", \"requests\"]:\n",
    "    try:\n",
    "        __import__(pkg)\n",
    "    except ImportError:\n",
    "        print(f\"Installing missing dependency: {pkg}\")\n",
    "        subprocess.run([\"python3\", \"-m\", \"pip\", \"install\", \"-q\", pkg], check=True)\n",
    "\n",
    "###Token Authorization###\n",
    "def get_secret(name: str):\n",
    "    \"\"\"Get secret from environment variables only.\"\"\"\n",
    "    value = os.environ.get(name)\n",
    "    if value:\n",
    "        return value.strip(), \"env\"\n",
    "    return None, None\n",
    "CIVITAI_TOKEN, CIVITAI_SRC = get_secret(\"CIVITAI_TOKEN\")\n",
    "HF_TOKEN, HF_SRC = get_secret(\"HF_TOKEN\")\n",
    "if not CIVITAI_TOKEN:\n",
    "    manual_civitai = getpass(\"Enter CIVITAI_TOKEN (leave blank to skip): \").strip()\n",
    "    if manual_civitai:\n",
    "        CIVITAI_TOKEN, CIVITAI_SRC = manual_civitai, \"manual_input\"\n",
    "if not HF_TOKEN:\n",
    "    manual_hf = getpass(\"Enter HF_TOKEN (leave blank to skip): \").strip()\n",
    "    if manual_hf:\n",
    "        HF_TOKEN, HF_SRC = manual_hf, \"manual_input\"\n",
    "TOKENS = {}\n",
    "if CIVITAI_TOKEN:\n",
    "    TOKENS[\"CIVITAI\"] = CIVITAI_TOKEN\n",
    "if HF_TOKEN:\n",
    "    TOKENS[\"HF_TOKEN\"] = HF_TOKEN\n",
    "print(\"Token sources:\")\n",
    "print(f\"  CIVITAI_TOKEN: {CIVITAI_SRC or 'not found'}\")\n",
    "print(f\"  HF_TOKEN: {HF_SRC or 'not found'}\")\n",
    "if ON_VAST:\n",
    "    print(\"Vast.ai tip: add CIVITAI_TOKEN/HF_TOKEN in template env vars, restart container, then rerun this cell.\")\n",
    "if not CIVITAI_TOKEN:\n",
    "    print(\"CivitAI token not found\")\n",
    "if not HF_TOKEN:\n",
    "    print(\"HF token not found\")\n",
    "if not TOKENS:\n",
    "    raise RuntimeError(\"No tokens were provided. Set secrets or enter at least one token (CivitAI or HF).\")\n",
    "\n",
    "###Download functions###\n",
    "def _prepare_download_url(url, token):\n",
    "    \"\"\"CivitAI download works more reliably with token as query param.\"\"\"\n",
    "    if token and \"civitai.com/api/download/models\" in url and \"token=\" not in url:\n",
    "        sep = \"&\" if \"?\" in url else \"?\"\n",
    "        return f\"{url}{sep}{urlencode({'token': token})}\"\n",
    "    return url\n",
    "\n",
    "def _looks_valid_file(path, min_bytes=MIN_VALID_FILE_BYTES):\n",
    "    return os.path.exists(path) and os.path.getsize(path) > min_bytes\n",
    "\n",
    "def _human_mb(num_bytes):\n",
    "    return f\"{num_bytes / (1024 * 1024):.1f} MB\"\n",
    "\n",
    "def _estimate_expected_mb(label):\n",
    "    # Examples: \"151 MB\", \"6,46 GB\"\n",
    "    match = re.search(r\"(\\d+[\\.,]?\\d*)\\s*(MB|GB)\", label, re.IGNORECASE)\n",
    "    if not match:\n",
    "        return None\n",
    "    value = float(match.group(1).replace(',', '.'))\n",
    "    unit = match.group(2).upper()\n",
    "    return value * (1024 if unit == \"GB\" else 1)\n",
    "\n",
    "def _size_sanity_warning(path, expected_mb, tolerance=0.7):\n",
    "    if expected_mb is None or not os.path.exists(path):\n",
    "        return\n",
    "    actual_mb = os.path.getsize(path) / (1024 * 1024)\n",
    "    if actual_mb < expected_mb * tolerance:\n",
    "        print(f\"  WARNING: file size looks low ({actual_mb:.1f} MB vs expected ~{expected_mb:.1f} MB)\")\n",
    "\n",
    "def _has_min_free_disk(path, required_mb, reserve_mb=1024):\n",
    "    if required_mb is None:\n",
    "        return True\n",
    "    usage = shutil.disk_usage(path)\n",
    "    free_mb = usage.free / (1024 * 1024)\n",
    "    return free_mb >= (required_mb + reserve_mb)\n",
    "\n",
    "\n",
    "\n",
    "def _run_command_with_live_output(cmd, prefix=\"\"):\n",
    "    proc = subprocess.run(\n",
    "        cmd,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT,\n",
    "        text=True,\n",
    "    )\n",
    "    return proc.returncode, proc.stdout or \"\"\n",
    "\n",
    "def _download_one(job, target_dir):\n",
    "    label, url, filename, token_name = job\n",
    "    token = TOKENS.get(token_name)\n",
    "    output_path = os.path.join(target_dir, filename)\n",
    "    expected_mb = _estimate_expected_mb(label)\n",
    "    if _looks_valid_file(output_path):\n",
    "        _size_sanity_warning(output_path, expected_mb)\n",
    "        return (filename, \"skipped\", \"exists\")\n",
    "    if expected_mb is not None and not _has_min_free_disk(target_dir, expected_mb):\n",
    "        return (filename, \"error\", \"insufficient disk\")\n",
    "    tmp_path = output_path + \".part\"\n",
    "    if os.path.exists(tmp_path):\n",
    "        os.remove(tmp_path)\n",
    "    final_url = _prepare_download_url(url, token if token_name == \"CIVITAI\" else None)\n",
    "    # CivitAI signed links on Vast can return 403 with aggressive multi-connection mode.\n",
    "    max_conn = \"1\" if token_name == \"CIVITAI\" else \"16\"\n",
    "    split = \"1\" if token_name == \"CIVITAI\" else \"16\"\n",
    "    cmd = [\n",
    "        \"aria2c\",\n",
    "        \"--allow-overwrite=true\",\n",
    "        \"--auto-file-renaming=false\",\n",
    "        \"--continue=true\",\n",
    "        f\"--max-connection-per-server={max_conn}\",\n",
    "        f\"--split={split}\",\n",
    "        \"--min-split-size=1M\",\n",
    "        \"--console-log-level=warn\",\n",
    "        \"--summary-interval=1\",\n",
    "        \"--check-certificate=false\",\n",
    "        \"--header=User-Agent: Mozilla/5.0\",\n",
    "        \"--header=Referer: https://civitai.com/\",\n",
    "        \"--out\", os.path.basename(tmp_path),\n",
    "        \"--dir\", target_dir,\n",
    "        final_url,\n",
    "    ]\n",
    "    if token_name == \"HF_TOKEN\" and token:\n",
    "        cmd.insert(-1, f\"--header=Authorization: Bearer {token}\")\n",
    "    started = time.time()\n",
    "    code, output = _run_command_with_live_output(cmd, prefix=f\"[{label}] \")\n",
    "    if code != 0:\n",
    "        msg = (output or \"\").strip() or f\"aria2c exited {code}\"\n",
    "        # Retry once for CivitAI with curl fallback if aria2 still gets 403.\n",
    "        if token_name == \"CIVITAI\":\n",
    "            curl_cmd = [\n",
    "                \"curl\", \"-L\", \"--fail\", \"--retry\", \"2\", \"--retry-delay\", \"3\",\n",
    "                \"-A\", \"Mozilla/5.0\", \"-e\", \"https://civitai.com/\",\n",
    "                \"-o\", tmp_path, final_url\n",
    "            ]\n",
    "            curl_code, curl_output = _run_command_with_live_output(curl_cmd, prefix=f\"[{label}][curl] \")\n",
    "            if curl_code == 0 and _looks_valid_file(tmp_path):\n",
    "                os.replace(tmp_path, output_path)\n",
    "                _size_sanity_warning(output_path, expected_mb)\n",
    "                elapsed = max(0.001, time.time() - started)\n",
    "                size_bytes = os.path.getsize(output_path)\n",
    "                speed_mb_s = (size_bytes / (1024 * 1024)) / elapsed\n",
    "                return (filename, \"downloaded\", f\"{_human_mb(size_bytes)} in {elapsed:.1f}s ({speed_mb_s:.2f} MB/s)\")\n",
    "            curl_err = (curl_output or \"curl failed\").strip()\n",
    "            msg = f\"{msg}\\nFallback curl: {curl_err}\"\n",
    "        if os.path.exists(tmp_path):\n",
    "            os.remove(tmp_path)\n",
    "        return (filename, \"error\", msg)\n",
    "    if not _looks_valid_file(tmp_path):\n",
    "        size = os.path.getsize(tmp_path) if os.path.exists(tmp_path) else 0\n",
    "        if os.path.exists(tmp_path):\n",
    "            os.remove(tmp_path)\n",
    "        return (filename, \"error\", f\"downloaded file too small ({_human_mb(size)})\")\n",
    "    os.replace(tmp_path, output_path)\n",
    "    _size_sanity_warning(output_path, expected_mb)\n",
    "    elapsed = max(0.001, time.time() - started)\n",
    "    size_bytes = os.path.getsize(output_path)\n",
    "    speed_mb_s = (size_bytes / (1024 * 1024)) / elapsed\n",
    "    return (filename, \"downloaded\", f\"{_human_mb(size_bytes)} in {elapsed:.1f}s ({speed_mb_s:.2f} MB/s)\")\n",
    "\n",
    "def run_download_list(download_list, target_dir, title):\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    if not download_list:\n",
    "        print(\"No items.\")\n",
    "        return\n",
    "    workers = min(MAX_PARALLEL_DOWNLOADS, len(download_list))\n",
    "    print(f\"Parallel downloads: {workers}\")\n",
    "    ok = 0\n",
    "    fail = 0\n",
    "    with ThreadPoolExecutor(max_workers=workers) as ex:\n",
    "        futures = [ex.submit(_download_one, job, target_dir) for job in download_list]\n",
    "        for fut in as_completed(futures):\n",
    "            filename, status, info = fut.result()\n",
    "            if status == \"downloaded\":\n",
    "                ok += 1\n",
    "                print(f\"**DOWNLOADED:** {filename}\")\n",
    "            elif status == \"skipped\":\n",
    "                ok += 1\n",
    "                print(f\"**SKIPPED:** {filename}\")\n",
    "            else:\n",
    "                fail += 1\n",
    "                print(f\"**ERROR:** {filename} -> {info}\")\n",
    "    print(f\"Done: OK={ok}, FAIL={fail}\")\n",
    "    if fail > 0:\n",
    "        raise RuntimeError(f\"Some downloads failed in {title}: {fail} item(s)\")"
   ],
   "metadata": {
    "id": "kTzG8SwHG4SC",
    "trusted": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "### FORGE BOOTSTRAP ###\n\nimport os\nimport shutil\nimport subprocess\n\n#Vars\nrequired_packages = [\"git\", \"python3-venv\", \"python3-pip\", \"libjpeg-dev\", \"zlib1g-dev\", \"libpng-dev\", \"build-essential\"]\nprint(\"Checking/installing platform dependencies...\")\nsubprocess.run([\"apt\", \"update\", \"-qq\"], check=False)\nsubprocess.run([\"apt\", \"install\", \"-y\", \"-qq\", *required_packages], check=False)\nlaunch_script = os.path.join(FORGE_DIR, \"webui.sh\")\ngit_head = os.path.join(FORGE_DIR, \".git\", \"HEAD\")\nforge_ready = os.path.isfile(launch_script) and os.path.isfile(git_head)\n\n#Forge install\nif forge_ready:\n    print(\"WebUI Forge already exists and looks valid, skipping clone.\")\nelse:\n    if os.path.isdir(FORGE_DIR):\n        print(\"FORGE_DIR exists but WebUI Forge is incomplete/corrupted. Recreating...\")\n        shutil.rmtree(FORGE_DIR)\n    print(\"Cloning WebUI Forge...\")\n    subprocess.run([\n        \"git\", \"clone\", \"https://github.com/lllyasviel/stable-diffusion-webui-forge\", FORGE_DIR\n    ], check=True)\n    if not os.path.isfile(os.path.join(FORGE_DIR, \"webui.sh\")):\n        raise FileNotFoundError(\"Clone completed but webui.sh not found. Check repository state.\")\nprint(\"Optional bootstrap finished.\")",
   "metadata": {
    "id": "wM3J4QA4G4SK",
    "trusted": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "###MAIN DOWNLOAD CELL###\n# CONTROLNET INSTALL OPTIONS\n# Options: A=force clean reinstall; B=\"update\" for git pull in existing repo; C=\"skip\" to keep current state\n\nCONTROLNET_REPO_URL = \"https://github.com/Mikubill/sd-webui-controlnet\"\nCONTROLNET_INSTALL_MODE = os.environ.get(\"CONTROLNET_INSTALL_MODE\", \"reinstall\").strip().lower()\n\nif CONTROLNET_INSTALL_MODE not in {\"reinstall\", \"update\", \"skip\"}:\n    raise ValueError(\"CONTROLNET_INSTALL_MODE must be one of: reinstall, update, skip\")\nprint(f\"ControlNet extension path: {CONTROLNET_DIR}\")\nprint(f\"Install mode: {CONTROLNET_INSTALL_MODE}\")\n\nif CONTROLNET_INSTALL_MODE == \"skip\":\n    print(\"ControlNet install skipped\")\nelse:\n    if os.path.isdir(CONTROLNET_DIR) and CONTROLNET_INSTALL_MODE == \"reinstall\":\n        print(\"Removing existing ControlNet directory...\")\n        shutil.rmtree(CONTROLNET_DIR)\n    if not os.path.isdir(CONTROLNET_DIR):\n        print(\"Cloning ControlNet repository...\")\n        subprocess.run([\"git\", \"clone\", CONTROLNET_REPO_URL, CONTROLNET_DIR], check=True)\n    else:\n        print(\"Updating ControlNet repository...\")\n        subprocess.run([\"git\", \"-C\", CONTROLNET_DIR, \"pull\", \"--ff-only\"], check=True)\n\nos.makedirs(CONTROLNET_MODELS_DIR, exist_ok=True)\nprint(\"ControlNet repository is ready\")\nprint(f\"ControlNet models directory: {CONTROLNET_MODELS_DIR}\")\n\n#ControlNET models download\ncontrolnet_models_to_download = [\n    (\"t2i-adapter_xl_openpose 151 MB\", \"https://huggingface.co/lllyasviel/sd_control_collection/resolve/main/t2i-adapter_xl_openpose.safetensors\", \"t2i-adapter_xl_openpose.safetensors\", \"HF_TOKEN\"),\n    (\"t2i-adapter_xl_canny 148 MB\", \"https://huggingface.co/lllyasviel/sd_control_collection/resolve/main/t2i-adapter_xl_canny.safetensors\", \"t2i-adapter_xl_canny.safetensors\", \"HF_TOKEN\"),\n    (\"t2i-adapter_xl_sketch 148 MB\", \"https://huggingface.co/lllyasviel/sd_control_collection/resolve/main/t2i-adapter_xl_sketch.safetensors\", \"t2i-adapter_xl_sketch.safetensors\", \"HF_TOKEN\"),\n    (\"t2i-adapter_diffusers_xl_depth_midas 151 MB\", \"https://huggingface.co/lllyasviel/sd_control_collection/resolve/main/t2i-adapter_diffusers_xl_depth_midas.safetensors\", \"t2i-adapter_diffusers_xl_depth_midas.safetensors\", \"HF_TOKEN\"),\n    (\"t2i-adapter_diffusers_xl_depth_zoe 151 MB\", \"https://huggingface.co/lllyasviel/sd_control_collection/resolve/main/t2i-adapter_diffusers_xl_depth_zoe.safetensors\", \"t2i-adapter_diffusers_xl_depth_zoe.safetensors\", \"HF_TOKEN\"),\n    (\"t2i-adapter_diffusers_xl_lineart 151 MB\", \"https://huggingface.co/lllyasviel/sd_control_collection/resolve/main/t2i-adapter_diffusers_xl_lineart.safetensors\", \"t2i-adapter_diffusers_xl_lineart.safetensors\", \"HF_TOKEN\"),\n]\nrun_download_list(controlnet_models_to_download, CONTROLNET_MODELS_DIR, \"ControlNet\")\n\n#Models(checkpoints) download\nmodels_to_download = [\n    (\"WAI ILL V16.0 6,46 GB\", \"https://civitai.com/api/download/models/2514310?type=Model&format=SafeTensor&size=pruned&fp=fp16\", \"wai_v160.safetensors\", \"CIVITAI\"),\n]\nrun_download_list(models_to_download, MODELS_DIR, \"Checkpoints\")\n\n#LoRa download\nlora_to_download = [\n    (\"Detailer IL V2 218 MB\",        \"https://civitai.com/api/download/models/1736373?type=Model&format=SafeTensor\",    \"detailer_v2_il.safetensors\",     \"CIVITAI\"),\n    (\"Realistic filter V1 55 MB\",    \"https://civitai.com/api/download/models/1124771?type=Model&format=SafeTensor\",    \"realistic_filter_v1_il.safetensors\", \"CIVITAI\"),\n    (\"Hyperrealistic V4 ILL 435 MB\", \"https://civitai.com/api/download/models/1914557?type=Model&format=SafeTensor\",    \"hyperrealistic_v4_ill.safetensors\",  \"CIVITAI\"),\n    (\"Niji semi realism V3.5 ILL 435 MB\", \"https://civitai.com/api/download/models/1882710?type=Model&format=SafeTensor\", \"niji_v35.safetensors\", \"CIVITAI\"),\n    (\"ATNR Style ILL V1.1 350 MB\", \"https://civitai.com/api/download/models/1711464?type=Model&format=SafeTensor\", \"atnr_style_ill_v1.1.safetensors\", \"CIVITAI\"),\n    (\"Face Enhancer Ill 218 MB\", \"https://civitai.com/api/download/models/1839268?type=Model&format=SafeTensor\", \"face_enhancer_ill.safetensors\", \"CIVITAI\"),\n    (\"Smooth Detailer Booster V4 243 MB\", \"https://civitai.com/api/download/models/2196453?type=Model&format=SafeTensor\", \"smooth_detailer_booster_v4.safetensors\", \"CIVITAI\"),\n    (\"USNR Style V-pred 157 MB\", \"https://civitai.com/api/download/models/2555444?type=Model&format=SafeTensor\", \"usnr_style.safetensors\", \"CIVITAI\"),\n    (\"748cm Style V1 243 MB\", \"https://civitai.com/api/download/models/1056404?type=Model&format=SafeTensor\", \"748cm_style_v1.safetensors\", \"CIVITAI\"),\n    (\"Velvet's Mythic Fantasy Styles IL 218 MB\", \"https://civitai.com/api/download/models/2620790?type=Model&format=SafeTensor\", \"velvets_styles.safetensors\", \"CIVITAI\"),\n    (\"Pixel Art Style IL V7 435 MB\", \"https://civitai.com/api/download/models/2661972?type=Model&format=SafeTensor\", \"pixel_art.safetensors\", \"CIVITAI\"),\n]\nrun_download_list(lora_to_download, LORA_DIR, \"LoRA\")",
   "metadata": {
    "id": "XMxh61s40Psj",
    "trusted": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "### RUN WEBUI ###\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "forge_dir = Path(FORGE_DIR)\n",
    "if not forge_dir.exists():\n",
    "    raise FileNotFoundError(f\"FORGE_DIR не найден: {forge_dir}\")\n",
    "\n",
    "launch_utils_path = forge_dir / \"modules\" / \"launch_utils.py\"\n",
    "\n",
    "# Force Pillow pin in Forge requirement files (Forge may reinstall dependencies on startup)\n",
    "for req_name in (\"requirements.txt\", \"requirements_versions.txt\"):\n",
    "    req_path = forge_dir / req_name\n",
    "    if not req_path.exists():\n",
    "        continue\n",
    "\n",
    "    lines = req_path.read_text(encoding=\"utf-8\").splitlines()\n",
    "    updated = []\n",
    "    pillow_line_found = False\n",
    "\n",
    "    for line in lines:\n",
    "        if line.strip().lower().startswith(\"pillow\"):\n",
    "            updated.append(\"pillow<12\")\n",
    "            pillow_line_found = True\n",
    "        else:\n",
    "            updated.append(line)\n",
    "\n",
    "    if not pillow_line_found:\n",
    "        updated.append(\"pillow<12\")\n",
    "\n",
    "    req_path.write_text(\"\\n\".join(updated) + \"\\n\", encoding=\"utf-8\")\n",
    "    print(f\"Pinned Pillow in {req_path}\")\n",
    "\n",
    "if launch_utils_path.exists():\n",
    "    content = launch_utils_path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "    clip_line_pattern = re.compile(\n",
    "        r'^(?P<indent>\\s*)run_pip\\(f\"install(?: --no-build-isolation)?(?: --no-use-pep517)? \\{clip_package\\}\", \"clip\"\\)\\s*$',\n",
    "        re.MULTILINE,\n",
    "    )\n",
    "    def _replace_clip_install(match):\n",
    "        indent = match.group(\"indent\")\n",
    "        return (\n",
    "            f'{indent}run_pip(\"install setuptools==69.5.1 wheel\", \"clip build deps\")\\n'\n",
    "            f'{indent}run_pip(f\"install --no-build-isolation {{clip_package}}\", \"clip\")'\n",
    "        )\n",
    "    content_new, replacements = clip_line_pattern.subn(_replace_clip_install, content, count=1)\n",
    "    numpy_fix_pattern = re.compile(\n",
    "        r'^(?P<indent>\\s*)startup_timer\\.record\\(\"install torch\"\\)\\s*$',\n",
    "        re.MULTILINE,\n",
    "    )\n",
    "    def _replace_torch_record(match):\n",
    "        indent = match.group(\"indent\")\n",
    "        return (\n",
    "            f\"{indent}run_pip('install \\\"numpy==2.2.6\\\" \\\"scikit-image>=0.24.0\\\" --force-reinstall', 'numpy/skimage compatibility')\\n\"\n",
    "            f'{indent}startup_timer.record(\"install torch\")'\n",
    "        )\n",
    "    torch_fix_replacements = 0\n",
    "    if \"numpy/skimage compatibility\" not in content_new:\n",
    "        content_new, torch_fix_replacements = numpy_fix_pattern.subn(_replace_torch_record, content_new, count=1)\n",
    "        if torch_fix_replacements:\n",
    "            print(\"Patched torch stage (reinstalled numpy 2.x + scikit-image for ABI compatibility).\")\n",
    "        else:\n",
    "            print(\"Torch-stage compatibility patch not applied (marker not found).\")\n",
    "    late_numpy_pattern = re.compile(r'^(?P<indent>\\s*)import webui(?:\\s*#.*)?\\s*$', re.MULTILINE)\n",
    "    def _replace_import_webui(match):\n",
    "        indent = match.group(\"indent\")\n",
    "        return (\n",
    "            f\"{indent}run_pip('install \\\"numpy==2.2.6\\\" \\\"scikit-image>=0.24.0\\\" --force-reinstall', 'numpy/skimage late compatibility')\\n\"\n",
    "            f'{indent}import webui'\n",
    "        )\n",
    "    late_fix_replacements = 0\n",
    "    if \"numpy/skimage late compatibility\" not in content_new:\n",
    "        content_new, late_fix_replacements = late_numpy_pattern.subn(_replace_import_webui, content_new, count=1)\n",
    "        if late_fix_replacements:\n",
    "            print(\"Patched start stage (re-pin numpy/scikit-image before importing webui).\")\n",
    "        else:\n",
    "            print(\"Start-stage compatibility patch not applied (import webui line not found).\")\n",
    "    avif_fix_replacements = 0\n",
    "    avif_fix_pattern = re.compile(r'^(?P<indent>\\s*)import webui(?:\\s*#.*)?\\s*$', re.MULTILINE)\n",
    "    def _replace_import_webui_avif(match):\n",
    "        indent = match.group(\"indent\")\n",
    "        return (\n",
    "            f\"{indent}run_pip('install \\\"pillow<12\\\" --force-reinstall', 'pillow late compatibility')\\n\"\n",
    "            f\"{indent}run_pip('install pillow-avif-plugin', 'pillow_avif plugin')\\n\"\n",
    "            f'{indent}import webui'\n",
    "        )\n",
    "    if \"pillow_avif plugin\" not in content_new:\n",
    "        content_new, avif_fix_replacements = avif_fix_pattern.subn(_replace_import_webui_avif, content_new, count=1)\n",
    "        if avif_fix_replacements:\n",
    "            print(\"Patched start stage (ensured Pillow<12 and pillow-avif-plugin before importing webui).\")\n",
    "        else:\n",
    "            print(\"Start-stage Pillow/AVIF patch not applied (import webui line not found).\")\n",
    "    if replacements or torch_fix_replacements or late_fix_replacements or avif_fix_replacements:\n",
    "        launch_utils_path.write_text(content_new, encoding=\"utf-8\")\n",
    "    if replacements:\n",
    "        print(\"Patched CLIP install command (fixed flags + preserved indentation).\")\n",
    "    else:\n",
    "        print(\"CLIP install patch already applied or target line not found.\")\n",
    "else:\n",
    "    print(f\"Warning: {launch_utils_path} not found, skipping CLIP patch.\")\n",
    "\n",
    "# Workaround for frequent startup issue in ControlNet: soft_inpainting.py\n",
    "\n",
    "soft_inpainting_path = forge_dir / \"extensions\" / \"sd-webui-controlnet\" / \"scripts\" / \"soft_inpainting.py\"\n",
    "\n",
    "\n",
    "soft_inpainting_disabled_path = soft_inpainting_path.with_suffix(\".py.disabled\")\n",
    "\n",
    "\n",
    "if soft_inpainting_path.exists():\n",
    "    os.replace(soft_inpainting_path, soft_inpainting_disabled_path)\n",
    "    print(f\"Disabled problematic script: {soft_inpainting_path.name} -> {soft_inpainting_disabled_path.name}\")\n",
    "else:\n",
    "    print(\"soft_inpainting.py already disabled or not present.\")\n",
    "\n",
    "cmd = [\"bash\", \"webui.sh\", \"-f\", \"--xformers\", \"--api\", \"--port\", \"17860\"]\n",
    "run_env = os.environ.copy()\n",
    "run_env[\"MPLBACKEND\"] = \"Agg\"\n",
    "print(\"Running (background):\", \" \".join(cmd), \"in\", forge_dir)\n",
    "print(\"MPLBACKEND forced to:\", run_env[\"MPLBACKEND\"])\n",
    "venv_python = forge_dir / \"venv\" / \"bin\" / \"python\"\n",
    "\n",
    "if venv_python.exists():\n",
    "    print(\"Ensuring Forge venv has Pillow < 12 + AVIF plugin\")\n",
    "    subprocess.run([str(venv_python), \"-m\", \"pip\", \"install\", \"pillow<12\", \"--force-reinstall\"], check=False)\n",
    "    subprocess.run([str(venv_python), \"-m\", \"pip\", \"install\", \"pillow-avif-plugin\"], check=False)\n",
    "\n",
    "    print(\"Installing missing dependency in Forge venv: joblib\")\n",
    "    subprocess.run([str(venv_python), \"-m\", \"pip\", \"install\", \"joblib\"], check=False)\n",
    "\n",
    "    print(\"Trying optional dependency in Forge venv: insightface\")\n",
    "    insightface_result = subprocess.run([str(venv_python), \"-m\", \"pip\", \"install\", \"insightface\"], check=False)\n",
    "    if insightface_result.returncode != 0:\n",
    "        print(\"Warning: insightface installation failed (optional dependency).\")\n",
    "else:\n",
    "    print(f\"Warning: venv python not found at {venv_python}, skipping pillow/joblib/insightface install.\")\n",
    "\n",
    "pid_path = forge_dir / \"webui.pid\"\n",
    "log_path = forge_dir / \"webui.log\"\n",
    "\n",
    "def _pid_alive(pid: int) -> bool:\n",
    "    try:\n",
    "        os.kill(pid, 0)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def _log_tail(chars: int = 4000) -> str:\n",
    "    if not log_path.exists():\n",
    "        return \"(webui.log not found yet)\"\n",
    "    return log_path.read_text(encoding=\"utf-8\", errors=\"ignore\")[-chars:]\n",
    "\n",
    "running_pid = None\n",
    "\n",
    "if pid_path.exists():\n",
    "    try:\n",
    "        old_pid = int(pid_path.read_text(encoding=\"utf-8\").strip())\n",
    "        if _pid_alive(old_pid):\n",
    "            running_pid = old_pid\n",
    "            print(f\"WebUI already running with PID={old_pid}. Можно запускать последнюю ячейку.\")\n",
    "        else:\n",
    "            pid_path.unlink(missing_ok=True)\n",
    "    except Exception:\n",
    "        pid_path.unlink(missing_ok=True)\n",
    "\n",
    "started_proc = None\n",
    "\n",
    "if running_pid is None:\n",
    "    with open(log_path, \"a\", encoding=\"utf-8\") as log_f:\n",
    "        started_proc = subprocess.Popen(\n",
    "            cmd,\n",
    "            cwd=forge_dir,\n",
    "            env=run_env,\n",
    "            stdout=log_f,\n",
    "            stderr=subprocess.STDOUT,\n",
    "            start_new_session=True,\n",
    "            text=True,\n",
    "        )\n",
    "    pid_path.write_text(str(started_proc.pid), encoding=\"utf-8\")\n",
    "    running_pid = started_proc.pid\n",
    "    print(f\"WebUI started in background. PID={running_pid}\")\n",
    "    print(f\"Logs: {log_path}\")\n",
    "\n",
    "observe_timeout = int(os.environ.get(\"FORGE_STARTUP_OBSERVE_TIMEOUT\", \"120\"))\n",
    "observe_interval = float(os.environ.get(\"FORGE_STARTUP_OBSERVE_INTERVAL\", \"2\"))\n",
    "\n",
    "for _ in range(max(1, int(observe_timeout / observe_interval))):\n",
    "    tail = _log_tail()\n",
    "    if started_proc is not None:\n",
    "        rc = started_proc.poll()\n",
    "        if rc is not None:\n",
    "            raise RuntimeError(\n",
    "                f\"WebUI process exited early with code {rc}.\\n\"\n",
    "                f\"Check logs at: {log_path}\\n\"\n",
    "                f\"Last log lines:\\n{tail}\"\n",
    "            )\n",
    "    if \"Running on local URL\" in tail or \"Uvicorn running\" in tail:\n",
    "        print(\"WebUI appears ready. Можно запускать последнюю ячейку.\")\n",
    "        break\n",
    "\n",
    "    time.sleep(observe_interval)\n",
    "else:\n",
    "    print(\"WebUI всё ещё запускается в фоне. Запускайте последнюю ячейку: она дождётся API.\")\n",
    "    print(f\"Если API не поднимется, проверьте лог: {log_path}\")"
   ],
   "metadata": {
    "id": "cpZBUyZoYFHL",
    "trusted": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "### API GENERATION FROM XLSX ###\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import glob\n",
    "import time\n",
    "import shutil\n",
    "import zipfile\n",
    "import base64\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "import requests\n",
    "\n",
    "try:\n",
    "    from openpyxl import load_workbook\n",
    "except ImportError:\n",
    "    import subprocess, sys\n",
    "    print(\"Installing missing dependency: openpyxl\")\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', 'openpyxl'], check=True)\n",
    "    from openpyxl import load_workbook\n",
    "\n",
    "\n",
    "API_BASE = os.environ.get(\"FORGE_API_BASE\", \"http://127.0.0.1:17860\")\n",
    "TXT2IMG_URL = f\"{API_BASE}/sdapi/v1/txt2img\"\n",
    "OPENAPI_URL = f\"{API_BASE}/openapi.json\"\n",
    "PROMPTS_CANDIDATES = [\n",
    "    os.path.join(GEN_DIR, \"prompts.xlsx\"),\n",
    "    os.path.join(GEN_DIR, \"prompts.xlxs\"),\n",
    "    os.path.join(GEN_DIR, \"Prompts.xlsx\"),\n",
    "    os.path.join(GEN_DIR, \"Prompts.xlxs\"),\n",
    "    os.path.join(BASE_DIR, \"prompts.xlsx\"),\n",
    "    os.path.join(BASE_DIR, \"prompts.xlxs\"),\n",
    "    os.path.join(BASE_DIR, \"Prompts.xlsx\"),\n",
    "    os.path.join(BASE_DIR, \"Prompts.xlxs\"),\n",
    "    \"/workspace/gen/prompts.xlsx\",\n",
    "    \"/workspace/gen/prompts.xlxs\",\n",
    "]\n",
    "PROMPTS_CANDIDATES = list(dict.fromkeys(PROMPTS_CANDIDATES))\n",
    "API_READY_TIMEOUT = int(os.environ.get(\"FORGE_API_READY_TIMEOUT\", \"600\"))\n",
    "API_READY_INTERVAL = float(os.environ.get(\"FORGE_API_READY_INTERVAL\", \"3\"))\n",
    "\n",
    "def wait_for_api_ready(base_url: str, timeout_s: int = API_READY_TIMEOUT, interval_s: float = API_READY_INTERVAL):\n",
    "    start = time.time()\n",
    "    last_error = None\n",
    "    webui_log = os.path.join(FORGE_DIR, \"webui.log\")\n",
    "    webui_pid = os.path.join(FORGE_DIR, \"webui.pid\")\n",
    "\n",
    "    def _log_tail(chars: int = 4000):\n",
    "        if not os.path.exists(webui_log):\n",
    "            return \"(webui.log not found yet)\"\n",
    "        with open(webui_log, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            return f.read()[-chars:]\n",
    "\n",
    "    def _is_pid_alive():\n",
    "        try:\n",
    "            if not os.path.exists(webui_pid):\n",
    "                return None\n",
    "            with open(webui_pid, \"r\", encoding=\"utf-8\") as f:\n",
    "                pid = int(f.read().strip())\n",
    "            os.kill(pid, 0)\n",
    "            return pid\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    while (time.time() - start) < timeout_s:\n",
    "        try:\n",
    "            r = requests.get(f\"{base_url}/sdapi/v1/progress\", timeout=10)\n",
    "            if r.status_code < 500:\n",
    "                return True\n",
    "            last_error = f\"HTTP {r.status_code}\"\n",
    "        except Exception as e:\n",
    "            last_error = str(e)\n",
    "\n",
    "        pid_state = _is_pid_alive()\n",
    "        if pid_state is False:\n",
    "            raise RuntimeError(\n",
    "                \"Forge API недоступен и процесс WebUI уже завершился.\\n\"\n",
    "                f\"Last error: {last_error}\\n\"\n",
    "                f\"WebUI log tail:\\n{_log_tail()}\"\n",
    "            )\n",
    "\n",
    "        time.sleep(interval_s)\n",
    "\n",
    "    raise RuntimeError(\n",
    "        f\"Forge API is not reachable at {base_url} after {timeout_s}s. \"\n",
    "        f\"Last error: {last_error}.\\n\"\n",
    "        f\"WebUI log tail:\\n{_log_tail()}\\n\"\n",
    "        \"Run the Forge startup cell first. If this is first launch, increase FORGE_API_READY_TIMEOUT (e.g. 900).\"\n",
    "    )\n",
    "\n",
    "LOG_PATH = os.path.join(VOLUME_DIR, \"log.txt\")\n",
    "API_IMAGES_DIR = os.path.join(OUTPUTS_DIR, \"api_generated\")\n",
    "REQUEST_DUMPS_DIR = os.path.join(VOLUME_DIR, \"request_dumps\")\n",
    "\n",
    "os.makedirs(API_IMAGES_DIR, exist_ok=True)\n",
    "os.makedirs(REQUEST_DUMPS_DIR, exist_ok=True)\n",
    "\n",
    "INPUT_IMAGE_DIRS = [\n",
    "    os.path.join(GEN_DIR, \"Images\"),\n",
    "    os.path.join(GEN_DIR, \"images\"),\n",
    "]\n",
    "INPUT_IMAGE_DIRS = [p for i, p in enumerate(INPUT_IMAGE_DIRS) if p and p not in INPUT_IMAGE_DIRS[:i]]\n",
    "CONFLICT_RULES = [\n",
    "    ((\"hr_resize_x\", \"hr_resize_y\"), (\"hr_scale\",)),\n",
    "]\n",
    "CONTROLNET_FIELDS = [\n",
    "    \"enabled\", \"image\", \"mask\", \"weight\", \"module\", \"model\", \"resize_mode\", \"lowvram\",\n",
    "    \"processor_res\", \"threshold_a\", \"threshold_b\", \"guidance_start\", \"guidance_end\",\n",
    "    \"control_mode\", \"pixel_perfect\",\n",
    "]\n",
    "\n",
    "def log_line(text: str):\n",
    "    ts = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    line = f\"[{ts}] {text}\"\n",
    "    print(line)\n",
    "    with open(LOG_PATH, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "def normalize_value(value):\n",
    "    if isinstance(value, str):\n",
    "        stripped = value.strip()\n",
    "        lowered = stripped.lower()\n",
    "        if lowered in {\"\", \"null\", \"none\", \"nan\"}:\n",
    "            return None\n",
    "        if lowered in {\"true\", \"yes\", \"1\"}:\n",
    "            return True\n",
    "        if lowered in {\"false\", \"no\", \"0\"}:\n",
    "            return False\n",
    "        if re.fullmatch(r\"-?\\d+\", stripped):\n",
    "            return int(stripped)\n",
    "        if re.fullmatch(r\"-?\\d+\\.\\d+\", stripped):\n",
    "            return float(stripped)\n",
    "        return stripped\n",
    "    return value\n",
    "\n",
    "def first_existing_path(candidates):\n",
    "    for p in candidates:\n",
    "        if os.path.exists(p):\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "def parse_workbook(path):\n",
    "    wb = load_workbook(path, data_only=True)\n",
    "    ws = wb.active\n",
    "    row1 = [normalize_value(v) for v in next(ws.iter_rows(min_row=1, max_row=1, values_only=True))]\n",
    "    row2 = [normalize_value(v) for v in next(ws.iter_rows(min_row=2, max_row=2, values_only=True))]\n",
    "    row1_headers_like = any(isinstance(v, str) and v in {\"prompt\", \"negative_prompt\", \"sampler_name\", \"steps\", \"cn1_enabled\"} for v in row1)\n",
    "    if row1_headers_like:\n",
    "        instruction = \"{prompt}\"\n",
    "        headers = [str(v).strip() if v is not None else \"\" for v in row1]\n",
    "        data_start_row = 2\n",
    "    else:\n",
    "        instruction_parts = [str(v).strip() for v in row1 if v is not None and str(v).strip()]\n",
    "        if not instruction_parts:\n",
    "            raise ValueError(\"The first line (instruction) is empty\")\n",
    "        instruction = \" \".join(instruction_parts)\n",
    "        headers = [str(v).strip() if v is not None else \"\" for v in row2]\n",
    "        data_start_row = 3\n",
    "    if not any(headers):\n",
    "        raise ValueError(\"Table headers with variables not found\")\n",
    "    rows = []\n",
    "    for row_idx in range(data_start_row, ws.max_row + 1):\n",
    "        row_values = [normalize_value(v) for v in next(ws.iter_rows(min_row=row_idx, max_row=row_idx, values_only=True))]\n",
    "        if all(v is None for v in row_values):\n",
    "            continue\n",
    "        row_map = {}\n",
    "        for idx, key in enumerate(headers):\n",
    "            if not key or key.startswith(\"S_\"):\n",
    "                continue\n",
    "            value = row_values[idx] if idx < len(row_values) else None\n",
    "            if value is None:\n",
    "                continue\n",
    "            row_map[key] = value\n",
    "        rows.append(row_map)\n",
    "    if not rows:\n",
    "        raise ValueError(\"No data rows found\")\n",
    "    return instruction, rows\n",
    "\n",
    "def render_instruction(template: str, variables: dict):\n",
    "    def repl(match):\n",
    "        key = match.group(1)\n",
    "        return str(variables.get(key, match.group(0)))\n",
    "    return re.sub(r\"\\{([a-zA-Z0-9_]+)\\}\", repl, template)\n",
    "\n",
    "def apply_conflict_rules(payload: dict):\n",
    "    cleaned = dict(payload)\n",
    "    for primary_keys, conflicting_keys in CONFLICT_RULES:\n",
    "        primary_present = all((k in cleaned and cleaned[k] is not None) for k in primary_keys)\n",
    "        if primary_present:\n",
    "            for ck in conflicting_keys:\n",
    "                cleaned.pop(ck, None)\n",
    "    return cleaned\n",
    "\n",
    "def fetch_txt2img_params_dump():\n",
    "    try:\n",
    "        response = requests.get(OPENAPI_URL, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        spec = response.json()\n",
    "        schemas = spec.get(\"components\", {}).get(\"schemas\", {})\n",
    "        candidates = [\n",
    "            \"StableDiffusionTxt2ImgProcessingApi\",\n",
    "            \"Txt2ImgRequest\",\n",
    "            \"StableDiffusionProcessingTxt2Img\",\n",
    "        ]\n",
    "        for name in candidates:\n",
    "            if name in schemas:\n",
    "                props = schemas[name].get(\"properties\", {})\n",
    "                return json.dumps({k: v.get(\"type\", \"unknown\") for k, v in props.items()}, ensure_ascii=False, indent=2)\n",
    "        return json.dumps(spec.get(\"paths\", {}).get(\"/sdapi/v1/txt2img\", {}), ensure_ascii=False, indent=2)\n",
    "    except Exception as e:\n",
    "        return f\"Failed to get list of variables: {e}\"\n",
    "\n",
    "def resolve_image_path(image_ref):\n",
    "    if image_ref is None:\n",
    "        return None\n",
    "    if isinstance(image_ref, str) and image_ref.startswith(\"data:image\"):\n",
    "        return image_ref\n",
    "    ref = str(image_ref).strip()\n",
    "    if not ref:\n",
    "        return None\n",
    "    if os.path.isabs(ref) and os.path.exists(ref):\n",
    "        return ref\n",
    "    candidates = [ref]\n",
    "    if not os.path.splitext(ref)[1]:\n",
    "        candidates += [f\"{ref}.png\", f\"{ref}.jpg\", f\"{ref}.jpeg\", f\"{ref}.webp\"]\n",
    "    for base_dir in INPUT_IMAGE_DIRS:\n",
    "        if not os.path.isdir(base_dir):\n",
    "            continue\n",
    "        for candidate in candidates:\n",
    "            full = os.path.join(base_dir, candidate)\n",
    "            if os.path.exists(full):\n",
    "                return full\n",
    "    for pattern in [f\"**/{ref}\", f\"**/{ref}.*\"]:\n",
    "        for base_dir in INPUT_IMAGE_DIRS:\n",
    "            if not os.path.isdir(base_dir):\n",
    "                continue\n",
    "            matches = glob.glob(os.path.join(base_dir, pattern), recursive=True)\n",
    "            if matches:\n",
    "                return matches[0]\n",
    "    return None\n",
    "\n",
    "def image_to_base64(image_ref):\n",
    "    if image_ref is None:\n",
    "        return None\n",
    "    if isinstance(image_ref, str) and image_ref.startswith(\"data:image\"):\n",
    "        return image_ref\n",
    "    image_path = resolve_image_path(image_ref)\n",
    "    if not image_path:\n",
    "        raise FileNotFoundError(f\"Image/Mask not found: {image_ref}. Search dirs: {INPUT_IMAGE_DIRS}\")\n",
    "    ext = os.path.splitext(image_path)[1].lower()\n",
    "    mime = {\n",
    "        \".png\": \"image/png\",\n",
    "        \".jpg\": \"image/jpeg\",\n",
    "        \".jpeg\": \"image/jpeg\",\n",
    "        \".webp\": \"image/webp\",\n",
    "        \".bmp\": \"image/bmp\",\n",
    "    }.get(ext, \"application/octet-stream\")\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        b64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "    return f\"data:{mime};base64,{b64}\"\n",
    "\n",
    "def parse_override_settings(value):\n",
    "    if value in (None, \"\", {}):\n",
    "        return {}\n",
    "    if isinstance(value, dict):\n",
    "        return value\n",
    "    if isinstance(value, str):\n",
    "        txt = value.strip()\n",
    "        if not txt:\n",
    "            return {}\n",
    "        if txt.startswith(\"{\"):\n",
    "            return json.loads(txt)\n",
    "        return json.loads(\"{\" + txt + \"}\")\n",
    "    raise ValueError(f\"Unsupported override_settings format: {type(value)}\")\n",
    "\n",
    "def deep_clean(value):\n",
    "    if isinstance(value, dict):\n",
    "        cleaned = {}\n",
    "        for k, v in value.items():\n",
    "            cv = deep_clean(v)\n",
    "            if cv is None:\n",
    "                continue\n",
    "            if isinstance(cv, str) and not cv.strip():\n",
    "                continue\n",
    "            if isinstance(cv, dict) and not cv:\n",
    "                continue\n",
    "            cleaned[k] = cv\n",
    "        return cleaned\n",
    "    if isinstance(value, list):\n",
    "        cleaned_list = []\n",
    "        for item in value:\n",
    "            ci = deep_clean(item)\n",
    "            if ci is None:\n",
    "                continue\n",
    "            if isinstance(ci, str) and not ci.strip():\n",
    "                continue\n",
    "            if isinstance(ci, dict) and not ci:\n",
    "                continue\n",
    "            cleaned_list.append(ci)\n",
    "        return cleaned_list\n",
    "    if isinstance(value, str):\n",
    "        stripped = value.strip()\n",
    "        if stripped == \"[]\":\n",
    "            return []\n",
    "        if stripped.lower() in {\"\", \"null\", \"none\", \"nan\"}:\n",
    "            return None\n",
    "        return stripped\n",
    "    return value\n",
    "\n",
    "def dump_payload(payload, generation_idx):\n",
    "    dump_path = os.path.join(REQUEST_DUMPS_DIR, f\"request_{generation_idx:04d}.json\")\n",
    "    with open(dump_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, ensure_ascii=False, indent=2)\n",
    "    return dump_path\n",
    "\n",
    "def build_payload(row_values, instruction_template):\n",
    "    payload = dict(row_values)\n",
    "    if \"prompt\" not in payload:\n",
    "        payload[\"prompt\"] = render_instruction(instruction_template, row_values)\n",
    "    payload = {k: v for k, v in payload.items() if v is not None}\n",
    "    if \"override_settings\" in payload:\n",
    "        payload[\"override_settings\"] = parse_override_settings(payload.get(\"override_settings\"))\n",
    "    controlnet_args = []\n",
    "    for idx in (1, 2, 3):\n",
    "        unit = {}\n",
    "        for field in CONTROLNET_FIELDS:\n",
    "            key = f\"cn{idx}_{field}\"\n",
    "            if key in payload:\n",
    "                unit[field] = payload.pop(key)\n",
    "        if not unit:\n",
    "            continue\n",
    "        if \"enabled\" not in unit:\n",
    "            unit[\"enabled\"] = True\n",
    "        if unit.get(\"image\") is not None:\n",
    "            unit[\"image\"] = image_to_base64(unit.get(\"image\"))\n",
    "        if unit.get(\"mask\") is not None:\n",
    "            unit[\"mask\"] = image_to_base64(unit.get(\"mask\"))\n",
    "        unit = deep_clean(unit)\n",
    "        if unit.get(\"enabled\"):\n",
    "            controlnet_args.append(unit)\n",
    "    if controlnet_args:\n",
    "        payload.setdefault(\"alwayson_scripts\", {})\n",
    "        payload[\"alwayson_scripts\"][\"ControlNet\"] = {\"args\": controlnet_args}\n",
    "    payload = deep_clean(payload)\n",
    "    payload = apply_conflict_rules(payload)\n",
    "    payload = deep_clean(payload)\n",
    "    return payload\n",
    "\n",
    "def expected_image_count(payload):\n",
    "    batch_size = int(payload.get(\"batch_size\", 1) or 1)\n",
    "    n_iter = int(payload.get(\"n_iter\", 1) or 1)\n",
    "    return max(1, batch_size) * max(1, n_iter)\n",
    "\n",
    "def save_images_from_response(images_b64, generation_idx):\n",
    "    saved = 0\n",
    "    for i, b64 in enumerate(images_b64, start=1):\n",
    "        image_data = b64.split(\",\", 1)[-1]\n",
    "        file_name = f\"gen_{generation_idx:04d}_{i:02d}.png\"\n",
    "        file_path = os.path.join(API_IMAGES_DIR, file_name)\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            f.write(base64.b64decode(image_data))\n",
    "        saved += 1\n",
    "    return saved\n",
    "\n",
    "def archive_outputs(tag: str):\n",
    "    stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    archive_name = os.path.join(VOLUME_DIR, f\"outputs_{tag}_{stamp}.zip\")\n",
    "    with zipfile.ZipFile(archive_name, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "        for root, _, files in os.walk(OUTPUTS_DIR):\n",
    "            for file in files:\n",
    "                full_path = os.path.join(root, file)\n",
    "                rel_path = os.path.relpath(full_path, OUTPUTS_DIR)\n",
    "                zf.write(full_path, rel_path)\n",
    "    for entry in os.listdir(OUTPUTS_DIR):\n",
    "        p = os.path.join(OUTPUTS_DIR, entry)\n",
    "        if os.path.isdir(p):\n",
    "            shutil.rmtree(p)\n",
    "        else:\n",
    "            os.remove(p)\n",
    "    os.makedirs(API_IMAGES_DIR, exist_ok=True)\n",
    "    log_line(f\"ARCHIVE created: {archive_name}. OUTPUTS_DIR cleaned.\")\n",
    "\n",
    "wait_for_api_ready(API_BASE)\n",
    "\n",
    "try:\n",
    "    result = subprocess.run([\"ss\", \"-tuln\"], capture_output=True, text=True, timeout=5)\n",
    "    if result.returncode == 0:\n",
    "        print(\"Open TCP ports (ss):\\n\")\n",
    "        print(result.stdout)\n",
    "    else:\n",
    "        print(\"ss didn't work\")\n",
    "except:\n",
    "    print(\"ss not available\")\n",
    "prompts_path = first_existing_path(PROMPTS_CANDIDATES)\n",
    "if not prompts_path:\n",
    "    raise FileNotFoundError(\n",
    "        \"The file prompts.xlsx/prompts.xlxs was not found. Expected paths: \" + \", \".join(PROMPTS_CANDIDATES)\n",
    "    )\n",
    "\n",
    "instruction_template, generation_rows = parse_workbook(prompts_path)\n",
    "log_line(f\"Loaded prompts file: {prompts_path}. Rows for generation: {len(generation_rows)}\")\n",
    "log_line(f\"Image search dirs: {INPUT_IMAGE_DIRS}\")\n",
    "syntax_error_dumped = False\n",
    "images_since_archive = 0\n",
    "total_expected_images = 0\n",
    "total_saved_images = 0\n",
    "\n",
    "for generation_idx, row_values in enumerate(generation_rows, start=1):\n",
    "    try:\n",
    "        payload = build_payload(row_values, instruction_template)\n",
    "    except Exception as e:\n",
    "        log_line(f\"#{generation_idx} FAIL: payload build error: {e}\")\n",
    "        continue\n",
    "    planned_images = expected_image_count(payload)\n",
    "    total_expected_images += planned_images\n",
    "    payload_dump_path = dump_payload(payload, generation_idx)\n",
    "    log_line(f\"#{generation_idx} request dump: {payload_dump_path}\")\n",
    "    if \"alwayson_scripts\" not in payload or not payload.get(\"alwayson_scripts\", {}).get(\"ControlNet\", {}).get(\"args\"):\n",
    "        log_line(f\"#{generation_idx} INFO: no ControlNet image/mask attached in payload\")\n",
    "    try:\n",
    "        response = requests.post(TXT2IMG_URL, json=payload, timeout=1800)\n",
    "        if response.status_code != 200:\n",
    "            detail = None\n",
    "            try:\n",
    "                detail = (response.json() or {}).get(\"detail\")\n",
    "            except Exception:\n",
    "                detail = None\n",
    "            err_text = str(detail or response.text or \"\")[:1200]\n",
    "            if not syntax_error_dumped:\n",
    "                params_dump = fetch_txt2img_params_dump()\n",
    "                log_line(f\"#{generation_idx} FAIL syntax/validation: {response.status_code} {err_text}\")\n",
    "                log_line(f\"#{generation_idx} FAIL payload: {payload_dump_path}\")\n",
    "                log_line(\"AVAILABLE PARAMS DUMP START\")\n",
    "                with open(LOG_PATH, \"a\", encoding=\"utf-8\") as f:\n",
    "                    f.write(params_dump + \"\\n\")\n",
    "                log_line(\"AVAILABLE PARAMS DUMP END\")\n",
    "                syntax_error_dumped = True\n",
    "            else:\n",
    "                log_line(f\"#{generation_idx} FAIL: {response.status_code} {err_text}\")\n",
    "                log_line(f\"#{generation_idx} FAIL payload: {payload_dump_path}\")\n",
    "            continue\n",
    "        saved_now = planned_images\n",
    "        total_saved_images += saved_now\n",
    "        images_since_archive += saved_now\n",
    "        log_line(f\"#{generation_idx} OK images={saved_now} expected={planned_images}\")\n",
    "        if images_since_archive >= 15:\n",
    "            archive_outputs(tag=f\"part_{generation_idx:04d}\")\n",
    "            images_since_archive = 0\n",
    "    except requests.exceptions.Timeout:\n",
    "        log_line(f\"#{generation_idx} FAIL: timeout (possible heavy generation or API freeze)\")\n",
    "        log_line(f\"#{generation_idx} FAIL payload: {payload_dump_path}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        reason = str(e)\n",
    "        if \"out of memory\" in reason.lower() or \"oom\" in reason.lower():\n",
    "            reason = \"OOM\"\n",
    "        log_line(f\"#{generation_idx} FAIL: {reason}\")\n",
    "        log_line(f\"#{generation_idx} FAIL payload: {payload_dump_path}\")\n",
    "    except Exception as e:\n",
    "        log_line(f\"#{generation_idx} FAIL: unexpected error: {e}\")\n",
    "        log_line(f\"#{generation_idx} FAIL payload: {payload_dump_path}\")\n",
    "if images_since_archive > 0:\n",
    "    archive_outputs(tag=\"final\")\n",
    "\n",
    "log_line(f\"Generation cycle completed. Requests={len(generation_rows)} expected_images={total_expected_images} saved_images={total_saved_images}\")"
   ],
   "metadata": {
    "id": "-aibyNgf0Psk",
    "trusted": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "###Ports list###\nimport subprocess\n\n# Пробуем ss (чаще всего работает в Ubuntu-контейнерах Vast.ai)\ntry:\n    result = subprocess.run([\"ss\", \"-tuln\"], capture_output=True, text=True, timeout=5)\n    if result.returncode == 0:\n        print(\"Открытые TCP-порты (ss):\\n\")\n        print(result.stdout)\n    else:\n        print(\"ss не сработал\")\nexcept:\n    print(\"ss недоступен\")",
   "metadata": {
    "trusted": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "###Get a list of current settings###\n",
    "import os\n",
    "import requests\n",
    "\n",
    "url = \"http://127.0.0.1:17860\"\n",
    "options = requests.get(f'{url}/sdapi/v1/options').json()\n",
    "models = requests.get(f'{url}/sdapi/v1/sd-models').json()\n",
    "\n",
    "info_lines = [\n",
    "    \"OPTIONS KEYS:\",\n",
    "    str(list(options.keys())),\n",
    "    \"\",\n",
    "    \"MODELS:\",\n",
    "    str(models),\n",
    "]\n",
    "\n",
    "for line in info_lines:\n",
    "    print(line)\n",
    "\n",
    "os.makedirs(\"/workspace/volume\", exist_ok=True)\n",
    "info_path = \"/workspace/volume/models_and_deps_info.txt\"\n",
    "with open(info_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(info_lines) + \"\\n\")\n",
    "print(f\"Saved info to {info_path}\")\n"
   ],
   "metadata": {
    "id": "tIvmDIey0Psl",
    "trusted": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
