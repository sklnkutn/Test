{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14862945,"sourceType":"datasetVersion","datasetId":9507658}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"color:rgb(0,0,255);font-size: 40px;font-weight:700;\">\nMAIN SETTINGS\n</div>","metadata":{}},{"cell_type":"code","source":"###SETTINGS###\n\nimport os\nimport re\nimport time\nimport subprocess\nimport shutil\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom getpass import getpass\nfrom urllib.parse import urlencode\n\n# Platform detection\nON_KAGGLE = os.path.exists('/kaggle')\nON_COLAB = 'COLAB_RELEASE_TAG' in os.environ or os.path.exists('/content')\nON_VAST = any(k in os.environ for k in (\"VAST_CONTAINERLABEL\", \"VAST_TCP_PORT_22\", \"CONTAINER_ID\")) or os.path.exists('/workspace')\n\n\nMAX_PARALLEL_DOWNLOADS = max(1, int(os.environ.get(\"MAX_PARALLEL_DOWNLOADS\", \"3\")))\nMIN_VALID_FILE_BYTES = int(os.environ.get(\"MIN_VALID_FILE_BYTES\", \"1000000\"))\n\nif shutil.which(\"aria2c\") is None:\n    print(\"aria2c not found → installing...\")\n    try:\n        subprocess.run([\"apt\", \"update\", \"-qq\"], check=True, capture_output=True)\n        result = subprocess.run([\"apt\", \"install\", \"-y\", \"-qq\", \"aria2\"], capture_output=True, text=True)\n        if result.returncode == 0:\n            print(\"aria2c installed successfully\")\n        else:\n            print(\"Install failed (code {}):\".format(result.returncode))\n            print(\"stderr:\", result.stderr.strip())\n    except subprocess.CalledProcessError as e:\n        print(f\"apt error (code {e.returncode}): {e.stderr}\")\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\nelse:\n    print(\"aria2c already available\")\n\n# Determining the working directory\npossible_bases = [\n    \"/workspace\",       # Vast.ai / RunPod\n    \"/kaggle/working\",  # Kaggle\n    \"/content\",         # Google Colab\n]\n\nBASE_DIR = None\nfor path in possible_bases:\n    if os.path.isdir(path):\n        BASE_DIR = path\n        break\n\nif BASE_DIR is None:\n    BASE_DIR = os.getcwd()\n    print(\"WARNING: Known directory not found:\", BASE_DIR)\n\nprint(\"Working directory:\", BASE_DIR)\n\n# Configuration\nFORGE_DIR        = os.path.join(BASE_DIR, \"stable-diffusion-webui-forge\")\nMODELS_DIR       = os.path.join(BASE_DIR, \"stable-diffusion-webui-forge\", \"models\", \"Stable-diffusion\")\nLORA_DIR         = os.path.join(BASE_DIR, \"stable-diffusion-webui-forge\", \"models\", \"Lora\")\nCONTROLNET_DIR   = os.path.join(FORGE_DIR, \"extensions\", \"sd-webui-controlnet\")\nCONTROLNET_MODELS_DIR = os.path.join(CONTROLNET_DIR, \"models\")\nEXTENSIONS_DIR   = os.path.join(FORGE_DIR, \"extensions\")\nOUTPUTS_DIR      = os.path.join(FORGE_DIR, \"outputs\")\nVOLUME_DIR       = os.path.join(BASE_DIR, \"volume\")\nGEN_DIR          = os.path.join(BASE_DIR, \"gen\")\n\nfor d in [MODELS_DIR, LORA_DIR, CONTROLNET_DIR, CONTROLNET_MODELS_DIR, EXTENSIONS_DIR, OUTPUTS_DIR, VOLUME_DIR, GEN_DIR]:\n    os.makedirs(d, exist_ok=True)\n\n# Dependencies used by generation cell\nfor pkg in [\"openpyxl\", \"requests\"]:\n    try:\n        __import__(pkg)\n    except Exception:\n        print(f\"Installing missing dependency: {pkg}\")\n        subprocess.run([\"python\", \"-m\", \"pip\", \"install\", \"-q\", pkg], check=False)\n\n\ndef get_secret(name: str):\n    \"\"\"Get secret from env/Kaggle/Colab only.\"\"\"\n    value = os.environ.get(name)\n    if value:\n        return value.strip(), \"env\"\n\n    # Kaggle secrets\n    if ON_KAGGLE:\n        try:\n            from kaggle_secrets import UserSecretsClient\n            value = UserSecretsClient().get_secret(name)\n            if value:\n                return value.strip(), \"kaggle_secrets\"\n        except Exception:\n            pass\n\n    # Colab secrets panel: from google.colab import userdata\n    if ON_COLAB:\n        try:\n            from google.colab import userdata\n            value = userdata.get(name)\n            if value:\n                return value.strip(), \"colab_userdata\"\n        except Exception:\n            pass\n\n    return None, None\n\n\nCIVITAI_TOKEN, CIVITAI_SRC = get_secret(\"CIVITAI_TOKEN\")\nHF_TOKEN, HF_SRC = get_secret(\"HF_TOKEN\")\n\nif not CIVITAI_TOKEN:\n    manual_civitai = getpass(\"Enter CIVITAI_TOKEN (leave blank to skip): \").strip()\n    if manual_civitai:\n        CIVITAI_TOKEN, CIVITAI_SRC = manual_civitai, \"manual_input\"\n\nif not HF_TOKEN:\n    manual_hf = getpass(\"Enter HF_TOKEN (leave blank to skip): \").strip()\n    if manual_hf:\n        HF_TOKEN, HF_SRC = manual_hf, \"manual_input\"\n\nTOKENS = {}\nif CIVITAI_TOKEN:\n    TOKENS[\"CIVITAI\"] = CIVITAI_TOKEN\nif HF_TOKEN:\n    TOKENS[\"HF_TOKEN\"] = HF_TOKEN\n\nprint(\"Token sources:\")\nprint(f\"  CIVITAI_TOKEN: {CIVITAI_SRC or 'not found'}\")\nprint(f\"  HF_TOKEN: {HF_SRC or 'not found'}\")\nif ON_VAST:\n    print(\"Vast.ai tip: add CIVITAI_TOKEN/HF_TOKEN in template env vars, restart container, then rerun this cell.\")\n\nif not CIVITAI_TOKEN:\n    print(\"CivitAI token not found\")\nif not HF_TOKEN:\n    print(\"HF token not found\")\nif not TOKENS:\n    raise RuntimeError(\"No tokens were provided. Set secrets or enter at least one token (CivitAI or HF).\")\n\n\ndef _prepare_download_url(url, token):\n    \"\"\"CivitAI download works more reliably with token as query param.\"\"\"\n    if token and \"civitai.com/api/download/models\" in url and \"token=\" not in url:\n        sep = \"&\" if \"?\" in url else \"?\"\n        return f\"{url}{sep}{urlencode({'token': token})}\"\n    return url\n\n\ndef _looks_valid_file(path, min_bytes=MIN_VALID_FILE_BYTES):\n    return os.path.exists(path) and os.path.getsize(path) > min_bytes\n\n\ndef _human_mb(num_bytes):\n    return f\"{num_bytes / (1024 * 1024):.1f} MB\"\n\n\ndef _estimate_expected_mb(label):\n    # Examples: \"151 MB\", \"6,46 GB\"\n    match = re.search(r\"(\\d+[\\.,]?\\d*)\\s*(MB|GB)\", label, re.IGNORECASE)\n    if not match:\n        return None\n    value = float(match.group(1).replace(',', '.'))\n    unit = match.group(2).upper()\n    return value * (1024 if unit == \"GB\" else 1)\n\n\ndef _size_sanity_warning(path, expected_mb, tolerance=0.7):\n    if expected_mb is None or not os.path.exists(path):\n        return\n    actual_mb = os.path.getsize(path) / (1024 * 1024)\n    if actual_mb < expected_mb * tolerance:\n        print(f\"  WARNING: file size looks low ({actual_mb:.1f} MB vs expected ~{expected_mb:.1f} MB)\")\n\n\ndef _has_min_free_disk(path, required_mb, reserve_mb=1024):\n    if required_mb is None:\n        return True\n    usage = shutil.disk_usage(path)\n    free_mb = usage.free / (1024 * 1024)\n    return free_mb >= (required_mb + reserve_mb)\n\n\ndef _download_one(job, target_dir):\n    label, url, filename, token_name = job\n    token = TOKENS.get(token_name)\n    output_path = os.path.join(target_dir, filename)\n\n    expected_mb = _estimate_expected_mb(label)\n\n    if _looks_valid_file(output_path):\n        size = os.path.getsize(output_path)\n        print(f\"[SKIP] {label}: already exists ({_human_mb(size)})\")\n        _size_sanity_warning(output_path, expected_mb)\n        return (label, True, \"exists\")\n\n    if expected_mb is not None and not _has_min_free_disk(target_dir, expected_mb):\n        return (label, False, \"insufficient_disk\")\n\n    tmp_path = output_path + \".part\"\n    if os.path.exists(tmp_path):\n        os.remove(tmp_path)\n\n    final_url = _prepare_download_url(url, token if token_name == \"CIVITAI\" else None)\n\n    cmd = [\n        \"aria2c\",\n        \"--allow-overwrite=true\",\n        \"--auto-file-renaming=false\",\n        \"--continue=true\",\n        \"--max-connection-per-server=16\",\n        \"--split=16\",\n        \"--min-split-size=1M\",\n        \"--console-log-level=warn\",\n        \"--summary-interval=1\",\n        \"--check-certificate=false\",\n        \"--out\", os.path.basename(tmp_path),\n        \"--dir\", target_dir,\n        final_url,\n    ]\n\n    if token_name == \"HF_TOKEN\" and token:\n        cmd.insert(-1, f\"--header=Authorization: Bearer {token}\")\n\n    print(f\"[DOWNLOADING] {label}\")\n    result = subprocess.run(cmd, text=True, capture_output=True)\n    if result.returncode != 0:\n        stderr = (result.stderr or \"\").strip()\n        stdout = (result.stdout or \"\").strip()\n        msg = stderr or stdout or f\"aria2c exited {result.returncode}\"\n        if os.path.exists(tmp_path):\n            os.remove(tmp_path)\n        return (label, False, msg)\n\n    if not _looks_valid_file(tmp_path):\n        size = os.path.getsize(tmp_path) if os.path.exists(tmp_path) else 0\n        if os.path.exists(tmp_path):\n            os.remove(tmp_path)\n        return (label, False, f\"downloaded file too small ({_human_mb(size)})\")\n\n    os.replace(tmp_path, output_path)\n    _size_sanity_warning(output_path, expected_mb)\n    return (label, True, _human_mb(os.path.getsize(output_path)))\n\n\ndef run_download_list(download_list, target_dir, title):\n    print(f\"\\n=== {title} ===\")\n    os.makedirs(target_dir, exist_ok=True)\n\n    if not download_list:\n        print(\"No items.\")\n        return\n\n    workers = min(MAX_PARALLEL_DOWNLOADS, len(download_list))\n    print(f\"Parallel downloads: {workers}\")\n\n    ok = 0\n    fail = 0\n\n    with ThreadPoolExecutor(max_workers=workers) as ex:\n        futures = [ex.submit(_download_one, job, target_dir) for job in download_list]\n        for fut in as_completed(futures):\n            label, success, info = fut.result()\n            if success:\n                ok += 1\n                print(f\"[OK]   {label} -> {info}\")\n            else:\n                fail += 1\n                print(f\"[FAIL] {label} -> {info}\")\n\n    print(f\"Done: OK={ok}, FAIL={fail}\")\n    if fail > 0:\n        raise RuntimeError(f\"Some downloads failed in {title}: {fail} item(s)\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T00:06:14.768893Z","iopub.execute_input":"2026-02-17T00:06:14.769191Z","iopub.status.idle":"2026-02-17T00:06:14.931370Z","shell.execute_reply.started":"2026-02-17T00:06:14.769165Z","shell.execute_reply":"2026-02-17T00:06:14.930711Z"}},"outputs":[{"name":"stdout","text":"aria2c already available\nWorking directory: /kaggle/working\nToken sources:\n  CIVITAI_TOKEN: kaggle_secrets\n  HF_TOKEN: kaggle_secrets\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"<div style=\"color:rgb(0,0,255);font-size: 40px;font-weight:700;\">\nDOWNLOAD BLOCK\n</div>","metadata":{}},{"cell_type":"code","source":"### CONTROLNET INSTALL OPTIONS ###\n\n# Option A (default): force clean reinstall\n# Option B: set CONTROLNET_INSTALL_MODE = \"update\" for git pull in existing repo\n# Option C: set CONTROLNET_INSTALL_MODE = \"skip\" to keep current state\n\nCONTROLNET_REPO_URL = \"https://github.com/Mikubill/sd-webui-controlnet\"\nCONTROLNET_INSTALL_MODE = os.environ.get(\"CONTROLNET_INSTALL_MODE\", \"reinstall\").strip().lower()\n\nif CONTROLNET_INSTALL_MODE not in {\"reinstall\", \"update\", \"skip\"}:\n    raise ValueError(\"CONTROLNET_INSTALL_MODE must be one of: reinstall, update, skip\")\n\nprint(f\"ControlNet extension path: {CONTROLNET_DIR}\")\nprint(f\"Install mode: {CONTROLNET_INSTALL_MODE}\")\n\nif CONTROLNET_INSTALL_MODE == \"skip\":\n    print(\"ControlNet install skipped\")\nelse:\n    if os.path.isdir(CONTROLNET_DIR) and CONTROLNET_INSTALL_MODE == \"reinstall\":\n        print(\"Removing existing ControlNet directory...\")\n        shutil.rmtree(CONTROLNET_DIR)\n\n    if not os.path.isdir(CONTROLNET_DIR):\n        print(\"Cloning ControlNet repository...\")\n        subprocess.run([\"git\", \"clone\", CONTROLNET_REPO_URL, CONTROLNET_DIR], check=True)\n    else:\n        print(\"Updating ControlNet repository...\")\n        subprocess.run([\"git\", \"-C\", CONTROLNET_DIR, \"pull\", \"--ff-only\"], check=True)\n\nos.makedirs(CONTROLNET_MODELS_DIR, exist_ok=True)\nprint(\"ControlNet repository is ready\")\nprint(f\"ControlNet models directory: {CONTROLNET_MODELS_DIR}\")\n\n#ControlNET models download\n\ncontrolnet_models_to_download = [\n    (\"t2i-adapter_xl_openpose 151 MB\", \"https://huggingface.co/lllyasviel/sd_control_collection/resolve/main/t2i-adapter_xl_openpose.safetensors\", \"t2i-adapter_xl_openpose.safetensors\", \"HF_TOKEN\"),\n    (\"t2i-adapter_xl_canny 148 MB\", \"https://huggingface.co/lllyasviel/sd_control_collection/resolve/main/t2i-adapter_xl_canny.safetensors\", \"t2i-adapter_xl_canny.safetensors\", \"HF_TOKEN\"),\n    (\"t2i-adapter_xl_sketch 148 MB\", \"https://huggingface.co/lllyasviel/sd_control_collection/resolve/main/t2i-adapter_xl_sketch.safetensors\", \"t2i-adapter_xl_sketch.safetensors\", \"HF_TOKEN\"),\n    (\"t2i-adapter_diffusers_xl_depth_midas 151 MB\", \"https://huggingface.co/lllyasviel/sd_control_collection/resolve/main/t2i-adapter_diffusers_xl_depth_midas.safetensors\", \"t2i-adapter_diffusers_xl_depth_midas.safetensors\", \"HF_TOKEN\"),\n    (\"t2i-adapter_diffusers_xl_depth_zoe 151 MB\", \"https://huggingface.co/lllyasviel/sd_control_collection/resolve/main/t2i-adapter_diffusers_xl_depth_zoe.safetensors\", \"t2i-adapter_diffusers_xl_depth_zoe.safetensors\", \"HF_TOKEN\"),\n    (\"t2i-adapter_diffusers_xl_lineart 151 MB\", \"https://huggingface.co/lllyasviel/sd_control_collection/resolve/main/t2i-adapter_diffusers_xl_lineart.safetensors\", \"t2i-adapter_diffusers_xl_lineart.safetensors\", \"HF_TOKEN\"),\n]\n\nrun_download_list(controlnet_models_to_download, CONTROLNET_MODELS_DIR, \"ControlNet\")\n\n#Checkpoints download\n\nmodels_to_download = [\n    (\"WAI ILL V16.0 6,46 GB\", \"https://civitai.com/api/download/models/2514310?type=Model&format=SafeTensor&size=pruned&fp=fp16\", \"wai_v160.safetensors\", \"CIVITAI\"),\n]\n\nrun_download_list(models_to_download, MODELS_DIR, \"Checkpoints\")\n\n#LoRa download\n\nlora_to_download = [\n    (\"Detailer IL V2 218 MB\",        \"https://civitai.com/api/download/models/1736373?type=Model&format=SafeTensor\",    \"detailer_v2_il.safetensors\",     \"CIVITAI\"),\n    (\"Realistic filter V1 55 MB\",    \"https://civitai.com/api/download/models/1124771?type=Model&format=SafeTensor\",    \"realistic_filter_v1_il.safetensors\", \"CIVITAI\"),\n    (\"Hyperrealistic V4 ILL 435 MB\", \"https://civitai.com/api/download/models/1914557?type=Model&format=SafeTensor\",    \"hyperrealistic_v4_ill.safetensors\",  \"CIVITAI\"),\n    (\"Niji semi realism V3.5 ILL 435 MB\", \"https://civitai.com/api/download/models/1882710?type=Model&format=SafeTensor\", \"niji_v35.safetensors\", \"CIVITAI\"),\n    (\"ATNR Style ILL V1.1 350 MB\", \"https://civitai.com/api/download/models/1711464?type=Model&format=SafeTensor\", \"atnr_style_ill_v1.1.safetensors\", \"CIVITAI\"),\n    (\"Face Enhancer Ill 218 MB\", \"https://civitai.com/api/download/models/1839268?type=Model&format=SafeTensor\", \"face_enhancer_ill.safetensors\", \"CIVITAI\"),\n    (\"Smooth Detailer Booster V4 243 MB\", \"https://civitai.com/api/download/models/2196453?type=Model&format=SafeTensor\", \"smooth_detailer_booster_v4.safetensors\", \"CIVITAI\"),\n    (\"USNR Style V-pred 157 MB\", \"https://civitai.com/api/download/models/2555444?type=Model&format=SafeTensor\", \"usnr_style.safetensors\", \"CIVITAI\"),\n    (\"748cm Style V1 243 MB\", \"https://civitai.com/api/download/models/1056404?type=Model&format=SafeTensor\", \"748cm_style_v1.safetensors\", \"CIVITAI\"),\n    (\"Velvet's Mythic Fantasy Styles IL 218 MB\", \"https://civitai.com/api/download/models/2620790?type=Model&format=SafeTensor\", \"velvets_styles.safetensors\", \"CIVITAI\"),\n    (\"Pixel Art Style IL V7 435 MB\", \"https://civitai.com/api/download/models/2661972?type=Model&format=SafeTensor\", \"pixel_art.safetensors\", \"CIVITAI\"),\n]\n\nrun_download_list(lora_to_download, LORA_DIR, \"LoRA\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T00:06:23.078040Z","iopub.execute_input":"2026-02-17T00:06:23.078711Z","iopub.status.idle":"2026-02-17T00:07:33.021892Z","shell.execute_reply.started":"2026-02-17T00:06:23.078685Z","shell.execute_reply":"2026-02-17T00:07:33.021102Z"}},"outputs":[{"name":"stdout","text":"ControlNet extension path: /kaggle/working/stable-diffusion-webui-forge/extensions/sd-webui-controlnet\nInstall mode: reinstall\nRemoving existing ControlNet directory...\nCloning ControlNet repository...\n","output_type":"stream"},{"name":"stderr","text":"Cloning into '/kaggle/working/stable-diffusion-webui-forge/extensions/sd-webui-controlnet'...\n","output_type":"stream"},{"name":"stdout","text":"ControlNet repository is ready\nControlNet models directory: /kaggle/working/stable-diffusion-webui-forge/extensions/sd-webui-controlnet/models\n\n=== ControlNet ===\nParallel downloads: 3\n[DOWNLOADING] t2i-adapter_xl_openpose 151 MB\n[DOWNLOADING] t2i-adapter_xl_canny 148 MB\n[DOWNLOADING] t2i-adapter_xl_sketch 148 MB\n[OK]   t2i-adapter_xl_openpose 151 MB -> 150.7 MB\n[DOWNLOADING] t2i-adapter_diffusers_xl_depth_midas 151 MB\n[DOWNLOADING] t2i-adapter_diffusers_xl_depth_zoe 151 MB\n[OK]   t2i-adapter_xl_canny 148 MB -> 147.9 MB\n[OK]   t2i-adapter_xl_sketch 148 MB -> 147.9 MB\n[DOWNLOADING] t2i-adapter_diffusers_xl_lineart 151 MB\n[OK]   t2i-adapter_diffusers_xl_depth_zoe 151 MB -> 150.7 MB\n[OK]   t2i-adapter_diffusers_xl_lineart 151 MB -> 150.7 MB\n[OK]   t2i-adapter_diffusers_xl_depth_midas 151 MB -> 150.7 MB\nDone: OK=6, FAIL=0\n\n=== Checkpoints ===\nParallel downloads: 1\n[DOWNLOADING] WAI ILL V16.0 6,46 GB\n[OK]   WAI ILL V16.0 6,46 GB -> 6616.6 MB\nDone: OK=1, FAIL=0\n\n=== LoRA ===\nParallel downloads: 3\n[DOWNLOADING] Realistic filter V1 55 MB\n[DOWNLOADING] Hyperrealistic V4 ILL 435 MB\n[DOWNLOADING] Detailer IL V2 218 MB\n[OK]   Realistic filter V1 55 MB -> 54.8 MB\n[DOWNLOADING] Niji semi realism V3.5 ILL 435 MB\n[OK]   Detailer IL V2 218 MB -> 217.9 MB\n[DOWNLOADING] ATNR Style ILL V1.1 350 MB\n[OK]   Hyperrealistic V4 ILL 435 MB -> 435.4 MB\n[DOWNLOADING] Face Enhancer Ill 218 MB\n[OK]   Face Enhancer Ill 218 MB -> 217.9 MB\n[DOWNLOADING] Smooth Detailer Booster V4 243 MB\n[DOWNLOADING] USNR Style V-pred 157 MB\n[OK]   Niji semi realism V3.5 ILL 435 MB -> 435.4 MB\n[OK]   ATNR Style ILL V1.1 350 MB -> 350.2 MB\n[DOWNLOADING] 748cm Style V1 243 MB\n[DOWNLOADING] Velvet's Mythic Fantasy Styles IL 218 MB\n[OK]   Smooth Detailer Booster V4 243 MB -> 243.2 MB\n[OK]   USNR Style V-pred 157 MB -> 156.9 MB\n[DOWNLOADING] Pixel Art Style IL V7 435 MB\n  WARNING: file size looks low (217.9 MB vs expected ~435.0 MB)\n[OK]   Pixel Art Style IL V7 435 MB -> 217.9 MB\n[OK]   Velvet's Mythic Fantasy Styles IL 218 MB -> 217.9 MB\n[OK]   748cm Style V1 243 MB -> 243.2 MB\nDone: OK=11, FAIL=0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"<div style=\"color:rgb(0,0,255);font-size: 40px;font-weight:700;\">\nARCHIVE+OUTPUT\n</div>","metadata":{}},{"cell_type":"code","source":"### API GENERATION FROM XLSX ###\n\nimport os\nimport re\nimport json\nimport time\nimport shutil\nimport zipfile\nimport base64\nfrom datetime import datetime\n\nimport requests\nfrom openpyxl import load_workbook\n\nAPI_BASE = os.environ.get(\"FORGE_API_BASE\", \"http://127.0.0.1:17860\")\nTXT2IMG_URL = f\"{API_BASE}/sdapi/v1/txt2img\"\nOPENAPI_URL = f\"{API_BASE}/openapi.json\"\n\nPROMPTS_CANDIDATES = [\n    os.path.join(GEN_DIR, \"prompts.xlsx\"),\n    os.path.join(GEN_DIR, \"prompts.xlxs\"),\n    \"/workspace/gen/prompts.xlsx\",\n    \"/workspace/gen/prompts.xlxs\",\n]\n\nLOG_PATH = os.path.join(VOLUME_DIR, \"log.txt\")\nAPI_IMAGES_DIR = os.path.join(OUTPUTS_DIR, \"api_generated\")\nos.makedirs(API_IMAGES_DIR, exist_ok=True)\n\n\nCONFLICT_RULES = [\n    ((\"hr_resize_x\", \"hr_resize_y\"), (\"hr_scale\",)),\n]\n\n\ndef log_line(text: str):\n    ts = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    line = f\"[{ts}] {text}\"\n    print(line)\n    with open(LOG_PATH, \"a\", encoding=\"utf-8\") as f:\n        f.write(line + \"\\n\")\n\n\ndef normalize_value(value):\n    if isinstance(value, str):\n        stripped = value.strip()\n        lowered = stripped.lower()\n        if lowered in {\"\", \"null\", \"none\", \"nan\"}:\n            return None\n        if lowered in {\"true\", \"yes\", \"1\"}:\n            return True\n        if lowered in {\"false\", \"no\", \"0\"}:\n            return False\n        if re.fullmatch(r\"-?\\d+\", stripped):\n            return int(stripped)\n        if re.fullmatch(r\"-?\\d+\\.\\d+\", stripped):\n            return float(stripped)\n        return stripped\n    return value\n\n\ndef first_existing_path(candidates):\n    for p in candidates:\n        if os.path.exists(p):\n            return p\n    return None\n\n\ndef parse_workbook(path):\n    wb = load_workbook(path, data_only=True)\n    ws = wb.active\n\n    instruction_parts = [str(v).strip() for v in ws[1] if v is not None and str(v).strip()]\n    if not instruction_parts:\n        raise ValueError(\"Первая строка (инструкция) пустая\")\n    instruction = \" \".join(instruction_parts)\n\n    headers = [str(v).strip() if v is not None else \"\" for v in ws[2]]\n    if not any(headers):\n        raise ValueError(\"Вторая строка должна содержать имена переменных (заголовки столбцов)\")\n\n    rows = []\n    for row_idx in range(3, ws.max_row + 1):\n        row_values = [normalize_value(v) for v in next(ws.iter_rows(min_row=row_idx, max_row=row_idx, values_only=True))]\n        if all(v is None for v in row_values):\n            continue\n\n        row_map = {}\n        for idx, key in enumerate(headers):\n            if not key:\n                continue\n            value = row_values[idx] if idx < len(row_values) else None\n            if value is None:\n                continue\n            row_map[key] = value\n        rows.append(row_map)\n\n    if not rows:\n        raise ValueError(\"Не найдено строк данных (начиная с 3-й строки)\")\n\n    return instruction, rows\n\n\ndef render_instruction(template: str, variables: dict):\n    def repl(match):\n        key = match.group(1)\n        return str(variables.get(key, match.group(0)))\n    return re.sub(r\"\\{([a-zA-Z0-9_]+)\\}\", repl, template)\n\n\ndef apply_conflict_rules(payload: dict):\n    cleaned = dict(payload)\n    for primary_keys, conflicting_keys in CONFLICT_RULES:\n        primary_present = all((k in cleaned and cleaned[k] is not None) for k in primary_keys)\n        if primary_present:\n            for ck in conflicting_keys:\n                cleaned.pop(ck, None)\n    return cleaned\n\n\ndef fetch_txt2img_params_dump():\n    try:\n        response = requests.get(OPENAPI_URL, timeout=30)\n        response.raise_for_status()\n        spec = response.json()\n        schemas = spec.get(\"components\", {}).get(\"schemas\", {})\n        candidates = [\n            \"StableDiffusionTxt2ImgProcessingApi\",\n            \"Txt2ImgRequest\",\n            \"StableDiffusionProcessingTxt2Img\",\n        ]\n        for name in candidates:\n            if name in schemas:\n                props = schemas[name].get(\"properties\", {})\n                return json.dumps({k: v.get(\"type\", \"unknown\") for k, v in props.items()}, ensure_ascii=False, indent=2)\n        return json.dumps(spec.get(\"paths\", {}).get(\"/sdapi/v1/txt2img\", {}), ensure_ascii=False, indent=2)\n    except Exception as e:\n        return f\"Не удалось получить список переменных: {e}\"\n\n\ndef save_images_from_response(images_b64, generation_idx):\n    saved = 0\n    for i, b64 in enumerate(images_b64, start=1):\n        image_data = b64.split(\",\", 1)[-1]\n        file_name = f\"gen_{generation_idx:04d}_{i:02d}.png\"\n        file_path = os.path.join(API_IMAGES_DIR, file_name)\n        with open(file_path, \"wb\") as f:\n            f.write(base64.b64decode(image_data))\n        saved += 1\n    return saved\n\n\ndef archive_outputs(tag: str):\n    stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    archive_name = os.path.join(VOLUME_DIR, f\"outputs_{tag}_{stamp}.zip\")\n    with zipfile.ZipFile(archive_name, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n        for root, _, files in os.walk(OUTPUTS_DIR):\n            for file in files:\n                full_path = os.path.join(root, file)\n                rel_path = os.path.relpath(full_path, OUTPUTS_DIR)\n                zf.write(full_path, rel_path)\n\n    for entry in os.listdir(OUTPUTS_DIR):\n        p = os.path.join(OUTPUTS_DIR, entry)\n        if os.path.isdir(p):\n            shutil.rmtree(p)\n        else:\n            os.remove(p)\n    os.makedirs(API_IMAGES_DIR, exist_ok=True)\n    log_line(f\"ARCHIVE created: {archive_name}. OUTPUTS_DIR cleaned.\")\n\n\nprompts_path = first_existing_path(PROMPTS_CANDIDATES)\nif not prompts_path:\n    raise FileNotFoundError(\n        \"Файл prompts.xlsx/prompts.xlxs не найден. Ожидались пути: \" + \", \".join(PROMPTS_CANDIDATES)\n    )\n\ninstruction_template, generation_rows = parse_workbook(prompts_path)\nlog_line(f\"Loaded prompts file: {prompts_path}. Rows for generation: {len(generation_rows)}\")\n\nsyntax_error_dumped = False\nimages_since_archive = 0\n\nfor generation_idx, row_values in enumerate(generation_rows, start=1):\n    payload = dict(row_values)\n\n    if \"prompt\" not in payload:\n        payload[\"prompt\"] = render_instruction(instruction_template, row_values)\n\n    payload = {k: v for k, v in payload.items() if v is not None}\n    payload = apply_conflict_rules(payload)\n\n    try:\n        response = requests.post(TXT2IMG_URL, json=payload, timeout=1800)\n        if response.status_code >= 400:\n            err_text = response.text[:1200]\n            if not syntax_error_dumped:\n                params_dump = fetch_txt2img_params_dump()\n                log_line(f\"#{generation_idx} FAIL syntax/validation: {response.status_code} {err_text}\")\n                log_line(\"AVAILABLE PARAMS DUMP START\")\n                with open(LOG_PATH, \"a\", encoding=\"utf-8\") as f:\n                    f.write(params_dump + \"\\n\")\n                log_line(\"AVAILABLE PARAMS DUMP END\")\n                syntax_error_dumped = True\n            else:\n                log_line(f\"#{generation_idx} FAIL: {response.status_code} {err_text}\")\n            continue\n\n        result = response.json()\n        images = result.get(\"images\", []) or []\n        saved_now = save_images_from_response(images, generation_idx)\n        images_since_archive += saved_now\n        log_line(f\"#{generation_idx} OK images={saved_now}\")\n\n        if images_since_archive >= 15:\n            archive_outputs(tag=f\"part_{generation_idx:04d}\")\n            images_since_archive = 0\n\n    except requests.exceptions.Timeout:\n        log_line(f\"#{generation_idx} FAIL: timeout (possible heavy generation or API freeze)\")\n    except requests.exceptions.RequestException as e:\n        reason = str(e)\n        if \"out of memory\" in reason.lower() or \"oom\" in reason.lower():\n            reason = \"OOM\"\n        log_line(f\"#{generation_idx} FAIL: {reason}\")\n    except Exception as e:\n        log_line(f\"#{generation_idx} FAIL: unexpected error: {e}\")\n\nif images_since_archive > 0:\n    archive_outputs(tag=\"final\")\n\nlog_line(\"Generation cycle completed\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T00:08:59.188372Z","iopub.execute_input":"2026-02-17T00:08:59.189033Z","iopub.status.idle":"2026-02-17T00:08:59.214624Z","shell.execute_reply.started":"2026-02-17T00:08:59.189003Z","shell.execute_reply":"2026-02-17T00:08:59.213833Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/2692662679.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0mprompts_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_existing_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPROMPTS_CANDIDATES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mprompts_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m     raise FileNotFoundError(\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;34m\"Файл prompts.xlsx/prompts.xlxs не найден. Ожидались пути: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPROMPTS_CANDIDATES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     )\n","\u001b[0;31mFileNotFoundError\u001b[0m: Файл prompts.xlsx/prompts.xlxs не найден. Ожидались пути: /kaggle/working/gen/prompts.xlsx, /kaggle/working/gen/prompts.xlxs, /workspace/gen/prompts.xlsx, /workspace/gen/prompts.xlxs"],"ename":"FileNotFoundError","evalue":"Файл prompts.xlsx/prompts.xlxs не найден. Ожидались пути: /kaggle/working/gen/prompts.xlsx, /kaggle/working/gen/prompts.xlxs, /workspace/gen/prompts.xlsx, /workspace/gen/prompts.xlxs","output_type":"error"}],"execution_count":6},{"cell_type":"code","source":"### OPTIONAL: KAGGLE/COLAB FORGE BOOTSTRAP ###\n\nimport os\nimport subprocess\n\nif not (ON_KAGGLE or ON_COLAB):\n    print(\"Optional cell: предназначена только для Kaggle/Colab. Текущая платформа пропущена.\")\nelse:\n    required_packages = [\"git\", \"python3-venv\", \"python3-pip\"]\n    print(\"Checking/installing platform dependencies...\")\n    subprocess.run([\"apt\", \"update\", \"-qq\"], check=False)\n    subprocess.run([\"apt\", \"install\", \"-y\", \"-qq\", *required_packages], check=False)\n\n    if not os.path.isdir(FORGE_DIR):\n        print(\"Cloning WebUI Forge...\")\n        subprocess.run([\n            \"git\", \"clone\", \"https://github.com/lllyasviel/stable-diffusion-webui-forge\", FORGE_DIR\n        ], check=False)\n    else:\n        print(\"WebUI Forge already exists, skipping clone.\")\n\n    print(\"Optional bootstrap finished.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T00:07:54.360536Z","iopub.execute_input":"2026-02-17T00:07:54.361252Z","iopub.status.idle":"2026-02-17T00:08:15.547959Z","shell.execute_reply.started":"2026-02-17T00:07:54.361221Z","shell.execute_reply":"2026-02-17T00:08:15.547254Z"}},"outputs":[{"name":"stdout","text":"Checking/installing platform dependencies...\n","output_type":"stream"},{"name":"stderr","text":"\nWARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n\nW: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n\nWARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n\n","output_type":"stream"},{"name":"stdout","text":"171 packages can be upgraded. Run 'apt list --upgradable' to see them.\ngit is already the newest version (1:2.34.1-1ubuntu1.15).\nThe following additional packages will be installed:\n  libpython3.10 libpython3.10-dev libpython3.10-minimal libpython3.10-stdlib\n  python3-pip-whl python3-pkg-resources python3-setuptools\n  python3-setuptools-whl python3-wheel python3.10 python3.10-dev\n  python3.10-minimal python3.10-venv\nSuggested packages:\n  python-setuptools-doc python3.10-doc binfmt-support\nThe following NEW packages will be installed:\n  python3-pip python3-pip-whl python3-setuptools python3-setuptools-whl\n  python3-venv python3-wheel python3.10-venv\nThe following packages will be upgraded:\n  libpython3.10 libpython3.10-dev libpython3.10-minimal libpython3.10-stdlib\n  python3-pkg-resources python3.10 python3.10-dev python3.10-minimal\n8 upgraded, 7 newly installed, 0 to remove and 163 not upgraded.\nNeed to get 17.2 MB of archives.\nAfter this operation, 12.4 MB of additional disk space will be used.\n(Reading database ... 129147 files and directories currently installed.)\nPreparing to unpack .../00-python3.10-dev_3.10.12-1~22.04.14_amd64.deb ...\nUnpacking python3.10-dev (3.10.12-1~22.04.14) over (3.10.12-1~22.04.11) ...\nPreparing to unpack .../01-libpython3.10-dev_3.10.12-1~22.04.14_amd64.deb ...\nUnpacking libpython3.10-dev:amd64 (3.10.12-1~22.04.14) over (3.10.12-1~22.04.11) ...\nPreparing to unpack .../02-libpython3.10_3.10.12-1~22.04.14_amd64.deb ...\nUnpacking libpython3.10:amd64 (3.10.12-1~22.04.14) over (3.10.12-1~22.04.11) ...\nPreparing to unpack .../03-python3.10_3.10.12-1~22.04.14_amd64.deb ...\nUnpacking python3.10 (3.10.12-1~22.04.14) over (3.10.12-1~22.04.11) ...\nPreparing to unpack .../04-libpython3.10-stdlib_3.10.12-1~22.04.14_amd64.deb ...\nUnpacking libpython3.10-stdlib:amd64 (3.10.12-1~22.04.14) over (3.10.12-1~22.04.11) ...\nPreparing to unpack .../05-python3.10-minimal_3.10.12-1~22.04.14_amd64.deb ...\nUnpacking python3.10-minimal (3.10.12-1~22.04.14) over (3.10.12-1~22.04.11) ...\nPreparing to unpack .../06-libpython3.10-minimal_3.10.12-1~22.04.14_amd64.deb ...\nUnpacking libpython3.10-minimal:amd64 (3.10.12-1~22.04.14) over (3.10.12-1~22.04.11) ...\nPreparing to unpack .../07-python3-pkg-resources_68.1.2-2~jammy3_all.deb ...\nUnpacking python3-pkg-resources (68.1.2-2~jammy3) over (59.6.0-1.2ubuntu0.22.04.3) ...\nSelecting previously unselected package python3-setuptools.\nPreparing to unpack .../08-python3-setuptools_68.1.2-2~jammy3_all.deb ...\nUnpacking python3-setuptools (68.1.2-2~jammy3) ...\nSelecting previously unselected package python3-wheel.\nPreparing to unpack .../09-python3-wheel_0.37.1-2ubuntu0.22.04.1_all.deb ...\nUnpacking python3-wheel (0.37.1-2ubuntu0.22.04.1) ...\nSelecting previously unselected package python3-pip.\nPreparing to unpack .../10-python3-pip_22.0.2+dfsg-1ubuntu0.7_all.deb ...\nUnpacking python3-pip (22.0.2+dfsg-1ubuntu0.7) ...\nSelecting previously unselected package python3-pip-whl.\nPreparing to unpack .../11-python3-pip-whl_22.0.2+dfsg-1ubuntu0.7_all.deb ...\nUnpacking python3-pip-whl (22.0.2+dfsg-1ubuntu0.7) ...\nSelecting previously unselected package python3-setuptools-whl.\nPreparing to unpack .../12-python3-setuptools-whl_68.1.2-2~jammy3_all.deb ...\nUnpacking python3-setuptools-whl (68.1.2-2~jammy3) ...\nSelecting previously unselected package python3.10-venv.\nPreparing to unpack .../13-python3.10-venv_3.10.12-1~22.04.14_amd64.deb ...\nUnpacking python3.10-venv (3.10.12-1~22.04.14) ...\nSelecting previously unselected package python3-venv.\nPreparing to unpack .../14-python3-venv_3.10.6-1~22.04.1_amd64.deb ...\nUnpacking python3-venv (3.10.6-1~22.04.1) ...\nSetting up python3-pkg-resources (68.1.2-2~jammy3) ...\nSetting up python3-setuptools-whl (68.1.2-2~jammy3) ...\nSetting up python3-setuptools (68.1.2-2~jammy3) ...\nSetting up python3-pip-whl (22.0.2+dfsg-1ubuntu0.7) ...\nSetting up python3-wheel (0.37.1-2ubuntu0.22.04.1) ...\nSetting up libpython3.10-minimal:amd64 (3.10.12-1~22.04.14) ...\nSetting up python3-pip (22.0.2+dfsg-1ubuntu0.7) ...\nSetting up python3.10-minimal (3.10.12-1~22.04.14) ...\nSetting up libpython3.10-stdlib:amd64 (3.10.12-1~22.04.14) ...\nSetting up libpython3.10:amd64 (3.10.12-1~22.04.14) ...\nSetting up python3.10 (3.10.12-1~22.04.14) ...\nSetting up libpython3.10-dev:amd64 (3.10.12-1~22.04.14) ...\nSetting up python3.10-dev (3.10.12-1~22.04.14) ...\nSetting up python3.10-venv (3.10.12-1~22.04.14) ...\nSetting up python3-venv (3.10.6-1~22.04.1) ...\nProcessing triggers for libc-bin (2.35-0ubuntu3.8) ...\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n\nProcessing triggers for man-db (2.10.2-1) ...\nProcessing triggers for mailcap (3.70+nmu1ubuntu1) ...\nWebUI Forge already exists, skipping clone.\nOptional bootstrap finished.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"### OPTIONAL: START WEBUI FORGE IN BACKGROUND ###\n\nimport os\nimport subprocess\n\nFORGE_ARGS = \"--xformers --api --cuda-malloc --cuda-stream --pin-shared-memory --theme dark --port 17860\"\n\nif not os.path.isdir(FORGE_DIR):\n    raise FileNotFoundError(f\"Forge directory not found: {FORGE_DIR}\")\n\nlaunch_script = os.path.join(FORGE_DIR, \"webui.sh\")\nif not os.path.exists(launch_script):\n    raise FileNotFoundError(f\"Launch script not found: {launch_script}\")\n\nlog_file = os.path.join(VOLUME_DIR, \"forge_background.log\")\ncmd = f\"cd {FORGE_DIR} && nohup bash webui.sh {FORGE_ARGS} > {log_file} 2>&1 &\"\nsubprocess.run([\"bash\", \"-lc\", cmd], check=True)\nprint(\"WebUI Forge started in background mode\")\nprint(f\"Log file: {log_file}\")\nprint(\"Tip: use !tail -f\", log_file)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T00:08:23.462582Z","iopub.execute_input":"2026-02-17T00:08:23.462969Z","iopub.status.idle":"2026-02-17T00:08:23.470523Z","shell.execute_reply.started":"2026-02-17T00:08:23.462942Z","shell.execute_reply":"2026-02-17T00:08:23.469642Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/1444396524.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mlaunch_script\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFORGE_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"webui.sh\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlaunch_script\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Launch script not found: {launch_script}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mlog_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVOLUME_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"forge_background.log\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: Launch script not found: /kaggle/working/stable-diffusion-webui-forge/webui.sh"],"ename":"FileNotFoundError","evalue":"Launch script not found: /kaggle/working/stable-diffusion-webui-forge/webui.sh","output_type":"error"}],"execution_count":5}]}