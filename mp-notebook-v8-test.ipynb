{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31259,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<div style=\"color:rgb(0,0,255);font-size: 40px;font-weight:700;\">\n",
    "MAIN SETTINGS\n",
    "</div>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "###SETTINGS###\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import subprocess\n",
    "import shutil\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from getpass import getpass\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "# Platform detection\n",
    "ON_KAGGLE = os.path.exists('/kaggle')\n",
    "ON_COLAB = 'COLAB_RELEASE_TAG' in os.environ or os.path.exists('/content')\n",
    "ON_VAST = any(k in os.environ for k in (\"VAST_CONTAINERLABEL\", \"VAST_TCP_PORT_22\", \"CONTAINER_ID\")) or os.path.exists('/workspace')\n",
    "\n",
    "\n",
    "MAX_PARALLEL_DOWNLOADS = max(1, int(os.environ.get(\"MAX_PARALLEL_DOWNLOADS\", \"3\")))\n",
    "MIN_VALID_FILE_BYTES = int(os.environ.get(\"MIN_VALID_FILE_BYTES\", \"1000000\"))\n",
    "\n",
    "if shutil.which(\"aria2c\") is None:\n",
    "    print(\"aria2c not found → installing...\")\n",
    "    try:\n",
    "        subprocess.run([\"apt\", \"update\", \"-qq\"], check=True, capture_output=True)\n",
    "        result = subprocess.run([\"apt\", \"install\", \"-y\", \"-qq\", \"aria2\"], capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(\"aria2c installed successfully\")\n",
    "        else:\n",
    "            print(\"Install failed (code {}):\".format(result.returncode))\n",
    "            print(\"stderr:\", result.stderr.strip())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"apt error (code {e.returncode}): {e.stderr}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "else:\n",
    "    print(\"aria2c already available\")\n",
    "\n",
    "# Determining the working directory\n",
    "possible_bases = [\n",
    "    \"/workspace\",       # Vast.ai / RunPod\n",
    "    \"/kaggle/working\",  # Kaggle\n",
    "    \"/content\",         # Google Colab\n",
    "]\n",
    "\n",
    "BASE_DIR = None\n",
    "for path in possible_bases:\n",
    "    if os.path.isdir(path):\n",
    "        BASE_DIR = path\n",
    "        break\n",
    "\n",
    "if BASE_DIR is None:\n",
    "    BASE_DIR = os.getcwd()\n",
    "    print(\"WARNING: Known directory not found:\", BASE_DIR)\n",
    "\n",
    "print(\"Working directory:\", BASE_DIR)\n",
    "\n",
    "# Configuration\n",
    "FORGE_DIR        = os.path.join(BASE_DIR, \"stable-diffusion-webui-forge\")\n",
    "MODELS_DIR       = os.path.join(BASE_DIR, \"stable-diffusion-webui-forge\", \"models\", \"Stable-diffusion\")\n",
    "LORA_DIR         = os.path.join(BASE_DIR, \"stable-diffusion-webui-forge\", \"models\", \"Lora\")\n",
    "CONTROLNET_DIR   = os.path.join(FORGE_DIR, \"extensions\", \"sd-webui-controlnet\")\n",
    "CONTROLNET_MODELS_DIR = os.path.join(CONTROLNET_DIR, \"models\")\n",
    "EXTENSIONS_DIR   = os.path.join(FORGE_DIR, \"extensions\")\n",
    "OUTPUTS_DIR      = os.path.join(FORGE_DIR, \"outputs\")\n",
    "VOLUME_DIR       = os.path.join(BASE_DIR, \"volume\")\n",
    "GEN_DIR          = os.path.join(BASE_DIR, \"gen\")\n",
    "\n",
    "for d in [MODELS_DIR, LORA_DIR, CONTROLNET_DIR, CONTROLNET_MODELS_DIR, EXTENSIONS_DIR, OUTPUTS_DIR, VOLUME_DIR, GEN_DIR]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# Dependencies used by generation cell\n",
    "for pkg in [\"openpyxl\", \"requests\"]:\n",
    "    try:\n",
    "        __import__(pkg)\n",
    "    except Exception:\n",
    "        print(f\"Installing missing dependency: {pkg}\")\n",
    "        subprocess.run([\"python\", \"-m\", \"pip\", \"install\", \"-q\", pkg], check=False)\n",
    "\n",
    "\n",
    "def get_secret(name: str):\n",
    "    \"\"\"Get secret from env/Kaggle/Colab only.\"\"\"\n",
    "    value = os.environ.get(name)\n",
    "    if value:\n",
    "        return value.strip(), \"env\"\n",
    "\n",
    "    # Kaggle secrets\n",
    "    if ON_KAGGLE:\n",
    "        try:\n",
    "            from kaggle_secrets import UserSecretsClient\n",
    "            value = UserSecretsClient().get_secret(name)\n",
    "            if value:\n",
    "                return value.strip(), \"kaggle_secrets\"\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Colab secrets panel: from google.colab import userdata\n",
    "    if ON_COLAB:\n",
    "        try:\n",
    "            from google.colab import userdata\n",
    "            value = userdata.get(name)\n",
    "            if value:\n",
    "                return value.strip(), \"colab_userdata\"\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return None, None\n",
    "\n",
    "\n",
    "CIVITAI_TOKEN, CIVITAI_SRC = get_secret(\"CIVITAI_TOKEN\")\n",
    "HF_TOKEN, HF_SRC = get_secret(\"HF_TOKEN\")\n",
    "\n",
    "if not CIVITAI_TOKEN:\n",
    "    manual_civitai = getpass(\"Enter CIVITAI_TOKEN (leave blank to skip): \").strip()\n",
    "    if manual_civitai:\n",
    "        CIVITAI_TOKEN, CIVITAI_SRC = manual_civitai, \"manual_input\"\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    manual_hf = getpass(\"Enter HF_TOKEN (leave blank to skip): \").strip()\n",
    "    if manual_hf:\n",
    "        HF_TOKEN, HF_SRC = manual_hf, \"manual_input\"\n",
    "\n",
    "TOKENS = {}\n",
    "if CIVITAI_TOKEN:\n",
    "    TOKENS[\"CIVITAI\"] = CIVITAI_TOKEN\n",
    "if HF_TOKEN:\n",
    "    TOKENS[\"HF_TOKEN\"] = HF_TOKEN\n",
    "\n",
    "print(\"Token sources:\")\n",
    "print(f\"  CIVITAI_TOKEN: {CIVITAI_SRC or 'not found'}\")\n",
    "print(f\"  HF_TOKEN: {HF_SRC or 'not found'}\")\n",
    "if ON_VAST:\n",
    "    print(\"Vast.ai tip: add CIVITAI_TOKEN/HF_TOKEN in template env vars, restart container, then rerun this cell.\")\n",
    "\n",
    "if not CIVITAI_TOKEN:\n",
    "    print(\"CivitAI token not found\")\n",
    "if not HF_TOKEN:\n",
    "    print(\"HF token not found\")\n",
    "if not TOKENS:\n",
    "    raise RuntimeError(\"No tokens were provided. Set secrets or enter at least one token (CivitAI or HF).\")\n",
    "\n",
    "\n",
    "def _prepare_download_url(url, token):\n",
    "    \"\"\"CivitAI download works more reliably with token as query param.\"\"\"\n",
    "    if token and \"civitai.com/api/download/models\" in url and \"token=\" not in url:\n",
    "        sep = \"&\" if \"?\" in url else \"?\"\n",
    "        return f\"{url}{sep}{urlencode({'token': token})}\"\n",
    "    return url\n",
    "\n",
    "\n",
    "def _looks_valid_file(path, min_bytes=MIN_VALID_FILE_BYTES):\n",
    "    return os.path.exists(path) and os.path.getsize(path) > min_bytes\n",
    "\n",
    "\n",
    "def _human_mb(num_bytes):\n",
    "    return f\"{num_bytes / (1024 * 1024):.1f} MB\"\n",
    "\n",
    "\n",
    "def _estimate_expected_mb(label):\n",
    "    # Examples: \"151 MB\", \"6,46 GB\"\n",
    "    match = re.search(r\"(\\d+[\\.,]?\\d*)\\s*(MB|GB)\", label, re.IGNORECASE)\n",
    "    if not match:\n",
    "        return None\n",
    "    value = float(match.group(1).replace(',', '.'))\n",
    "    unit = match.group(2).upper()\n",
    "    return value * (1024 if unit == \"GB\" else 1)\n",
    "\n",
    "\n",
    "def _size_sanity_warning(path, expected_mb, tolerance=0.7):\n",
    "    if expected_mb is None or not os.path.exists(path):\n",
    "        return\n",
    "    actual_mb = os.path.getsize(path) / (1024 * 1024)\n",
    "    if actual_mb < expected_mb * tolerance:\n",
    "        print(f\"  WARNING: file size looks low ({actual_mb:.1f} MB vs expected ~{expected_mb:.1f} MB)\")\n",
    "\n",
    "\n",
    "def _has_min_free_disk(path, required_mb, reserve_mb=1024):\n",
    "    if required_mb is None:\n",
    "        return True\n",
    "    usage = shutil.disk_usage(path)\n",
    "    free_mb = usage.free / (1024 * 1024)\n",
    "    return free_mb >= (required_mb + reserve_mb)\n",
    "\n",
    "\n",
    "def _download_one(job, target_dir):\n",
    "    label, url, filename, token_name = job\n",
    "    token = TOKENS.get(token_name)\n",
    "    output_path = os.path.join(target_dir, filename)\n",
    "\n",
    "    expected_mb = _estimate_expected_mb(label)\n",
    "\n",
    "    if _looks_valid_file(output_path):\n",
    "        size = os.path.getsize(output_path)\n",
    "        print(f\"[SKIP] {label}: already exists ({_human_mb(size)})\")\n",
    "        _size_sanity_warning(output_path, expected_mb)\n",
    "        return (label, True, \"exists\")\n",
    "\n",
    "    if expected_mb is not None and not _has_min_free_disk(target_dir, expected_mb):\n",
    "        return (label, False, \"insufficient_disk\")\n",
    "\n",
    "    tmp_path = output_path + \".part\"\n",
    "    if os.path.exists(tmp_path):\n",
    "        os.remove(tmp_path)\n",
    "\n",
    "    final_url = _prepare_download_url(url, token if token_name == \"CIVITAI\" else None)\n",
    "\n",
    "    cmd = [\n",
    "        \"aria2c\",\n",
    "        \"--allow-overwrite=true\",\n",
    "        \"--auto-file-renaming=false\",\n",
    "        \"--continue=true\",\n",
    "        \"--max-connection-per-server=16\",\n",
    "        \"--split=16\",\n",
    "        \"--min-split-size=1M\",\n",
    "        \"--console-log-level=warn\",\n",
    "        \"--summary-interval=1\",\n",
    "        \"--check-certificate=false\",\n",
    "        \"--out\", os.path.basename(tmp_path),\n",
    "        \"--dir\", target_dir,\n",
    "        final_url,\n",
    "    ]\n",
    "\n",
    "    if token_name == \"HF_TOKEN\" and token:\n",
    "        cmd.insert(-1, f\"--header=Authorization: Bearer {token}\")\n",
    "\n",
    "    print(f\"[DOWNLOADING] {label}\")\n",
    "    result = subprocess.run(cmd, text=True, capture_output=True)\n",
    "    if result.returncode != 0:\n",
    "        stderr = (result.stderr or \"\").strip()\n",
    "        stdout = (result.stdout or \"\").strip()\n",
    "        msg = stderr or stdout or f\"aria2c exited {result.returncode}\"\n",
    "        if os.path.exists(tmp_path):\n",
    "            os.remove(tmp_path)\n",
    "        return (label, False, msg)\n",
    "\n",
    "    if not _looks_valid_file(tmp_path):\n",
    "        size = os.path.getsize(tmp_path) if os.path.exists(tmp_path) else 0\n",
    "        if os.path.exists(tmp_path):\n",
    "            os.remove(tmp_path)\n",
    "        return (label, False, f\"downloaded file too small ({_human_mb(size)})\")\n",
    "\n",
    "    os.replace(tmp_path, output_path)\n",
    "    _size_sanity_warning(output_path, expected_mb)\n",
    "    return (label, True, _human_mb(os.path.getsize(output_path)))\n",
    "\n",
    "\n",
    "def run_download_list(download_list, target_dir, title):\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "    if not download_list:\n",
    "        print(\"No items.\")\n",
    "        return\n",
    "\n",
    "    workers = min(MAX_PARALLEL_DOWNLOADS, len(download_list))\n",
    "    print(f\"Parallel downloads: {workers}\")\n",
    "\n",
    "    ok = 0\n",
    "    fail = 0\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=workers) as ex:\n",
    "        futures = [ex.submit(_download_one, job, target_dir) for job in download_list]\n",
    "        for fut in as_completed(futures):\n",
    "            label, success, info = fut.result()\n",
    "            if success:\n",
    "                ok += 1\n",
    "                print(f\"[OK]   {label} -> {info}\")\n",
    "            else:\n",
    "                fail += 1\n",
    "                print(f\"[FAIL] {label} -> {info}\")\n",
    "\n",
    "    print(f\"Done: OK={ok}, FAIL={fail}\")\n",
    "    if fail > 0:\n",
    "        raise RuntimeError(f\"Some downloads failed in {title}: {fail} item(s)\")\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div style=\"color:rgb(0,0,255);font-size: 40px;font-weight:700;\">\n",
    "DOWNLOAD BLOCK\n",
    "</div>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "### CONTROLNET INSTALL OPTIONS ###\n",
    "\n",
    "# Option A (default): force clean reinstall\n",
    "# Option B: set CONTROLNET_INSTALL_MODE = \"update\" for git pull in existing repo\n",
    "# Option C: set CONTROLNET_INSTALL_MODE = \"skip\" to keep current state\n",
    "\n",
    "CONTROLNET_REPO_URL = \"https://github.com/Mikubill/sd-webui-controlnet\"\n",
    "CONTROLNET_INSTALL_MODE = os.environ.get(\"CONTROLNET_INSTALL_MODE\", \"reinstall\").strip().lower()\n",
    "\n",
    "if CONTROLNET_INSTALL_MODE not in {\"reinstall\", \"update\", \"skip\"}:\n",
    "    raise ValueError(\"CONTROLNET_INSTALL_MODE must be one of: reinstall, update, skip\")\n",
    "\n",
    "print(f\"ControlNet extension path: {CONTROLNET_DIR}\")\n",
    "print(f\"Install mode: {CONTROLNET_INSTALL_MODE}\")\n",
    "\n",
    "if CONTROLNET_INSTALL_MODE == \"skip\":\n",
    "    print(\"ControlNet install skipped\")\n",
    "else:\n",
    "    if os.path.isdir(CONTROLNET_DIR) and CONTROLNET_INSTALL_MODE == \"reinstall\":\n",
    "        print(\"Removing existing ControlNet directory...\")\n",
    "        shutil.rmtree(CONTROLNET_DIR)\n",
    "\n",
    "    if not os.path.isdir(CONTROLNET_DIR):\n",
    "        print(\"Cloning ControlNet repository...\")\n",
    "        subprocess.run([\"git\", \"clone\", CONTROLNET_REPO_URL, CONTROLNET_DIR], check=True)\n",
    "    else:\n",
    "        print(\"Updating ControlNet repository...\")\n",
    "        subprocess.run([\"git\", \"-C\", CONTROLNET_DIR, \"pull\", \"--ff-only\"], check=True)\n",
    "\n",
    "os.makedirs(CONTROLNET_MODELS_DIR, exist_ok=True)\n",
    "print(\"ControlNet repository is ready\")\n",
    "print(f\"ControlNet models directory: {CONTROLNET_MODELS_DIR}\")\n",
    "\n",
    "#ControlNET models download\n",
    "\n",
    "controlnet_models_to_download = [\n",
    "    (\"t2i-adapter_xl_openpose 151 MB\", \"https://huggingface.co/lllyasviel/sd_control_collection/resolve/main/t2i-adapter_xl_openpose.safetensors\", \"t2i-adapter_xl_openpose.safetensors\", \"HF_TOKEN\"),\n",
    "    (\"t2i-adapter_xl_canny 148 MB\", \"https://huggingface.co/lllyasviel/sd_control_collection/resolve/main/t2i-adapter_xl_canny.safetensors\", \"t2i-adapter_xl_canny.safetensors\", \"HF_TOKEN\"),\n",
    "    (\"t2i-adapter_xl_sketch 148 MB\", \"https://huggingface.co/lllyasviel/sd_control_collection/resolve/main/t2i-adapter_xl_sketch.safetensors\", \"t2i-adapter_xl_sketch.safetensors\", \"HF_TOKEN\"),\n",
    "    (\"t2i-adapter_diffusers_xl_depth_midas 151 MB\", \"https://huggingface.co/lllyasviel/sd_control_collection/resolve/main/t2i-adapter_diffusers_xl_depth_midas.safetensors\", \"t2i-adapter_diffusers_xl_depth_midas.safetensors\", \"HF_TOKEN\"),\n",
    "    (\"t2i-adapter_diffusers_xl_depth_zoe 151 MB\", \"https://huggingface.co/lllyasviel/sd_control_collection/resolve/main/t2i-adapter_diffusers_xl_depth_zoe.safetensors\", \"t2i-adapter_diffusers_xl_depth_zoe.safetensors\", \"HF_TOKEN\"),\n",
    "    (\"t2i-adapter_diffusers_xl_lineart 151 MB\", \"https://huggingface.co/lllyasviel/sd_control_collection/resolve/main/t2i-adapter_diffusers_xl_lineart.safetensors\", \"t2i-adapter_diffusers_xl_lineart.safetensors\", \"HF_TOKEN\"),\n",
    "]\n",
    "\n",
    "run_download_list(controlnet_models_to_download, CONTROLNET_MODELS_DIR, \"ControlNet\")\n",
    "\n",
    "#Checkpoints download\n",
    "\n",
    "models_to_download = [\n",
    "    (\"WAI ILL V16.0 6,46 GB\", \"https://civitai.com/api/download/models/2514310?type=Model&format=SafeTensor&size=pruned&fp=fp16\", \"wai_v160.safetensors\", \"CIVITAI\"),\n",
    "]\n",
    "\n",
    "run_download_list(models_to_download, MODELS_DIR, \"Checkpoints\")\n",
    "\n",
    "#LoRa download\n",
    "\n",
    "lora_to_download = [\n",
    "    (\"Detailer IL V2 218 MB\",        \"https://civitai.com/api/download/models/1736373?type=Model&format=SafeTensor\",    \"detailer_v2_il.safetensors\",     \"CIVITAI\"),\n",
    "    (\"Realistic filter V1 55 MB\",    \"https://civitai.com/api/download/models/1124771?type=Model&format=SafeTensor\",    \"realistic_filter_v1_il.safetensors\", \"CIVITAI\"),\n",
    "    (\"Hyperrealistic V4 ILL 435 MB\", \"https://civitai.com/api/download/models/1914557?type=Model&format=SafeTensor\",    \"hyperrealistic_v4_ill.safetensors\",  \"CIVITAI\"),\n",
    "    (\"Niji semi realism V3.5 ILL 435 MB\", \"https://civitai.com/api/download/models/1882710?type=Model&format=SafeTensor\", \"niji_v35.safetensors\", \"CIVITAI\"),\n",
    "    (\"ATNR Style ILL V1.1 350 MB\", \"https://civitai.com/api/download/models/1711464?type=Model&format=SafeTensor\", \"atnr_style_ill_v1.1.safetensors\", \"CIVITAI\"),\n",
    "    (\"Face Enhancer Ill 218 MB\", \"https://civitai.com/api/download/models/1839268?type=Model&format=SafeTensor\", \"face_enhancer_ill.safetensors\", \"CIVITAI\"),\n",
    "    (\"Smooth Detailer Booster V4 243 MB\", \"https://civitai.com/api/download/models/2196453?type=Model&format=SafeTensor\", \"smooth_detailer_booster_v4.safetensors\", \"CIVITAI\"),\n",
    "    (\"USNR Style V-pred 157 MB\", \"https://civitai.com/api/download/models/2555444?type=Model&format=SafeTensor\", \"usnr_style.safetensors\", \"CIVITAI\"),\n",
    "    (\"748cm Style V1 243 MB\", \"https://civitai.com/api/download/models/1056404?type=Model&format=SafeTensor\", \"748cm_style_v1.safetensors\", \"CIVITAI\"),\n",
    "    (\"Velvet's Mythic Fantasy Styles IL 218 MB\", \"https://civitai.com/api/download/models/2620790?type=Model&format=SafeTensor\", \"velvets_styles.safetensors\", \"CIVITAI\"),\n",
    "    (\"Pixel Art Style IL V7 435 MB\", \"https://civitai.com/api/download/models/2661972?type=Model&format=SafeTensor\", \"pixel_art.safetensors\", \"CIVITAI\"),\n",
    "]\n",
    "\n",
    "run_download_list(lora_to_download, LORA_DIR, \"LoRA\")\n",
    "\n"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div style=\"color:rgb(0,0,255);font-size: 40px;font-weight:700;\">\n",
    "ARCHIVE+OUTPUT\n",
    "</div>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "### API GENERATION FROM XLSX ###\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import glob\n",
    "import time\n",
    "import shutil\n",
    "import zipfile\n",
    "import base64\n",
    "from datetime import datetime\n",
    "\n",
    "import requests\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "API_BASE = os.environ.get(\"FORGE_API_BASE\", \"http://127.0.0.1:17860\")\n",
    "TXT2IMG_URL = f\"{API_BASE}/sdapi/v1/txt2img\"\n",
    "OPENAPI_URL = f\"{API_BASE}/openapi.json\"\n",
    "\n",
    "PROMPTS_CANDIDATES = [\n",
    "    os.path.join(GEN_DIR, \"prompts.xlsx\"),\n",
    "    os.path.join(GEN_DIR, \"prompts.xlxs\"),\n",
    "    os.path.join(GEN_DIR, \"Prompts.xlsx\"),\n",
    "    os.path.join(GEN_DIR, \"Prompts.xlxs\"),\n",
    "    os.path.join(BASE_DIR, \"prompts.xlsx\"),\n",
    "    os.path.join(BASE_DIR, \"prompts.xlxs\"),\n",
    "    os.path.join(BASE_DIR, \"Prompts.xlsx\"),\n",
    "    os.path.join(BASE_DIR, \"Prompts.xlxs\"),\n",
    "    \"/workspace/gen/prompts.xlsx\",\n",
    "    \"/workspace/gen/prompts.xlxs\",\n",
    "]\n",
    "\n",
    "if ON_COLAB:\n",
    "    # Colab manual upload often lands in /content\n",
    "    PROMPTS_CANDIDATES.extend([\n",
    "        \"/content/prompts.xlsx\",\n",
    "        \"/content/prompts.xlxs\",\n",
    "        \"/content/Prompts.xlsx\",\n",
    "        \"/content/Prompts.xlxs\",\n",
    "    ])\n",
    "\n",
    "if ON_KAGGLE:\n",
    "    # Fixed Kaggle dataset path used in your workflow\n",
    "    PROMPTS_CANDIDATES.append(\"/kaggle/input/datasets/sokolenkotimofei/prompts/Prompts.xlsx\")\n",
    "\n",
    "    # Additional Kaggle dataset input path patterns (fallback)\n",
    "    for pattern in [\n",
    "        \"/kaggle/input/*/*/prompts/prompts.xlsx\",\n",
    "        \"/kaggle/input/*/*/prompts/prompts.xlxs\",\n",
    "        \"/kaggle/input/*/*/prompts/Prompts.xlsx\",\n",
    "        \"/kaggle/input/*/*/prompts/Prompts.xlxs\",\n",
    "        \"/kaggle/input/*/prompts/prompts.xlsx\",\n",
    "        \"/kaggle/input/*/prompts/prompts.xlxs\",\n",
    "        \"/kaggle/input/*/prompts/Prompts.xlsx\",\n",
    "        \"/kaggle/input/*/prompts/Prompts.xlxs\",\n",
    "    ]:\n",
    "        PROMPTS_CANDIDATES.extend(sorted(glob.glob(pattern)))\n",
    "\n",
    "# Keep order, remove duplicates\n",
    "PROMPTS_CANDIDATES = list(dict.fromkeys(PROMPTS_CANDIDATES))\n",
    "\n",
    "API_READY_TIMEOUT = int(os.environ.get(\"FORGE_API_READY_TIMEOUT\", \"180\"))\n",
    "API_READY_INTERVAL = float(os.environ.get(\"FORGE_API_READY_INTERVAL\", \"3\"))\n",
    "FORGE_BG_LOG = os.path.join(VOLUME_DIR, \"forge_background.log\")\n",
    "\n",
    "\n",
    "def diagnose_api_unreachable(base_url: str):\n",
    "    print(\"\\n[DIAG] Forge API still unavailable.\")\n",
    "    print(\"[DIAG] Expected run order for Kaggle/Colab: 1) SETTINGS → 2) DOWNLOAD BLOCK → 3) OPTIONAL BOOTSTRAP (cell 7) → 4) OPTIONAL START FORGE (cell 8) → 5) API GENERATION (this cell).\")\n",
    "\n",
    "    launch_script = os.path.join(FORGE_DIR, \"webui.sh\")\n",
    "    if not os.path.exists(launch_script):\n",
    "        print(f\"[DIAG] webui.sh not found: {launch_script}\")\n",
    "        print(\"[DIAG] Run OPTIONAL BOOTSTRAP cell first (cell 7).\")\n",
    "\n",
    "    if os.path.exists(FORGE_BG_LOG):\n",
    "        print(f\"[DIAG] Found Forge background log: {FORGE_BG_LOG}\")\n",
    "        try:\n",
    "            with open(FORGE_BG_LOG, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                tail_lines = f.readlines()[-20:]\n",
    "            print(\"[DIAG] Last Forge log lines:\")\n",
    "            print(\"\".join(tail_lines).strip() or \"[DIAG] (log is empty)\")\n",
    "        except Exception as log_err:\n",
    "            print(f\"[DIAG] Failed to read Forge log: {log_err}\")\n",
    "    else:\n",
    "        print(f\"[DIAG] Forge log not found: {FORGE_BG_LOG}\")\n",
    "        print(\"[DIAG] Start OPTIONAL START FORGE IN BACKGROUND cell (cell 8), then retry this cell.\")\n",
    "\n",
    "def wait_for_api_ready(base_url: str, timeout_s: int = API_READY_TIMEOUT, interval_s: float = API_READY_INTERVAL):\n",
    "    start = time.time()\n",
    "    last_error = None\n",
    "    while (time.time() - start) < timeout_s:\n",
    "        try:\n",
    "            r = requests.get(f\"{base_url}/sdapi/v1/progress\", timeout=10)\n",
    "            if r.status_code < 500:\n",
    "                return True\n",
    "            last_error = f\"HTTP {r.status_code}\"\n",
    "        except Exception as e:\n",
    "            last_error = str(e)\n",
    "        time.sleep(interval_s)\n",
    "    diagnose_api_unreachable(base_url)\n",
    "    raise RuntimeError(\n",
    "        f\"Forge API is not reachable at {base_url} after {timeout_s}s. \"\n",
    "        f\"Last error: {last_error}. \"\n",
    "        \"Run cells 7 and 8 first on Kaggle/Colab, then rerun this API cell.\"\n",
    "    )\n",
    "\n",
    "\n",
    "LOG_PATH = os.path.join(VOLUME_DIR, \"log.txt\")\n",
    "API_IMAGES_DIR = os.path.join(OUTPUTS_DIR, \"api_generated\")\n",
    "os.makedirs(API_IMAGES_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "CONFLICT_RULES = [\n",
    "    ((\"hr_resize_x\", \"hr_resize_y\"), (\"hr_scale\",)),\n",
    "]\n",
    "\n",
    "\n",
    "def log_line(text: str):\n",
    "    ts = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    line = f\"[{ts}] {text}\"\n",
    "    print(line)\n",
    "    with open(LOG_PATH, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "\n",
    "def normalize_value(value):\n",
    "    if isinstance(value, str):\n",
    "        stripped = value.strip()\n",
    "        lowered = stripped.lower()\n",
    "        if lowered in {\"\", \"null\", \"none\", \"nan\"}:\n",
    "            return None\n",
    "        if lowered in {\"true\", \"yes\", \"1\"}:\n",
    "            return True\n",
    "        if lowered in {\"false\", \"no\", \"0\"}:\n",
    "            return False\n",
    "        if re.fullmatch(r\"-?\\d+\", stripped):\n",
    "            return int(stripped)\n",
    "        if re.fullmatch(r\"-?\\d+\\.\\d+\", stripped):\n",
    "            return float(stripped)\n",
    "        return stripped\n",
    "    return value\n",
    "\n",
    "\n",
    "def first_existing_path(candidates):\n",
    "    for p in candidates:\n",
    "        if os.path.exists(p):\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "\n",
    "def parse_workbook(path):\n",
    "    wb = load_workbook(path, data_only=True)\n",
    "    ws = wb.active\n",
    "\n",
    "    instruction_parts = [str(v).strip() for v in ws[1] if v is not None and str(v).strip()]\n",
    "    if not instruction_parts:\n",
    "        raise ValueError(\"Первая строка (инструкция) пустая\")\n",
    "    instruction = \" \".join(instruction_parts)\n",
    "\n",
    "    headers = [str(v).strip() if v is not None else \"\" for v in ws[2]]\n",
    "    if not any(headers):\n",
    "        raise ValueError(\"Вторая строка должна содержать имена переменных (заголовки столбцов)\")\n",
    "\n",
    "    rows = []\n",
    "    for row_idx in range(3, ws.max_row + 1):\n",
    "        row_values = [normalize_value(v) for v in next(ws.iter_rows(min_row=row_idx, max_row=row_idx, values_only=True))]\n",
    "        if all(v is None for v in row_values):\n",
    "            continue\n",
    "\n",
    "        row_map = {}\n",
    "        for idx, key in enumerate(headers):\n",
    "            if not key:\n",
    "                continue\n",
    "            value = row_values[idx] if idx < len(row_values) else None\n",
    "            if value is None:\n",
    "                continue\n",
    "            row_map[key] = value\n",
    "        rows.append(row_map)\n",
    "\n",
    "    if not rows:\n",
    "        raise ValueError(\"Не найдено строк данных (начиная с 3-й строки)\")\n",
    "\n",
    "    return instruction, rows\n",
    "\n",
    "\n",
    "def render_instruction(template: str, variables: dict):\n",
    "    def repl(match):\n",
    "        key = match.group(1)\n",
    "        return str(variables.get(key, match.group(0)))\n",
    "    return re.sub(r\"\\{([a-zA-Z0-9_]+)\\}\", repl, template)\n",
    "\n",
    "\n",
    "def apply_conflict_rules(payload: dict):\n",
    "    cleaned = dict(payload)\n",
    "    for primary_keys, conflicting_keys in CONFLICT_RULES:\n",
    "        primary_present = all((k in cleaned and cleaned[k] is not None) for k in primary_keys)\n",
    "        if primary_present:\n",
    "            for ck in conflicting_keys:\n",
    "                cleaned.pop(ck, None)\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def fetch_txt2img_params_dump():\n",
    "    try:\n",
    "        response = requests.get(OPENAPI_URL, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        spec = response.json()\n",
    "        schemas = spec.get(\"components\", {}).get(\"schemas\", {})\n",
    "        candidates = [\n",
    "            \"StableDiffusionTxt2ImgProcessingApi\",\n",
    "            \"Txt2ImgRequest\",\n",
    "            \"StableDiffusionProcessingTxt2Img\",\n",
    "        ]\n",
    "        for name in candidates:\n",
    "            if name in schemas:\n",
    "                props = schemas[name].get(\"properties\", {})\n",
    "                return json.dumps({k: v.get(\"type\", \"unknown\") for k, v in props.items()}, ensure_ascii=False, indent=2)\n",
    "        return json.dumps(spec.get(\"paths\", {}).get(\"/sdapi/v1/txt2img\", {}), ensure_ascii=False, indent=2)\n",
    "    except Exception as e:\n",
    "        return f\"Не удалось получить список переменных: {e}\"\n",
    "\n",
    "\n",
    "def save_images_from_response(images_b64, generation_idx):\n",
    "    saved = 0\n",
    "    for i, b64 in enumerate(images_b64, start=1):\n",
    "        image_data = b64.split(\",\", 1)[-1]\n",
    "        file_name = f\"gen_{generation_idx:04d}_{i:02d}.png\"\n",
    "        file_path = os.path.join(API_IMAGES_DIR, file_name)\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            f.write(base64.b64decode(image_data))\n",
    "        saved += 1\n",
    "    return saved\n",
    "\n",
    "\n",
    "def archive_outputs(tag: str):\n",
    "    stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    archive_name = os.path.join(VOLUME_DIR, f\"outputs_{tag}_{stamp}.zip\")\n",
    "    with zipfile.ZipFile(archive_name, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "        for root, _, files in os.walk(OUTPUTS_DIR):\n",
    "            for file in files:\n",
    "                full_path = os.path.join(root, file)\n",
    "                rel_path = os.path.relpath(full_path, OUTPUTS_DIR)\n",
    "                zf.write(full_path, rel_path)\n",
    "\n",
    "    for entry in os.listdir(OUTPUTS_DIR):\n",
    "        p = os.path.join(OUTPUTS_DIR, entry)\n",
    "        if os.path.isdir(p):\n",
    "            shutil.rmtree(p)\n",
    "        else:\n",
    "            os.remove(p)\n",
    "    os.makedirs(API_IMAGES_DIR, exist_ok=True)\n",
    "    log_line(f\"ARCHIVE created: {archive_name}. OUTPUTS_DIR cleaned.\")\n",
    "\n",
    "\n",
    "wait_for_api_ready(API_BASE)\n",
    "\n",
    "prompts_path = first_existing_path(PROMPTS_CANDIDATES)\n",
    "if not prompts_path:\n",
    "    raise FileNotFoundError(\n",
    "        \"Файл prompts.xlsx/prompts.xlxs не найден. Ожидались пути: \" + \", \".join(PROMPTS_CANDIDATES)\n",
    "    )\n",
    "\n",
    "instruction_template, generation_rows = parse_workbook(prompts_path)\n",
    "log_line(f\"Loaded prompts file: {prompts_path}. Rows for generation: {len(generation_rows)}\")\n",
    "\n",
    "syntax_error_dumped = False\n",
    "images_since_archive = 0\n",
    "\n",
    "for generation_idx, row_values in enumerate(generation_rows, start=1):\n",
    "    payload = dict(row_values)\n",
    "\n",
    "    if \"prompt\" not in payload:\n",
    "        payload[\"prompt\"] = render_instruction(instruction_template, row_values)\n",
    "\n",
    "    payload = {k: v for k, v in payload.items() if v is not None}\n",
    "    payload = apply_conflict_rules(payload)\n",
    "\n",
    "    try:\n",
    "        response = requests.post(TXT2IMG_URL, json=payload, timeout=1800)\n",
    "        if response.status_code >= 400:\n",
    "            err_text = response.text[:1200]\n",
    "            if not syntax_error_dumped:\n",
    "                params_dump = fetch_txt2img_params_dump()\n",
    "                log_line(f\"#{generation_idx} FAIL syntax/validation: {response.status_code} {err_text}\")\n",
    "                log_line(\"AVAILABLE PARAMS DUMP START\")\n",
    "                with open(LOG_PATH, \"a\", encoding=\"utf-8\") as f:\n",
    "                    f.write(params_dump + \"\\n\")\n",
    "                log_line(\"AVAILABLE PARAMS DUMP END\")\n",
    "                syntax_error_dumped = True\n",
    "            else:\n",
    "                log_line(f\"#{generation_idx} FAIL: {response.status_code} {err_text}\")\n",
    "            continue\n",
    "\n",
    "        result = response.json()\n",
    "        images = result.get(\"images\", []) or []\n",
    "        saved_now = save_images_from_response(images, generation_idx)\n",
    "        images_since_archive += saved_now\n",
    "        log_line(f\"#{generation_idx} OK images={saved_now}\")\n",
    "\n",
    "        if images_since_archive >= 15:\n",
    "            archive_outputs(tag=f\"part_{generation_idx:04d}\")\n",
    "            images_since_archive = 0\n",
    "\n",
    "    except requests.exceptions.Timeout:\n",
    "        log_line(f\"#{generation_idx} FAIL: timeout (possible heavy generation or API freeze)\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        reason = str(e)\n",
    "        if \"out of memory\" in reason.lower() or \"oom\" in reason.lower():\n",
    "            reason = \"OOM\"\n",
    "        log_line(f\"#{generation_idx} FAIL: {reason}\")\n",
    "    except Exception as e:\n",
    "        log_line(f\"#{generation_idx} FAIL: unexpected error: {e}\")\n",
    "\n",
    "if images_since_archive > 0:\n",
    "    archive_outputs(tag=\"final\")\n",
    "\n",
    "log_line(\"Generation cycle completed\")\n",
    "\n"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OPTIONAL: KAGGLE/COLAB FORGE BOOTSTRAP ###\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "if not (ON_KAGGLE or ON_COLAB):\n",
    "    print(\"Optional cell: предназначена только для Kaggle/Colab. Текущая платформа пропущена.\")\n",
    "else:\n",
    "    required_packages = [\"git\", \"python3-venv\", \"python3-pip\"]\n",
    "    print(\"Checking/installing platform dependencies...\")\n",
    "    subprocess.run([\"apt\", \"update\", \"-qq\"], check=False)\n",
    "    subprocess.run([\"apt\", \"install\", \"-y\", \"-qq\", *required_packages], check=False)\n",
    "\n",
    "    launch_script = os.path.join(FORGE_DIR, \"webui.sh\")\n",
    "    git_head = os.path.join(FORGE_DIR, \".git\", \"HEAD\")\n",
    "    forge_ready = os.path.isfile(launch_script) and os.path.isfile(git_head)\n",
    "\n",
    "    if forge_ready:\n",
    "        print(\"WebUI Forge already exists and looks valid, skipping clone.\")\n",
    "    else:\n",
    "        if os.path.isdir(FORGE_DIR):\n",
    "            print(\"FORGE_DIR exists but WebUI Forge is incomplete/corrupted. Recreating...\")\n",
    "            shutil.rmtree(FORGE_DIR)\n",
    "\n",
    "        print(\"Cloning WebUI Forge...\")\n",
    "        subprocess.run([\n",
    "            \"git\", \"clone\", \"https://github.com/lllyasviel/stable-diffusion-webui-forge\", FORGE_DIR\n",
    "        ], check=True)\n",
    "\n",
    "        if not os.path.isfile(os.path.join(FORGE_DIR, \"webui.sh\")):\n",
    "            raise FileNotFoundError(\"Clone completed but webui.sh not found. Check repository state.\")\n",
    "\n",
    "    print(\"Optional bootstrap finished.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OPTIONAL: START WEBUI FORGE IN BACKGROUND ###\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "FORGE_ARGS = \"--xformers --api --cuda-malloc --cuda-stream --pin-shared-memory --theme dark --port 17860\"\n",
    "\n",
    "if not os.path.isdir(FORGE_DIR):\n",
    "    raise FileNotFoundError(f\"Forge directory not found: {FORGE_DIR}\")\n",
    "\n",
    "launch_script = os.path.join(FORGE_DIR, \"webui.sh\")\n",
    "if not os.path.exists(launch_script):\n",
    "    raise FileNotFoundError(f\"Launch script not found: {launch_script}\")\n",
    "\n",
    "log_file = os.path.join(VOLUME_DIR, \"forge_background.log\")\n",
    "cmd = f\"cd {FORGE_DIR} && nohup bash webui.sh {FORGE_ARGS} > {log_file} 2>&1 &\"\n",
    "\n",
    "if ON_COLAB and os.geteuid() == 0:\n",
    "    run_user = os.environ.get(\"FORGE_RUN_USER\", \"forge\")\n",
    "\n",
    "    user_exists = subprocess.run([\"id\", \"-u\", run_user], capture_output=True, text=True).returncode == 0\n",
    "    if not user_exists:\n",
    "        subprocess.run([\"useradd\", \"-m\", \"-s\", \"/bin/bash\", run_user], check=True)\n",
    "\n",
    "    subprocess.run([\"chown\", \"-R\", f\"{run_user}:{run_user}\", FORGE_DIR], check=True)\n",
    "    subprocess.run([\"chown\", \"-R\", f\"{run_user}:{run_user}\", VOLUME_DIR], check=True)\n",
    "\n",
    "    subprocess.run([\"su\", \"-s\", \"/bin/bash\", \"-c\", cmd, run_user], check=True)\n",
    "    print(f\"WebUI Forge started in background mode as user '{run_user}'\")\n",
    "else:\n",
    "    subprocess.run([\"bash\", \"-lc\", cmd], check=True)\n",
    "    print(\"WebUI Forge started in background mode\")\n",
    "\n",
    "print(f\"Log file: {log_file}\")\n",
    "print(\"Tip: use !tail -f\", log_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OPTIONAL: WAIT FOR FORGE API CONNECTION ###\n",
    "\n",
    "import time\n",
    "import requests\n",
    "\n",
    "API_BASE = os.environ.get(\"FORGE_API_BASE\", \"http://127.0.0.1:17860\")\n",
    "PING_URL = f\"{API_BASE}/sdapi/v1/progress\"\n",
    "WAIT_TIMEOUT = int(os.environ.get(\"FORGE_WAIT_TIMEOUT\", \"1800\"))\n",
    "WAIT_INTERVAL = int(os.environ.get(\"FORGE_WAIT_INTERVAL\", \"30\"))\n",
    "\n",
    "print(f\"Проверка связи с Forge API: {PING_URL}\")\n",
    "print(f\"Интервал проверки: {WAIT_INTERVAL} сек, таймаут: {WAIT_TIMEOUT} сек\")\n",
    "\n",
    "start = time.time()\n",
    "attempt = 0\n",
    "last_error = None\n",
    "\n",
    "while (time.time() - start) < WAIT_TIMEOUT:\n",
    "    attempt += 1\n",
    "    try:\n",
    "        r = requests.get(PING_URL, timeout=10)\n",
    "        if r.status_code < 500:\n",
    "            elapsed = int(time.time() - start)\n",
    "            print(f\"✅ связь установлена: Forge API готов (попытка {attempt}, ~{elapsed} сек).\")\n",
    "            break\n",
    "        last_error = f\"HTTP {r.status_code}\"\n",
    "    except Exception as e:\n",
    "        last_error = str(e)\n",
    "\n",
    "    elapsed = int(time.time() - start)\n",
    "    print(f\"[{elapsed:>4} сек] API ещё не готов ({last_error}). Следующая проверка через {WAIT_INTERVAL} сек...\")\n",
    "    time.sleep(WAIT_INTERVAL)\n",
    "else:\n",
    "    raise RuntimeError(\n",
    "        f\"Не удалось дождаться Forge API за {WAIT_TIMEOUT} сек. Последняя ошибка: {last_error}\"\n",
    "    )\n"
   ]
  }
 ]
}
